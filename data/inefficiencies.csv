inefficiencies
Name,Slug,Collection ID,Locale ID,Item ID,Archived,Draft,Created On,Updated On,Published On,Author name,Cloud provider,Cloud service,Inefficiency Type,Service Category,Explanation,Relevant Billing Model,Detection,Remediation,Relevant Documentation,Show on popular,Show on community picks,Intro text,Definition ID,Example Cost Savings,CER:
Always-On PTUs for Seasonal or Cyclical Azure OpenAI Workloads,always-on-ptus-for-seasonal-or-cyclical-azure-openai-workloads-ccb97,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e042bd81767e0a979d,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:18 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Ariel Lichterman,azure,azure-cognitive-services,unnecessary-continuous-provisioning,ai,"<p id="""">Many Azure OpenAI workloads—such as reporting pipelines, marketing workflows, batch inference jobs, or time-bound customer interactions—only run during specific periods. When PTUs remain fully provisioned 24/7, organizations incur continuous fixed cost even during extended idle time. Although Azure does not offer native PTU scheduling, teams can use automation to provision and deprovision PTUs based on predictable cycles. This allows them to retain performance during peak windows while reducing cost during low-activity periods.</p>","<p id="""">PTUs are billed for every hour they remain provisioned. If workloads operate on weekly, monthly, or seasonal cycles, keeping PTUs active outside of demand windows results in paying for idle dedicated capacity.</p>","<ul id=""""><li id="""">Identify PTU-backed workloads with recurring or predictable activity patterns</li><li id="""">Review utilization trends showing regular periods of inactivity despite continuous PTU allocation</li><li id="""">Evaluate whether workload timing aligns with business cycles, reporting schedules, or seasonal events</li><li id="""">Assess whether PTUs remain active during nights, weekends, or large idle blocks</li></ul>","<ul id=""""><li id="""">Use automation to scale PTUs up and down according to expected workload schedules</li><li id="""">Provision PTUs only during windows where predictable throughput and low latency are required</li><li id="""">Establish scheduling policies for cyclical workloads to prevent unnecessary continuous provisioning</li><li id="""">Regularly re-evaluate patterns to ensure scheduling aligns with evolving business cycles</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput"" id="""">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput</a></li></ul>",FALSE,FALSE,,,,Azure-AI-2017
Archival Blob Container Storing Objects in Non-Archival Tiers,archival-blob-container-storing-objects-in-non-archival-tiers,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b43f7704f33ce49c8d8,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,azure,azure-blob-storage,inefficient-configuration,storage,"<p id="""">This inefficiency occurs when a blob container intended for long-term or infrequently accessed data continues to store objects in higher-cost tiers like Hot or Cool, instead of using the Archive tier. This often happens when containers are created without lifecycle policies or default tier settings. Over time, storing archival data in non-archival tiers results in avoidable cost without any performance benefit, especially for compliance data, backups, or historical logs that rarely need to be accessed.</p>","<p id="""">Blob storage is billed based on the total volume of data stored per tier (Hot, Cool, Archive), with higher per-GB costs for tiers optimized for frequent access. The Hot tier has the highest storage cost but lowest access latency and transaction cost. Archive tier is the least expensive for storage but comes with higher data retrieval costs and latency. Cool tier sits in the middle. Costs accrue based on the selected tier, regardless of whether the data is actively used.</p>","<ul id=""""><li id="""">Identify blob containers with large volumes of data stored in the Hot or Cool tier</li><li id="""">Evaluate access patterns to confirm whether the data is rarely or never read</li><li id="""">Review whether the container’s data retention requirements align with archival use cases</li><li id="""">Check for the absence of lifecycle management rules or default tier settings</li><li id="""">Confirm with data owners that the objects do not require immediate or frequent retrieval</li></ul>","<p id="""">Transition eligible objects to the Archive tier to reduce long-term storage costs. Implement Azure Blob Lifecycle Management policies to automatically move data to lower-cost tiers based on age or last access time. When migrating existing objects, confirm that the retrieval latency and cost profile of the Archive tier aligns with business needs.</p>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/storage/blobs/\&quot;"" id="""">Azure Blob Storage Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview\"" id="""">Blob Lifecycle Management</a></li></ul>",FALSE,FALSE,,111,,<p>Azure-Storage-2119</p>
Azure Hybrid Benefit Not Enabled on SQL Databases,azure-hybrid-benefit-not-enabled-on-sql-databases-41927,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80df5c6143ecb7bea792,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:22 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,azure,azure-sql,licensing-configuration-gap,databases,"<p id="""">Azure Hybrid Benefit allows organizations to apply existing SQL Server licenses with Software Assurance or qualifying subscriptions to Azure SQL Databases. When this configuration is missed or not enforced, workloads continue to incur license-inclusive costs despite license ownership. This oversight often occurs in environments where licensing governance is decentralized or when databases are provisioned manually without applying existing entitlements. Across multiple databases or elastic pools, these duplicated license costs can accumulate substantially over time.</p>","<p id="""">Azure SQL Database charges for compute and storage. Compute pricing can be either license-inclusive (pay-as-you-go) or license-excluded (when Azure Hybrid Benefit is applied). If AHB is not enabled, eligible databases are billed at higher license-inclusive rates even though valid SQL Server licenses already exist, resulting in double payment.</p>","<ul id=""""><li id="""">Review all SQL Databases and Elastic Pools to determine if Azure Hybrid Benefit is enabled in their configuration</li><li id="""">Cross-reference SQL workloads with licensing records to confirm Software Assurance or subscription eligibility</li><li id="""">Identify databases still billed under license-inclusive (pay-as-you-go) rates despite license ownership</li><li id="""">Evaluate whether automated deployment templates or policies enforce AHB consistently across environments **Remediation:**</li><li id="""">Enable Azure Hybrid Benefit for all eligible SQL Databases and Elastic Pools to leverage owned licenses</li><li id="""">Apply Azure Policy to automatically enforce AHB configuration for new database deployments</li><li id="""">Centralize license management to ensure entitlements are applied consistently across subscriptions</li><li id="""">Periodically audit SQL Database configurations to confirm continued alignment with active Software Assurance coverage **Relevant Documentation:Contributor:** Aaran Bhambra **Definition ID:** N/A</li></ul>","<ul id=""""><li id="""">Enable Azure Hybrid Benefit for all eligible SQL Databases and Elastic Pools to leverage owned licenses</li><li id="""">Apply Azure Policy to automatically enforce AHB configuration for new database deployments</li><li id="""">Centralize license management to ensure entitlements are applied consistently across subscriptions</li><li id="""">Periodically audit SQL Database configurations to confirm continued alignment with active Software Assurance coverage **Relevant Documentation:Contributor:** Aaran Bhambra **Definition ID:** N/A</li></ul>",,FALSE,FALSE,,,,Azure-Databases-8397
Azure Hybrid Benefit Not Enabled on Virtual Machines,azure-hybrid-benefit-not-enabled-on-virtual-machines-a8f4b,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80df6593fb2d43573cd5,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:21 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Aaran Bhambra,azure,azure-virtual-machines,licensing-configuration-gap,compute,"<p id="""">Many organizations purchase Software Assurance or subscription-based Windows and SQL Server licenses that entitle them to use Azure Hybrid Benefit. However, if the setting is not applied on eligible resources, Azure continues charging pay-as-you-go rates that already include Microsoft licensing costs. This oversight results in paying twice—once for the on-premises license and once for the built-in Azure license. The inefficiency often goes unnoticed because licensing configurations are not centrally validated or enforced. Enabling AHUB can reduce costs by up to 40% for Windows server VMs and up to 30% for SQL Databases.</p>","<p id="""">Azure bills Windows and SQL-based VMs either as license-inclusive (pay-as-you-go) or license-excluded (when Azure Hybrid Benefit is applied). Without AHB enabled, workloads that already own valid licenses are billed at the higher license-inclusive rate, resulting in duplicated spend.</p>","<ul id=""""><li id="""">Review all Windows and SQL Server virtual machines to determine if Azure Hybrid Benefit is enabled in their configuration</li><li id="""">Correlate VM licensing status with internal Software Assurance or Microsoft volume license records to confirm eligibility</li><li id="""">Evaluate the proportion of eligible VMs still billed at license-inclusive (pay-as-you-go) rates</li><li id="""">Check whether governance policies exist to enforce AHB application during provisioning</li></ul>","<ul id=""""><li id="""">Enable Azure Hybrid Benefit for all eligible Windows and SQL-based VMs to apply owned licenses and reduce compute costs</li><li id="""">Implement Azure Policy or blueprints to automatically enforce AHB for future deployments where eligibility applies</li><li id="""">Maintain an up-to-date inventory of licensed assets and ensure license entitlements align with deployed resources</li><li id="""">Regularly review licensing configuration compliance across subscriptions to prevent recurrence</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/windows/hybrid-use-benefit-licensing"" id="""">Azure Hybrid Benefit for Windows Server</a></li></ul>",FALSE,FALSE,,,,Azure-Compute-1255
Billing Account Migration Creating Emergency List-Price Purchases in Google Cloud Marketplace,billing-account-migration-creating-emergency-list-price-purchases-in-google-cloud-marketplace,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58e0ae4b378aa3bc270,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Alexa Abbruscato,gcp,gcp-cloud-marketplace,subscription-disruption-due-to-billing-migration,other,"<p id="""">Changing a Google Cloud billing account can unintentionally break existing Marketplace subscriptions. If entitlements are tied to the original billing account, the subscription may fail or become invalid, prompting teams to make urgent, direct purchases of the same services, often at higher list or on-demand rates. These emergency purchases bypass previously negotiated Marketplace pricing and can result in significantly higher short-term costs. The issue is common during reorganizations, mergers, or changes to billing hierarchy and is often not discovered until after costs have spiked.</p>","<p id="""">Subscription-based billing through Google Cloud Marketplace; defaults to on-demand or direct-vendor list pricing when Marketplace entitlements are interrupted.</p>","<ul id=""""><li id="""">Review billing changes or migrations and identify any correlated increases in list-price or on-demand service charges</li><li id="""">Analyze invoice data for new direct purchases following a billing account switch</li><li id="""">Identify gaps in Marketplace subscription continuity during or after billing hierarchy updates</li><li id="""">Check for duplicated services: one via Marketplace, the other via direct vendor or on-demand</li><li id="""">Confirm whether Marketplace entitlements were invalidated or paused during billing transitions</li></ul>","<ul id=""""><li id="""">Secure fallback agreements with vendors prior to billing account changes to ensure service continuity</li><li id="""">Establish ""true-up"" clauses that allow emergency direct purchases to be retroactively priced at Marketplace rates</li><li id="""">Document and communicate subscription dependencies before initiating billing account migrations</li><li id="""">Coordinate billing migrations with vendor account teams to maintain entitlement continuity</li></ul>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/marketplace/docs/overview"" id="""">https://cloud.google.com/marketplace/docs/overview</a></li><li id=""""><a href=""https://cloud.google.com/billing/docs/how-to/manage-billing-account"" id="""">https://cloud.google.com/billing/docs/how-to/manage-billing-account</a></li><li id=""""><a href=""https://cloud.google.com/marketplace/docs/manage-subscriptions"" id="""">https://cloud.google.com/marketplace/docs/manage-subscriptions</a></li></ul>",FALSE,FALSE,,N/A,,<p>GCP-Other-1612</p>
Business Critical Tier on Non-Production SQL Instance,business-critical-tier-on-non-production-sql-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b460450b904e0d65a7d,FALSE,FALSE,Thu May 22 2025 14:57:10 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,azure,azure-sql,inefficient-configuration,databases,"<p id="""">Non-production environments such as development, testing, or staging often do not require the high availability, failover capabilities, and premium storage performance offered by the Business Critical tier. Running these workloads on Business Critical unnecessarily inflates costs. Choosing a lower-cost tier like General Purpose typically provides sufficient performance and availability for non-production use cases, significantly reducing ongoing database expenses.</p>","<p id="""">Azure SQL Database instances are billed based on the selected service tier, compute size (vCores), and storage consumption. The Business Critical tier provides the highest availability and performance, using additional replicas and premium storage, but comes at a significantly higher cost compared to lower tiers like General Purpose. Selecting an unnecessarily high tier for non-production workloads leads to elevated expenses without proportional business value.</p>","<ul id=""""><li id="""">Identify Azure SQL Database instances deployed in the Business Critical service tier</li><li id="""">Review associated workload environments to determine if the instance supports production or non-production use</li><li id="""">Assess application performance and availability requirements to confirm if Business Critical features are necessary</li><li id="""">Evaluate whether General Purpose or Hyperscale tiers would adequately meet workload needs</li><li id="""">Validate findings with application owners, development teams, or database administrators before making changes</li></ul>","<ul id=""""><li id="""">Migrate non-production SQL instances from the Business Critical tier to a lower-cost alternative, such as General Purpose.</li><li id="""">Use downtime windows or database copy strategies to minimize risk during tier transitions, depending on instance size and availability requirements.</li><li id="""">Monitor performance after migration to ensure the workload remains stable and meets operational needs.</li><li id="""">Update deployment templates and provisioning practices to avoid defaulting non-production instances to the Business Critical tier going forward.</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-general-purpose-business-critical-hyperscale\&quot;"" id="""">Azure SQL Database Service Tiers</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/azure-sql-database/single/\"" id="""">Azure SQL Database Pricing</a></li></ul>",FALSE,FALSE,,113,,<p>Azure-Databases-5008</p>
Continuous AWS Config Recording in Non-Production Environments,continuous-aws-config-recording-in-non-production-environments,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58eb2b51da41a6420c1,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Jérémy Nancel,aws,aws-config,excessive-recording-frequency,other,"<p id="""">By default, AWS Config is enabled in continuous recording mode. While this may be justified for production workloads where detailed auditability is critical, it is rarely necessary in non-production environments. Frequent changes in development or testing environments — such as redeploying Lambda functions, ECS tasks, or EC2 instances — generate large volumes of CIRs. This results in disproportionately high costs with minimal benefit to governance or compliance. Switching non-production environments to daily recording reduces CIR volume significantly while maintaining sufficient visibility for tracking changes.</p>","<p id="""">AWS Config charges per Configuration Item Recorded (CIR). Continuous mode generates a CIR for every resource change, while daily mode generates a maximum of one CIR per resource per day. In non-production environments with frequent deployments or updates, continuous mode creates unnecessary cost without meaningful compliance value.</p>","<ul id=""""><li id="""">Review whether AWS Config is set to continuous recording in non-production accounts or environments</li><li id="""">Evaluate change frequency in development, QA, and staging systems to estimate unnecessary CIR volume</li><li id="""">Confirm whether governance or compliance requirements justify continuous recording in these environments</li><li id="""">Assess consistency of recording settings across production vs. non-production accounts</li></ul>","<ul id=""""><li id="""">Update AWS Config settings in non-production accounts to daily recording frequency instead of continuous</li><li id="""">Apply environment-specific configuration baselines to enforce lower granularity tracking outside of production</li><li id="""">Validate that compliance and auditing needs remain satisfied after reducing recording frequency</li></ul>",,FALSE,FALSE,,N/A,,<p>AWS-Other-6716</p>
Delayed Transition of Objects to Intelligent-Tiering in an S3 Bucket,delayed-transition-of-objects-to-intelligent-tiering-in-an-s3-bucket,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b41e507026472042b2b,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-s3,inefficient-configuration,storage,"<p id="""">Some S3 lifecycle policies are configured to transition objects from Standard storage to Intelligent-Tiering after a fixed number of days (e.g., 30 days). This creates a delay where objects reside in S3 Standard, incurring higher storage costs without benefit. Since Intelligent-Tiering does not require prior access history and can be used immediately, it is often more efficient to place objects directly into Intelligent-Tiering at the time of upload. Lifecycle transitions introduce unnecessary intermediate costs that can be avoided entirely through configuration changes.</p>","<p id="""">S3 storage is billed per GB per month, with rates varying by storage class.</p><ul id=""""><li id="""">S3 Standard is the default and most expensive tier for frequently accessed data.</li><li id="""">Intelligent-Tiering includes a small per-object monitoring and automation fee but automatically transitions data to lower-cost tiers based on access patterns.</li><li id="""">When objects are uploaded to S3 Standard and later transitioned to Intelligent-Tiering using a lifecycle policy, you incur:</li><li id="""">S3 Standard rates during the delay period, and</li><li id="""">Transition request fees for moving the object</li></ul><p id="""">Uploading directly into Intelligent-Tiering avoids both of these.</p>","<ul id=""""><li id="""">Identify buckets where Intelligent-Tiering is the intended or primary storage class</li><li id="""">Review how new objects are placed into those buckets—determine whether they are uploaded directly into Intelligent-Tiering or initially stored in S3 Standard and later moved to Intelligent-Tiering via a Lifecycle Policy</li><li id="""">Evaluate whether the delay provides any functional or operational benefit, or if it is a legacy configuration</li><li id="""">Confirm with application owners or data teams whether objects can be placed directly into Intelligent-Tiering at time of upload</li></ul>","<p id="""">Configure applications and upload processes to default new objects directly into the S3 Intelligent-Tiering storage class. If application-level changes aren’t feasible, update lifecycle rules to transition objects on day 0. This minimizes time spent in the more expensive S3 Standard tier and avoids transition request fees.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/s3/storage-classes/"" id="""">S3 Storage Classes</a></li><li id=""""><a href=""https://aws.amazon.com/s3/pricing/"" id="""">S3 Pricing</a></li></ul>",FALSE,FALSE,,50,,<p>AWS-Storage-4583</p>
Delete-on-Termination Disabled for EBS Volume,delete-on-termination-disabled-for-ebs-volume,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589418749d147775bb8925,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Dvir Mizrahi,aws,aws-ebs,misconfiguration-leading-to-future-orphaned-resource,storage,"<p id="""">When EC2 instances are provisioned, each attached EBS volume has a `DeleteOnTermination` flag that determines whether it will be deleted when the instance is terminated. If this flag is set to `false` — often unintentionally in custom launch templates, AMIs, or older automation scripts — volumes persist after termination, resulting in orphaned storage. While detached volumes are easy to detect and clean up after the fact, proactively identifying attached volumes with `DeleteOnTermination=false` can prevent future waste before it occurs.</p>","<p id="""">EBS volumes are billed per GB-month of provisioned storage. If a volume is not automatically deleted when its EC2 instance is terminated, charges continue to accrue even if the volume is no longer in use.</p>","<ul id=""""><li id="""">The volume is attached to an active EC2 instance</li><li id="""">The `DeleteOnTermination` flag is set to `false`</li><li id="""">The instance is not intended to retain persistent data on termination (e.g., ephemeral workloads or stateless apps)</li></ul>","<ul id=""""><li id="""">Update the instance configuration to set `DeleteOnTermination=true` for non-persistent volumes</li><li id="""">Modify infrastructure-as-code templates and launch configurations to use the correct flag by default</li><li id="""">Establish policy controls or monitoring to flag instances with unnecessary persistent volume retention</li><li id="""">Periodically review long-lived instances for non-root volumes with this misconfiguration</li><li id="""">Educate engineering teams on the default behavior and risks of leaving the flag unset</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html"" id="""">Amazon EBS Volume Lifecycle</a><br><a href=""https://docs.aws.amazon.com/cli/latest/reference/ec2/modify-instance-attribute.html"" id="""">ModifyInstanceAttribute – AWS CLI</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-1070</p>
Disabled Retry Policies in EventBridge,disabled-retry-policies-in-eventbridge,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941b63a59b4ee44bda28,FALSE,FALSE,Sun Jun 22 2025 23:39:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Jarred Clore,aws,aws-eventbridge,misconfiguration,other,"<p id="""">By default, EventBridge includes retry mechanisms for delivery failures, particularly when targets like Lambda functions or Step Functions fail to process an event. However, if these retry policies are disabled or misconfigured, EventBridge may treat failed deliveries as successful, prompting upstream services to republish the same event multiple times in response to undelivered outcomes.  This leads to:  * Duplicate event publishing and delivery   * Unnecessary compute triggered by repeated events   * Increased EventBridge, downstream service, and data transfer costs  This behavior is especially problematic in systems where idempotency is not strictly enforced and retries are managed externally by upstream services.</p>","<p id="""">EventBridge charges are based on the number of events published to an event bus, including retries or duplicate events. Each event published — whether successful or redundant — incurs cost.</p>","<ul id=""""><li id="""">Review whether retry policies are explicitly disabled on EventBridge rules</li><li id="""">Check event bus logs or metrics for repeated identical events over short intervals</li><li id="""">Identify downstream targets receiving multiple identical invocations for the same source event</li><li id="""">Assess whether upstream services are implementing manual retry logic due to undelivered events</li><li id="""">Evaluate whether duplicate events align with downstream failures or outages</li></ul>","<ul id=""""><li id="""">Enable built-in retry policies on EventBridge rules to reduce reliance on external retry logic</li><li id="""">Confirm downstream targets are configured with error handling (e.g., DLQs, retry settings)</li><li id="""">Audit event patterns for high duplication rates and correlate with retry settings</li><li id="""">Consider using EventBridge archive and replay for reliable reprocessing when retries are disabled</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-quota.html"" id="""">Amazon EventBridge Quotas and Limits</a><br><a href=""https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-event-delivery-retries.html"" id="""">EventBridge Event Retry Policy</a><br><a href=""https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-best-practices.html"" id="""">Best Practices for EventBridge</a></p>",FALSE,FALSE,,,,<p>AWS-Other-4734</p>
Double Counting on EDP Commitments,double-counting-on-edp-commitments,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58d6d74b5efe8dcf552,FALSE,FALSE,Mon Nov 03 2025 16:17:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Alexa Abbruscato,aws,aws-marketplace,commitment-misalignment,other,"<p id="""">Many organizations mistakenly believe that all AWS Marketplace spend automatically contributes to their EDP commitment. In reality, only certain Marketplace transactions, those involving EDP-eligible vendors and transactable SKUs, will count towards a portion of their EDP commitment. This misunderstanding can lead to double counting: forecasting based on the assumption that both native AWS usage and Marketplace purchases will fully draw down the commitment. If the assumptions are incorrect, the organization risks failing to meet its EDP threshold, incurring penalties or losing expected discounts.</p>","<p id="""">Marketplace purchases are billed via AWS and may count toward Enterprise Discount Program (EDP) commitments only if they are considered “EDP-eligible.” However, not all SKUs or sellers qualify, and eligibility may vary based on how the contract was negotiated. Buyers sometimes assume that Marketplace spend always contributes to EDP drawdown, leading to overcommitment or missed discounts.</p>","<ul id=""""><li id="""">Review EDP commitment reports and compare drawdown pace against forecasted Marketplace purchases</li><li id="""">Evaluate if Marketplace purchases were included in EDP planning assumptions</li><li id="""">Identify high-value Marketplace purchases that did not contribute to commitment drawdown</li><li id="""">Check whether Marketplace SKUs are transactable and EDP-eligible for the specific seller and contract</li><li id="""">Automated cost anomaly detection for commitment vs. actual drawdown</li><li id="""">API-based monitoring of Marketplace EDP eligibility status</li><li id="""">Regular reconciliation reports comparing forecasted vs. actual EDP contributions</li><li id="""">Confirm whether any sellers have opted out of EDP eligibility</li></ul>","<ul id=""""><li id="""">Request explicit confirmation of EDP eligibility for key Marketplace vendors and SKUs before purchase</li><li id="""">Negotiate drawdown terms into enterprise contracts when possible</li><li id="""">Maintain a list of verified EDP-eligible SKUs used in cost modeling</li><li id="""">Adjust future forecasts and budgets to reflect actual drawdown behavior</li><li id="""">Implement automated tagging policies for all Marketplace purchases</li><li id="""">Establish monthly EDP utilization reviews with finance and procurement teams</li><li id="""">Create approval workflows for large Marketplace purchases that verify EDP eligibility upfront</li><li id="""">Consider alternative commitment vehicles (Savings Plans, RIs) for non-EDP eligible workloads</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/marketplace/faqs/"" id="""">https://aws.amazon.com/marketplace/faqs/</a></li><li id=""""><a href=""https://docs.aws.amazon.com/marketplace/latest/userguide/entitlements.html"" id="""">https://docs.aws.amazon.com/marketplace/latest/userguide/entitlements.html</a></li><li id=""""><a href=""https://aws.amazon.com/partners/programs/marketplace-seller/edp/"" id="""">https://aws.amazon.com/partners/programs/marketplace-seller/edp/</a></li></ul>",FALSE,FALSE,,N/A,,<p>AWS-Other-1883</p>
Duplicate Storage of Logs in Cloud Logging,duplicate-storage-of-logs-in-cloud-logging-bd625,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80df199f36e194497ccc,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:27 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Yuval Goldstein,gcp,gcp-cloud-logging,redundant-log-routing-configuration,other,"<p id="""">Duplicate log storage occurs when multiple sinks capture the same log data — for example, organization-wide sinks exporting all logs to Cloud Storage and project-level sinks doing the same. This redundancy results in paying twice (or more) for identical data. It often arises from decentralized logging configurations, inherited policies, or unclear ownership between teams. The problem is compounded when logs are routed both to Cloud Logging and external observability platforms, creating parallel ingestion streams and double billing.</p>","<p id="""">Cloud Logging charges separately for data ingestion and for storage in each destination. When logs are routed to multiple sinks with overlapping filters, organizations pay duplicate ingestion and storage costs for the same log entries. These costs scale linearly with the number of redundant destinations.</p>","<ul id=""""><li id="""">Analyze log sink filters across projects, folders, and the organization to identify overlapping export scopes</li><li id="""">Review log destinations to confirm whether the same log types are stored in multiple buckets or BigQuery datasets</li><li id="""">Assess whether central and project-level teams maintain independent sinks that duplicate the same data</li><li id="""">Determine if redundant routing exists between Cloud Logging and external observability or SIEM platforms</li></ul>","<ul id=""""><li id="""">Consolidate log routing and ensure filters define mutually exclusive export scopes</li><li id="""">Remove or disable redundant sinks that replicate the same logs to multiple destinations</li><li id="""">Implement centralized governance for sink management to avoid overlapping configurations across projects</li><li id="""">Periodically audit all sinks at the organization level to ensure routing remains efficient and non-duplicative</li></ul>","<ul id=""""><li id="""">Managing Sinks in Cloud Logging</li></ul>",FALSE,FALSE,,,,GCP-Other-9482
Duplicate or Overlapping AWS CloudTrail Trails,duplicate-or-overlapping-aws-cloudtrail-trails,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4afd4a3be1d4fa4102,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Matt Walls,aws,aws-cloudtrail,redundant-configuration,other,"<p id="""">AWS CloudTrail enables event logging across AWS services, but when multiple trails are configured to log overlapping events — especially data events — it can result in redundant charges and unnecessary storage or ingestion costs. This commonly occurs in decentralized environments where teams create trails independently, unaware of existing coverage or shared logging destinations.Each trail that records data events contributes to billing on a per-event basis, even if the same activity is logged by multiple trails. Additional costs may also arise from delivering duplicate logs to separate S3 buckets or CloudWatch Log groups. While separate trails may be justified for audit, compliance, or operational segmentation, unintentional duplication increases both cost and operational complexity without added value.</p>","<p id="""">CloudTrail billing is driven by:</p><ul id=""""><li id="""">Management events: One free copy to S3 per region per account</li><li id="""">Data events: Charged per event recorded (e.g., object-level access, function invokes)</li><li id="""">CloudTrail Insights: Charged per analyzed event</li><li id="""">Multiple trails: Additional trails that log the same events or deliver to additional destinations (e.g., CloudWatch Logs) incur cumulative charges</li></ul><p id="""">Costs scale with both the volume of logged activity and the number of trails capturing it.</p>","<ul id=""""><li id="""">List all CloudTrail trails in the account and examine their configurations</li><li id="""">Identify overlapping event types (e.g., same data events logged by multiple trails)</li><li id="""">Review whether multiple trails are logging the same resource activity</li><li id="""">Check whether each trail’s delivery destination (S3, CloudWatch Logs) is unique and necessary</li><li id="""">Evaluate whether each trail serves a distinct purpose or is redundant with existing logging</li></ul>","<ul id=""""><li id="""">Delete or disable redundant trails that provide no unique audit or compliance value</li><li id="""">Consolidate overlapping trails into a single unified configuration where feasible</li><li id="""">Use centralized log destinations (e.g., one S3 bucket) to reduce storage and ingestion cost</li><li id="""">Document trail ownership and logging strategy across teams to avoid future duplication</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/cloudtrail/pricing/\&quot;"" id="""">AWS CloudTrail Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html\"" id="""">Understanding Trails in AWS CloudTrail</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Other-8369</p>
Elastic Load Balancer with Only One EC2 Instance,elastic-load-balancer-with-only-one-ec2-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589415cb772959e146e997,FALSE,FALSE,Sun Jun 22 2025 23:39:01 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Mike Rosenberg,aws,aws-elb,inefficient-architecture,networking,"<p id="""">An ELB with only one registered EC2 instance does not achieve its core purpose—distributing traffic across multiple backends. In this configuration, the ELB adds complexity and cost without improving availability, scalability, or fault tolerance. This setup is often the result of premature scaling design or misunderstood architecture patterns. If there's no plan to horizontally scale the application, the ELB can often be removed entirely without user impact.</p>","<p id="""">All ELB types are billed per hour of provisioned uptime and, in most cases, per GB of data processed. These charges apply regardless of the number of targets or the actual volume of traffic being handled. Running an ELB with only a single EC2 instance incurs fixed costs without delivering the intended benefits of load distribution or redundancy.</p>","<ul id=""""><li id="""">Identify load balancers with only one healthy or active EC2 instance registered</li><li id="""">Check ELB metadata and target group configurations to validate instance count</li><li id="""">Review deployment patterns to determine whether the architecture expects scaling but hasn't implemented it</li><li id="""">Assess whether the ELB is used for static routing or simply passing traffic to a single backend</li></ul>","<ul id=""""><li id="""">If no scaling is planned or needed, remove the ELB and route traffic directly to the EC2 instance using a static IP or DNS entry</li><li id="""">If future scaling is expected, consider retaining the ELB but update documentation and monitoring to ensure it doesn't remain in this state long-term</li><li id="""">Document architectural decisions around ELB usage to prevent future misconfigurations</li></ul>","<p id=""""><a href=""https://aws.amazon.com/elasticloadbalancing/pricing/"" id="""">Elastic Load Balancing Pricing</a></p>",FALSE,FALSE,,,,<p>AWS-Networking-9602</p>
Excessive AWS Config Costs from Spot Instances,excessive-aws-config-costs-from-spot-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58ee7bab718d3e2ae1c,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Abdeldjallil Koutchoukali,aws,aws-config,over-recording-of-ephemeral-resources,other,"<p id="""">Spot Instances are designed to be short-lived, with frequent interruptions and replacements. When AWS Config continuously records every lifecycle change for these instances, it produces a large number of CIRs. This drives costs significantly higher without delivering meaningful compliance insight, since Spot Instances are typically stateless and non-critical. In environments with heavy Spot usage, Config costs can balloon and exceed the value of tracking these transient resources.</p>","<p id="""">AWS Config charges per CIR. Ephemeral resources like Spot Instances can generate extremely high CIR volumes because each interruption and replacement is recorded as a new resource event. This creates cost disproportionate to governance value.</p>","<ul id=""""><li id="""">Review AWS Config cost trends and correlate spikes with increased Spot Instance usage</li><li id="""">Assess the number of configuration items recorded for EC2 instances relative to the count of long-lived resources</li><li id="""">Identify whether Spot Instances are generating repeated records due to interruptions and replacements</li><li id="""">Confirm whether tracking ephemeral Spot resources is required for compliance or governance purposes</li></ul>","<ul id=""""><li id="""">Use tag-based exclusions to prevent AWS Config from recording ephemeral Spot Instances and other transient resources</li><li id="""">Apply standardized tagging (e.g., `finops:config-exclude:true`) and configure AWS Config to filter them out</li><li id="""">If some visibility is required, switch Config from continuous to periodic recording to reduce event volume</li><li id="""">Maintain full Config tracking for long-lived production resources while limiting or excluding transient workloads</li></ul>","<ul id=""""><li id="""">AWS Config Pricing | Selecting Resources to Record</li></ul>",FALSE,FALSE,,N/A,,<p>AWS-Other-3662</p>
Excessive Auto-Clustering Costs from High-Churn Tables,excessive-auto-clustering-costs-from-high-churn-tables,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4b24510cc7c808ebb2,FALSE,FALSE,Thu May 22 2025 14:57:15 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-automatic-clustering-service,inefficient-configuration,other,"<p id="""">Excessive Auto-Clustering costs occur when tables experience frequent and large-scale modifications (""high churn""), causing Snowflake to constantly recluster data. This leads to significant and often hidden compute consumption for maintenance tasks, especially when table structures or loading patterns are not optimized. Poor clustering key choices, unordered data loads, or frequent full-table replacements are common drivers of unnecessary Auto-Clustering activity.</p>",,"<ul id="""">  <li id="""">Review Auto-Clustering activity metrics to identify tables with large volumes of data reclustered relative to total table size (e.g., &gt;30–40% of the table re-clustered monthly)</li>  <li id="""">Analyze loading patterns for high-churn behaviors such as frequent bulk inserts, updates, or deletes</li>  <li id="""">Evaluate clustering key configurations to determine if keys align with actual query filter and join patterns</li></ul><p id="""">Assess whether tables experiencing high churn are critical for business performance or could tolerate less aggressive clustering</p>","<ul id="""">  <li id="""">Optimize data loading practices by using incremental loads and pre-sorting data where possible to minimize disruption to partition structures</li>  <li id="""">Redesign cluster key selections to prioritize columns commonly used in query filters and joins, limit the number of keys, and order by cardinality</li>  <li id="""">Disable or adjust clustering maintenance for low-value or rarely queried tables to reduce unnecessary overhead</li>  <li id="""">Periodically monitor clustering costs and modify configurations based on evolving query access patterns</li></ul>",,FALSE,FALSE,,,,<p>Snowflake-Other-9045</p>
Excessive CloudTrail Charges from Bulk S3 Deletes,excessive-cloudtrail-charges-from-bulk-s3-deletes,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941843e28528568c4e36,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Liam Greenamyre,aws,aws-s3,misconfigured-logging,storage,"<p id="""">When large numbers of objects are deleted from S3—such as during cleanup or lifecycle transitions—CloudTrail can log every individual delete operation if data event logging is enabled. This is especially costly when deleting millions of objects from buckets configured with CloudTrail data event logging at the object level. The resulting volume of logs can cause a significant, unexpected spike in CloudTrail charges, sometimes exceeding the cost of the underlying S3 operations themselves.  This inefficiency often occurs when teams initiate bulk deletions for cleanup or cost savings without realizing that CloudTrail logs every API call, including `DeleteObject`, if data event logging is active for the bucket.</p>","<p id="""">CloudTrail charges based on the volume of management and data events captured. S3 data events—especially object-level delete operations—can generate significant logging volume if not scoped or disabled appropriately.</p>","<ul id=""""><li id="""">Review whether CloudTrail data event logging is enabled for S3 buckets targeted for bulk deletes</li><li id="""">Check for a high volume of `DeleteObject` or `DeleteObjects` events in CloudTrail logs</li><li id="""">Validate if logging configuration is necessary for objects being deleted (e.g., legacy or non-sensitive data)</li></ul>","<ul id=""""><li id="""">Temporarily disable S3 data event logging<strong id=""""> before initiating bulk deletes where logging is unnecessary</strong></li><li id=""""><strong id="""">Scope CloudTrail data event logging</strong> to only include relevant prefixes or buckets requiring detailed auditability</li></ul>","<p id=""""><a href=""https://aws.amazon.com/cloudtrail/pricing/"" id="""">https://aws.amazon.com/cloudtrail/pricing/</a><br><a href=""https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html"" id="""">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a><br><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-objects.html"" id="""">https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-objects.html</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-3023</p>
Excessive CloudWatch Log Volume from Persistently Enabled Debugging,excessive-cloudwatch-log-volume-from-persistently-enabled-debugging,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4994d3e1f351ef3382,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Trig Ghosh,aws,aws-cloudwatch,inefficient-configuration,other,"<p id="""">Engineers often enable verbose logging (e.g., debug or trace-level) during development or troubleshooting, then forget to disable it after deployment. This results in elevated log ingestion rates — and therefore costs — even when the detailed logs are no longer needed. Because CloudWatch Logs charges per GB ingested, persistent debug logging in production environments can create silent but material cost increases, particularly for high-throughput services.In environments with multiple teams or loosely governed log group policies, this issue can go undetected for long periods. Identifying and deactivating unnecessary debug-level logging is a low-risk, high-leverage optimization.</p>","<p id="""">CloudWatch Logs charges include:</p><ul id=""""><li id="""">Ingestion cost per GB of data sent to log groups</li><li id="""">Storage cost based on retention period</li><li id="""">Optional costs for subscription filters, insights queries, and cross-account delivery</li></ul><p id="""">Log volume is driven directly by the application’s logging configuration and verbosity.</p>","<ul id=""""><li id="""">Identify log groups with consistently high ingestion volume over time</li><li id="""">Review logging level configurations in application code, infrastructure-as-code, or service parameters</li><li id="""">Correlate high-volume log groups with services not actively under debugging or incident investigation</li><li id="""">Check whether log volume spikes correspond to elevated verbosity (e.g., debug/trace level output)</li></ul>","<ul id=""""><li id="""">Reduce log verbosity from debug/trace to info or warn levels where appropriate</li><li id="""">Implement logging configuration standards across environments, with production defaults</li><li id="""">Use dynamic log level toggling (e.g., via environment variables or feature flags) to avoid persistent debug logging</li><li id="""">Set up alerts for anomalous log ingestion volumes tied to specific services or log groups</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/cloudwatch/pricing/\&quot;"" id="""">Amazon CloudWatch Logs Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch_logs_best_practices.html\"" id="""">Best Practices for CloudWatch Logs</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Other-7159</p>
Excessive Cold Starts in GCP Cloud Functions,excessive-cold-starts-in-gcp-cloud-functions,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589418432e579dce5249d5,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),,gcp,gcp-cloud-functions,inefficient-configuration,compute,"<p id="""">Cloud Functions scale to zero when idle. When invoked after inactivity, they undergo a ""cold start,"" initializing runtime, loading dependencies, and establishing any required network connections (e.g., VPC connectors). These cold starts can dramatically increase execution time, especially for functions with:  * High memory allocations   * Heavy initialization logic   * VPC connector requirements  If cold starts are frequent, customers may be paying for unnecessary compute time — particularly in latency-sensitive workloads — without receiving proportional value.</p>","<p id="""">Billed based on:  * Number of invocations   * Execution duration (rounded up to the nearest 100ms)   * Memory allocated per function   * Network egress (if applicable)  Cold starts — which occur when no instances are available to handle an incoming request — result in latency and extended compute time, especially for larger functions or functions requiring VPC access. This not only affects performance but increases billable time per invocation.</p>","<ul id=""""><li id="""">Review logs for high variance in execution times for the same function</li><li id="""">Identify functions with long initialization logic or large dependency trees</li><li id="""">Assess frequency and distribution of cold starts across time</li><li id="""">Evaluate whether the function consistently idles between invocations (leading to scale-to-zero behavior)</li><li id="""">Check for functions using VPC connectors, which further increase cold start time</li></ul>","<ul id=""""><li id="""">Reduce function size by minimizing dependencies and optimizing startup code</li><li id="""">Use minimum instance settings to keep warm instances running during active periods</li><li id="""">Avoid using VPC connectors unless absolutely necessary — consider Private Google Access instead</li><li id="""">Split monolithic functions into smaller, more targeted functions to reduce cold start impact</li><li id="""">Use Cloud Run for workloads that require more control over warm container instances</li></ul>",,FALSE,FALSE,,,,<p>GCP-Compute-7557</p>
Excessive Data Scanned Due to Unpartitioned Tables in BigQuery,excessive-data-scanned-due-to-unpartitioned-tables-in-bigquery,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589419b7616d514947da18,FALSE,FALSE,Sun Jun 22 2025 23:39:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),,gcp,gcp-bigquery,suboptimal-configuration,databases,"<p id="""">If a table is not partitioned by a relevant column (typically a timestamp), every query scans the entire dataset, even if filtering by date. This leads to:  * High costs per query   * Long execution times   * Inefficient use of resources when querying recent or small subsets of data  This inefficiency is especially common in:  * Event or log data stored in raw, unpartitioned form     Historical data migrations without schema optimization   * Workloads developed without awareness of BigQuery’s scanning model</p>","<p id="""">BigQuery charges primarily based on:  * The amount of data scanned per query (on-demand pricing)   * Alternatively, flat-rate slots (for enterprises with high query volumes)  When using on-demand pricing, scanning large unpartitioned tables dramatically increases cost—even for queries targeting small slices of data.</p>","<ul id=""""><li id="""">Review frequently queried tables without partitioning enabled</li><li id="""">Identify queries that filter by date but scan full tables</li><li id="""">Evaluate cost per query for common lookups on historical tables</li><li id="""">Inspect schema definitions for missing partition and clustering configurations</li><li id="""">Analyze cost spikes related to ad-hoc or dashboard queries on large datasets</li></ul>","<ul id=""""><li id="""">Enable time-based partitioning on large fact or event tables</li><li id="""">Retrofit existing tables with ingestion- or column-based partitioning</li><li id="""">Cluster tables by frequently filtered fields (e.g., customer ID) to reduce scan volume</li><li id="""">Educate data teams on query best practices and partition-aware schema design</li><li id="""">Monitor top-cost queries and prioritize optimization of high-volume datasets</li></ul>","<p id="""">Partitioned Tables in BigQuery  Best Practices for Controlling Costs</p>",FALSE,FALSE,,,,<p>GCP-Databases-2141</p>
Excessive KMS Charges from Missing S3 Bucket Key Configuration,excessive-kms-charges-from-missing-s3-bucket-key-configuration,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8ffff8d072e59468791,FALSE,FALSE,Thu Aug 28 2025 22:32:31 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Kevin Talbierz,aws,aws-s3,misconfiguration,storage,"<p id="""">S3 buckets configured with SSE-KMS but without Bucket Keys generate a separate KMS request for each object operation. This behavior results in disproportionately high KMS request costs for data-intensive workloads such as analytics, backups, or frequently accessed objects. Bucket Keys allow S3 to cache KMS data keys at the bucket level, reducing the volume of KMS calls and cutting encryption costs—often with no impact on security or performance.</p>","<p id="""">When using SSE-KMS, each encryption or decryption request results in a paid call to AWS KMS. Without S3 Bucket Keys, every object operation triggers a KMS request. Bucket Keys reduce cost by allowing S3 to cache data keys and minimize KMS calls, especially beneficial for high-throughput workloads.</p>","<p id="""">• Identify S3 buckets with SSE-KMS encryption enabled</p><p id="""">• Check if Bucket Keys are disabled or not configured</p><p id="""">• Analyze object access frequency and KMS request volume</p><p id="""">• Estimate potential cost savings by enabling Bucket Keys</p><p id="""">• Prioritize buckets with high object counts or frequent read/write operations</p><p id="""">‍</p>","<p id="""">• Enable S3 Bucket Keys for eligible buckets using SSE-KMS</p><p id="""">• Document any security exceptions or requirements that prevent Bucket Key use</p><p id="""">• Note: Enabling Bucket Keys applies only to newly encrypted objects; existing objects must be re-encrypted or re-uploaded to benefit</p><p id="""">• Track KMS request metrics before and after rollout to validate cost impact</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html"" id="""">https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucketKeys.html"" id="""">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucketKeys.html</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Storage-7226</p>
Excessive Lambda Duration from Synchronous Waiting,excessive-lambda-duration-from-synchronous-waiting,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fd1f197be87f99e311,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Igor Bareev,aws,aws-lambda,inefficient-configuration,compute,"<p id="""">Some Lambda functions perform synchronous calls to other services, APIs, or internal microservices and wait for the response before proceeding. During this time, the Lambda is idle from a compute perspective but still fully billed. This anti-pattern can lead to unnecessarily long durations and elevated costs, especially when repeated across high-volume workflows or under memory-intensive configurations.</p><p id="""">While this behavior might be functionally correct, it is rarely optimal. Asynchronous invocation patterns—such as decoupling downstream calls with queues, events, or callbacks—can reduce runtime and avoid charging for waiting time. However, detecting this inefficiency is nontrivial, as high duration alone doesn’t always indicate synchronous waiting. Understanding function logic and workload patterns is key.</p>","<p id="""">Lambda costs are based on two primary dimensions:</p>","<ul id=""""><li id="""">Review Lambda functions with consistently high average or p95 durations relative to expected execution complexity</li><li id="""">Identify functions that invoke external services or APIs synchronously</li><li id="""">Check whether long execution times correspond to periods of low CPU or memory utilization</li><li id="""">Evaluate whether downstream systems could be decoupled or moved to an async pattern</li><li id="""">Consult application owners to confirm whether waiting time is built into function logic</li><li id="""">Correlate duration spikes with network latency or known API response delays</li></ul>","<ul id=""""><li id="""">Redesign functions to offload synchronous calls using asynchronous patterns (e.g., queues, event buses, Step Functions)</li><li id="""">Break apart long-running workflows into smaller chained or event-driven Lambdas</li><li id="""">Optimize memory allocation to minimize idle cost when waiting cannot be avoided</li><li id="""">Include async design principles in serverless architecture reviews</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro.html"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html</a></li><li id=""""><a href=""https://www.getorchestra.io/guides/anti-pattern-in-aws-lambda-synchronous-waiting-within-a-function"" id="""">https://www.getorchestra.io/guides/anti-pattern-in-aws-lambda-synchronous-waiting-within-a-function</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-2980</p>
Excessive Lambda Retries (Retry Storms),excessive-lambda-retries-retry-storms,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941ab12b3a0869510db6,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Liam Greenamyre,aws,aws-lambda,retry-misconfiguration,compute,"<p id="""">Retry storms occur when a function fails and is automatically retried repeatedly due to default retry behavior for asynchronous events (e.g., SQS, EventBridge). If the error is persistent and unhandled, retries can accumulate rapidly — often invisibly — creating a large volume of billable executions with no successful outcome. This is especially costly when functions run for extended durations or have high memory allocation.</p>","<p id="""">* Each retry counts as a separate request   * Billed per millisecond of execution time for each retry   * Retries from failed invocations can exponentially increase cost if not controlled</p>","<ul id=""""><li id="""">Check for high error-to-success ratios in CloudWatch Metrics</li><li id="""">Investigate spikes in invocation count after initial failure events</li><li id="""">Correlate retries with common payloads and error messages</li><li id="""">Review configurations for MaximumRetryAttempts and DLQs</li><li id="""">Assess the presence of throttling, downstream timeouts, or missing exception handling</li></ul>","<ul id=""""><li id="""">Configure DLQs to isolate and inspect failed invocations</li><li id="""">Implement exponential backoff or circuit breaker patterns in retry logic</li><li id="""">Set appropriate retry limits on event source mappings</li><li id="""">Improve exception handling and idempotency in function code</li><li id="""">Utilize anomaly detection software to alert on abnormal retry patterns</li><li id="""">Monitor CloudWatch for retry-specific metrics and alarms</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html</a><br><a href=""https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#invocation-dlq"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\#invocation-dlq</a><br><a href=""https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html#invocation-eventsourcemapping-retries"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html\#invocation-eventsourcemapping-retries</a><br><a href=""https://aws.amazon.com/lambda/pricing/"" id="""">https://aws.amazon.com/lambda/pricing/</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-5270</p>
Excessive ListBucket API Calls to an S3 Bucket,excessive-listbucket-api-calls-to-an-s3-bucket,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b43f7704f33ce49c8e1,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-s3,inefficient-architecture,storage,"<p id="""">ListBucket requests are commonly used to enumerate objects in a bucket, such as by backup systems, scheduled sync jobs, data catalogs, or monitoring tools. When these operations are frequent or target buckets with large object counts, they can generate disproportionately high request charges. In many cases, real-time enumeration is not necessary and can be replaced with more efficient alternatives like S3 Inventory, which provides object metadata on a scheduled basis at lower cost.</p>","<p id="""">S3 API operations are billed per request. In high-frequency or large-bucket scenarios, these costs can grow significantly—especially when List operations are called repeatedly by automation or monitoring processes.</p>","<ul id=""""><li id="""">Identify buckets with a high volume of ListObjects or ListObjectsV2 API requests during the lookback period</li><li id="""">Review whether these requests are part of scheduled automation, backup jobs, or inventory scans</li><li id="""">Check if the frequency of list operations aligns with actual business or operational requirements</li><li id="""">Evaluate whether repeated full-bucket scans are being performed when incremental or point-in-time data would suffice</li><li id="""">Confirm if S3 Inventory or other metadata services could replace live listing without performance impact</li></ul>","<p id="""">Start by identifying the source of the ListBucket API activity and evaluating whether the frequency and scope of these requests are truly necessary. Determine if the operation can be performed less frequently, on a smaller scope (e.g., using prefix filters), or avoided altogether without impacting business or operational needs. If real-time object listing is not required, replace frequent ListBucket calls with S3 Inventory, which provides scheduled object metadata reports at a lower cost. Additional strategies include introducing caching, modifying automation logic, or restructuring how object metadata is accessed to reduce redundant listing operations.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/s3/pricing/\&quot;"" id="""">S3 Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html\"" id="""">Amazon S3 Inventory</a></li></ul>",FALSE,FALSE,,109,,<p>AWS-Storage-1658</p>
Excessive Model Logging Enabled in Production Environments,excessive-model-logging-enabled-in-production-environments-f76b9,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e1c27f1aad7ae66b9d,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:26 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,gcp,gcp-vertex-ai,excessive-logging-configuration,ai,"<p id="""">Verbose logging is useful during development, but many teams forget to disable it before deploying to production. Generative AI workloads often include long prompts, large multi-paragraph outputs, embedding vectors, and structured metadata. When these full payloads are logged on high-throughput production endpoints, Cloud Logging costs can quickly exceed the cost of the model inference itself. This inefficiency commonly arises when development-phase logging settings carry into production environments without review.</p>","<p id="""">Cloud Logging charges per ingested GiB. Generative AI requests often contain large prompts and outputs, so logging full payloads—especially at scale—can generate substantial ingestion cost unrelated to model inference.</p>","<ul id=""""><li id="""">Identify production Vertex AI endpoints generating unusually high Cloud Logging ingestion</li><li id="""">Review logging settings to confirm whether full request and response bodies are captured</li><li id="""">Inspect logs for large text prompts, long responses, or embeddings</li><li id="""">Look for services where logging was never reduced after testing or model iteration phases</li><li id="""">Compare log volume vs. inference volume to detect disproportionate ingestion</li></ul>","<ul id=""""><li id="""">Disable full payload logging for production endpoints unless explicitly required</li><li id="""">Log only minimal request metadata (e.g., status, latency) rather than full bodies</li><li id="""">Use sampling or partial logging strategies to reduce ingestion volume</li><li id="""">Limit verbose logging to short-term debugging in dev/test environments</li><li id="""">Periodically audit Cloud Logging ingestion tied to AI endpoints</li></ul>",,FALSE,FALSE,,,,GCP-AI-2074
Excessive Retention of Audit Logs,excessive-retention-of-audit-logs,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fd149a8a50db7e7775,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 16:20:14 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 16:23:56 GMT+0000 (Coordinated Universal Time),Jurian van Hoorn,azure,azure-blob-storage,over-retention-of-data,storage,"<p id="""">Audit logs are often retained longer than necessary, especially in environments where the logging destination is not carefully selected. Projects that initially route SQL Audit Logs or other high-volume sources to LAW or Azure Storage may forget to revisit their retention strategy. Without policies in place, logs can accumulate unchecked—particularly problematic with SQL logs, which can generate significant volume. Lifecycle Management Policies in Azure Storage are a key tool for addressing this inefficiency but are often overlooked.</p><p id="""">However, tier transitions are not always cost-saving. For example, in cases where log data consists of extremely large numbers of very small files (such as AKS audit logs across many pods), the transaction charges incurred when moving objects between storage tiers may exceed the potential savings from reduced storage rates. In these scenarios, it can be more cost-effective to retain logs in Hot tier until deletion, rather than moving them to lower-cost tiers first.</p><p>‍</p>","<p id="""">Storage costs accrue based on:</p><p id="""">Volume of data stored per tier (Hot, Cool, Archive)</p><p id="""">Duration retained (GB-months)</p><p id="""">Log Analytics data stored in Analytics or Basic tables (charged per GB ingested and retained)</p><p id="""">Audit logs routed to LAW or Storage will continue to generate cost until explicitly deleted or transitioned to a cheaper tier.</p>","<ul id=""""><li id="""">Identify resources with Audit Logging enabled</li><li id="""">Determine whether logs are routed to Log Analytics Workspace or Azure Storage</li><li id="""">Assess whether current retention aligns with compliance or operational needs</li><li id="""">Evaluate volume and cost of logs retained beyond required periods</li><li id="""">Review whether lifecycle policies or retention settings are currently configured</li><li id="""">Check if any projects have a “set and forget” logging configuration that has never been reviewed</li></ul><p id="""">‍</p>","<p id="""">Apply Azure Storage Lifecycle Management Policies to transition older logs to lower-cost tiers or delete them after a set retention period. Before implementing tier transitions, assess whether the additional transaction costs from moving large volumes of small log files could outweigh potential storage savings. In such cases, consider retaining logs in Hot tier until deletion if that results in lower overall cost.</p><p id="""">For logs in Log Analytics Workspace, assess whether they can be moved to Basic tables or stored in Storage Accounts instead</p><p id="""">Establish project-specific retention requirements with stakeholders and enforce them across all audit logging configurations</p><p id="""">Periodically audit logging destinations and lifecycle settings to prevent silent cost creep</p><p>‍</p>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview"">Azure Storage Lifecycle Management Overview</a></li></ul><p id="""">‍</p>",FALSE,FALSE,,,,<p>Azure-Storage-4643</p>
Excessive Retention of Automated RDS Backups,excessive-retention-of-automated-rds-backups,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941b432e579dce524a73,FALSE,FALSE,Sun Jun 22 2025 23:39:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Andrew Shieh,aws,aws-rds,retention,storage,"<p id="""">If backup retention settings are too high or old automated backups are unnecessarily retained, costs can accumulate rapidly. RDS backup storage is significantly more expensive than equivalent storage in S3. For long-term retention or compliance use cases, exporting backups to S3 (e.g., via snapshot export to Amazon S3 in Parquet) is often more cost-effective than retaining them in RDS-native format.</p>","<p id="""">Backups in RDS are billed by the total volume of backup data retained beyond the free tier (equal to the size of the active DB storage). Additional backup storage is charged per GB-month, which can lead to disproportionately high costs if backup volumes grow over time. Exporting snapshots to S3 offers a lower-cost archival alternative.</p>","<ul id=""""><li id="""">Review backup storage consumption by DB instance</li><li id="""">Review total backup size versus DB size to identify excess retention</li><li id="""">Check if automated backup retention periods are longer than necessary</li><li id="""">Determine whether legacy snapshots are still required or can be deleted</li><li id="""">Assess whether long-term backup needs could be satisfied with lower-cost S3 exports</li></ul>","<ul id=""""><li id="""">Adjust backup retention periods to match business or compliance needs</li><li id="""">Delete outdated snapshots or unnecessary automated backups</li><li id="""">Export long-term backups to S3 using snapshot export features</li><li id="""">Monitor backup growth and align storage tier with data access needs</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"" id="""">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER\_WorkingWithAutomatedBackups.html</a><br><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ExportSnapshot.html"" id="""">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER\_ExportSnapshot.html</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-7976</p>
Excessive Retention of Logs in Cloud Logging,excessive-retention-of-logs-in-cloud-logging,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589419a77e8d025cf7c324,FALSE,FALSE,Sun Jun 22 2025 23:39:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,gcp,gcp-cloud-logging,excessive-retention-of-non-critical-data,other,"<p id="""">By default, Cloud Logging retains logs for 30 days. However, many organizations increase retention to 90 days, 365 days, or longer — even for non-critical logs such as debug-level messages, transient system logs, or audit logs in dev environments. This extended retention can lead to unnecessary costs, especially when:  * Logs are never queried after the first few days   * Observability tooling duplicates logs elsewhere (e.g., SIEM platforms)   * Retention settings are applied globally without considering log type or project criticality</p>","<p id="""">* Billed per GiB of log data ingested (beyond free tier)   * Billed per GiB of storage retained (beyond 30-day default)   * Long-term retention of high-volume log types (e.g., debug logs) increases cost without added value   * Indexed logs also incur additional costs if retained in Logging buckets with longer durations</p>","<ul id=""""><li id="""">Review Logging buckets with extended retention policies beyond the 30-day default</li><li id="""">Identify high-ingestion log types with little to no query activity</li><li id="""">Assess if the logs are duplicated in other observability or SIEM platforms</li><li id="""">Examine dev/test projects with elevated retention policies that may not be justified</li><li id="""">Evaluate whether regulatory or compliance needs require longer retention for each log category</li></ul>","<ul id=""""><li id="""">Set log-specific retention policies aligned with usage and compliance requirements</li><li id="""">Reduce retention on verbose log types such as DEBUG, INFO, or system health logs</li><li id="""">Route non-essential logs to a lower-cost or exclusionary sink (e.g., exclude from ingestion)</li><li id="""">Use aggregated sinks to centralize critical logs while filtering noise</li><li id="""">Automate regular audits of Logging buckets and retention policies across projects</li></ul>","<p id=""""><a href=""https://cloud.google.com/stackdriver/pricing"" id="""">Cloud Logging Pricing</a><br><a href=""https://cloud.google.com/logging/docs/storage/set-retention"" id="""">Configure Log Retention</a><br><a href=""https://cloud.google.com/logging/docs/exclusions"" id="""">Excluding Logs from Ingestion</a></p>",FALSE,FALSE,,,,<p>GCP-Other-2316</p>
Excessive Retries for Large Inference Outputs,excessive-retries-for-large-inference-outputs-f4348,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e2ce3b45d5ea205f7f,FALSE,FALSE,Sun Dec 14 2025 09:18:26 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:32 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,gcp,gcp-vertex-ai,excessive-retry-induced-token-consumption,ai,"<p id="""">Generative workloads that produce long outputs—such as detailed summaries, document rewrites, or multi-paragraph chat completions—require extended model runtime.</p>","<p id="""">Vertex AI generative models are billed per input and output token. Retries—especially those triggered by timeouts or long model latencies—cause repeated inference calls and duplicate token charges. This increases costs without delivering additional value.</p>","<ul id=""""><li id="""">Identify workloads generating long responses that show repeated or duplicate inference requests</li><li id="""">Review application logs for timeout-related retries or fallback invocations</li><li id="""">Examine client or API metrics for elevated retry counts on Vertex AI endpoints</li><li id="""">Assess whether output size consistently exceeds client timeout thresholds</li><li id="""">Verify whether streaming is available but unused for workloads producing large outputs</li></ul>",,,FALSE,FALSE,,,,GCP-AI-9101
Excessive Shard Count in GCP Bigtable,excessive-shard-count-in-gcp-bigtable,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941afee0dbe251836b08,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),,gcp,gcp-bigtable,inefficient-configuration,databases,"<p id="""">Bigtable automatically splits data into tablets (shards), which are distributed across provisioned nodes. However, poorly designed row key schemas or excessive shard counts (caused by high cardinality, hash-based keys, or timestamp-first designs) can result in performance bottlenecks or hot spotting.  To compensate, users often scale up node counts — increasing costs — when the real issue lies in suboptimal data distribution. This leads to inflated infrastructure spend without actual workload increase.</p>","<p id="""">Billed based on:  * Number of provisioned nodes per hour   * Storage usage   * Network egress  While storage and bandwidth costs are usage-based, node count is the primary driver of ongoing cost. Over-sharding can indirectly increase required nodes due to uneven load distribution or inefficient tablet-to-node ratios.</p>","<ul id=""""><li id="""">Analyze tablet distribution across nodes to identify imbalance or hotspots</li><li id="""">Review row key design for high-cardinality prefixes or timestamp-first patterns</li><li id="""">Evaluate node utilization vs. throughput to identify overprovisioning driven by poor sharding</li><li id="""">Compare actual query patterns against how data is distributed and accessed</li><li id="""">Investigate autoscaling triggers caused by inefficient tablet layout rather than true load growth</li></ul>","<ul id=""""><li id="""">Redesign row keys to promote even tablet distribution (e.g., avoid monotonically increasing keys)</li><li id="""">Consolidate shards where appropriate to reduce overhead</li><li id="""">Use Bigtable’s Key Visualizer tool to identify and resolve hot spotting</li><li id="""">Review autoscaling policies to ensure node count reflects actual demand, not inefficiencies in sharding</li><li id="""">Regularly audit schema and data access patterns as workload evolves</li></ul>","<p id="""">* Bigtable Pricing  * Designing Row Keys  * Avoiding Hotspots  * Using Key Visualizer###</p>",FALSE,FALSE,,,,<p>GCP-Databases-2135</p>
Excessive Snapshot Storage from High-Churn Snowflake Tables,excessive-snapshot-storage-from-high-churn-snowflake-tables,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4b2d677a5f43b912f1,FALSE,FALSE,Thu May 22 2025 14:57:15 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-snapshots,inefficient-storage-usage,storage,"<p id="""">Snowflake automatically maintains previous versions of data when tables are modified or deleted. For tables with high churn—meaning frequent INSERT, UPDATE, DELETE, or MERGE operations—this can cause a significant buildup of historical snapshot data, even if the active data size remains small.</p><p id="""">This hidden accumulation leads to elevated storage costs, particularly when Time Travel retention periods are long and data change rates are high. Often, teams are unaware of how much snapshot data is being stored behind the scenes.</p>",,"<ul id="""">  <li id="""">Identify tables with high storage consumption relative to their current active data size.</li>  <li id="""">Use transient tables or temporary tables where modification patterns (INSERTs, UPDATEs, DELETEs) to detect high-churn tables.</li>  <li id="""">Review Time Travel retention settings at the table and account level to determine if default settings are unnecessarily long.</li>  <li id="""">Estimate the ratio of historical snapshot storage versus active storage for key tables.</li>  <li id="""">Confirm with application and data engineering teams whether frequent table modifications are necessary, or if design changes could reduce churn.</li></ul>","<ul id="""">  <li id="""">Optimize Time Travel retention settings: Reduce retention periods (e.g., from 90 days to 1 day) for high-churn tables where long recovery windows are not necessary.</li>  <li id="""">Periodically clone and recreate heavily churned tables to ""reset"" accumulated historical storage if appropriate.</li>  <li id="""">Regularly monitor table storage metrics to proactively manage and clean up storage waste in evolving datasets.</li></ul>","<ul id=""""><li id="""">‍<a href=""https://docs.snowflake.com/en/user-guide/data-availability"" id="""">Time Travel and Fail Safe</a><a href=""https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs"" id="""">‍</a></li><li id=""""><a href=""https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs"" id="""">Costs for Time Travel and Fail Safe</a></li></ul><p id="""">‍</p><p id="""">‍</p>",FALSE,FALSE,,,,<p>Snowflake-Storage-4350</p>
Hidden Marketplace Spend Preventing Commitment Optimization,hidden-marketplace-spend-preventing-commitment-optimization,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58e03c20265dc71f135,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Alexa Abbruscato,aws,aws-marketplace,commitment-misalignment,other,"<p id="""">In many organizations, AWS Marketplace purchases are lumped into a single consolidated billing line without visibility into individual vendors. This lack of transparency makes it difficult to identify which Marketplace spend is eligible to count toward the EDP cap. As a result, teams may either overspend on direct AWS services to fulfill their commitment unnecessarily or miss the opportunity to right-size new commitments based on existing Marketplace purchases. In both cases, the absence of vendor-level detail hinders optimization.</p>","<p id="""">Marketplace spend is billed via AWS and may count toward EDP commitments—but only specific vendors and SKUs qualify. Additionally, Marketplace purchases that *do* count are capped at contributing no more than 25% of the total EDP commitment. Without detailed billing data, it's difficult to determine how much of the spend is eligible, leading to inaccurate forecasting and underutilized discounts.</p>","<ul id=""""><li id="""">Review EDP commitment utilization and assess if it’s lower than expected despite high Marketplace usage</li><li id="""">Analyze whether large Marketplace purchases are being made without contributing to EDP drawdown</li><li id="""">Check if cost reporting lacks vendor-level granularity for Marketplace spend</li><li id="""">Evaluate whether new commitments are being sized without factoring in eligible Marketplace spend</li></ul>","<ul id=""""><li id="""">Enable detailed cost allocation and tagging to isolate Marketplace spend by vendor</li><li id="""">Cross-reference vendor eligibility with AWS to determine which purchases count toward the 25% Marketplace cap</li><li id="""">Update forecasting and commitment planning to include both direct AWS and eligible Marketplace purchases</li><li id="""">Rebalance future commitments to maximize EDP drawdown across both purchase types</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/marketplace/latest/userguide/cost-management.html"" id="""">https://docs.aws.amazon.com/marketplace/latest/userguide/cost-management.html</a></li><li id=""""><a href=""https://aws.amazon.com/marketplace/faqs/"" id="""">https://aws.amazon.com/marketplace/faqs/</a></li><li id=""""><a href=""https://aws.amazon.com/partners/programs/marketplace-seller/edp/"" id="""">https://aws.amazon.com/partners/programs/marketplace-seller/edp/</a></li></ul>",FALSE,FALSE,,N/A,,<p>AWS-Other-9040</p>
High Transaction Cost Due to Misaligned Tier in Azure Blob Storage,high-transaction-cost-due-to-misaligned-tier-in-azure-blob-storage,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4e99df9343c0f0ef18,FALSE,FALSE,Thu May 22 2025 14:57:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-blob-storage,inefficient-configuration,storage,"<p id="""">  Azure Blob Storage tiers are designed to optimize cost based on access frequency. However, when frequently accessed data is stored in the Cool or Archive tiers—either due to misconfiguration, default settings, or cost-only optimization—transaction costs can spike. These tiers impose significantly higher charges for read/write operations and metadata access compared to the Hot tier.</p><p id="""">This misalignment is common in analytics, backup, and log-processing scenarios where large volumes of object-level operations occur regularly. While the per-GB storage rate is lower, the overall cost becomes higher due to frequent access. This inefficiency is silent but accumulates rapidly in active workloads.</p>","<ul id=""""><li id="""">Charges include per-GB storage cost and per-operation transaction cost</li><li id=""""><strong id="""">Hot tier</strong>: higher storage cost, lower transaction cost</li><li id=""""><strong id="""">Cool tier</strong>: lower storage cost, <strong id="""">higher transaction cost</strong></li><li id=""""><strong id="""">Archive tier</strong>: lowest storage cost, highest retrieval cost, with delayed access</li><li id="""">Storage tier is set per object or container and determines both baseline storage and access costs</li></ul>","<ul id=""""><li id="""">Identify storage accounts with Blob data stored in the Cool or Archive tier</li><li id="""">Analyze access patterns to determine if objects in these tiers are frequently read, written, or queried</li><li id="""">Review transaction cost line items relative to storage cost to detect skewed ratios</li><li id="""">Determine whether lifecycle management policies are in place and correctly applied</li></ul>","<ul id=""""><li id="""">Move frequently accessed data to the Hot tier, either manually or via lifecycle management policies</li><li id="""">Evaluate default tiering settings on upload processes to prevent misplacement of active data</li><li id="""">Incorporate access pattern analysis into storage tier selection decisions</li><li id="""">Periodically review storage access metrics to ensure ongoing alignment between access frequency and tier</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview"" id="""">Azure Blob Storage Access Tiers</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/storage/blobs/"" id="""">Blob Storage Pricing</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Storage-8411</p>
High Transaction Cost Due to Misaligned Tier in Azure Files,high-transaction-cost-due-to-misaligned-tier-in-azure-files,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4eec494686b8924837,FALSE,FALSE,Thu May 22 2025 14:57:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-files,inefficient-configuration,storage,"<p id="""">  Azure Files Standard tier is cost-effective for low-traffic scenarios but imposes per-operation charges that grow rapidly with frequent access. In contrast, Premium tier provides consistent IOPS and throughput without additional transaction charges. When high-throughput or performance-sensitive workloads (e.g., real-time application data, logs, user file interactions) are placed in the Standard tier, transaction costs can significantly exceed expectations.</p><p id="""">This inefficiency occurs when teams prioritize low storage cost without considering IOPS or throughput needs, or when workloads grow more active over time without reevaluation of their storage configuration. Unlike Blob Storage, migrating to Azure Files Premium requires creating a new storage account, making this an often-overlooked optimization.</p>","<ul id=""""><li id=""""><strong id="""">Standard tier</strong>: billed per GB stored and per transaction (e.g., read/write, metadata access)</li><li id=""""><strong id="""">Premium tier</strong>: billed based on provisioned capacity (minimum 100 GiB), no transaction fees</li><li id="""">High-throughput or metadata-heavy workloads can accumulate significant transaction charges in the Standard tier</li></ul>","<ul id=""""><li id="""">Review Azure Files usage with high transaction costs relative to total storage cost</li><li id="""">Identify workloads performing frequent read/write or metadata operations</li><li id="""">Determine whether workloads are latency-sensitive or experiencing throttling</li><li id="""">Check whether Premium tier would reduce cost or improve performance based on traffic profile</li></ul>","<ul id=""""><li id="""">Evaluate cost-performance tradeoffs between Standard and Premium tiers</li><li id="""">If justified, migrate data to a new Azure Files Premium account (required for tier change)</li><li id="""">Use performance metrics and transaction volume to guide future provisioning decisions</li><li id="""">Incorporate workload profiling into tier selection criteria for new deployments</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/files/storage-files-planning#premium-file-shares"" id="""">Azure Files Premium Tier</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/storage/files/"" id="""">Azure Files Pricing</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Storage-3782</p>
Idle Azure App Service Plan Without Deployed Applications,idle-azure-app-service-plan-without-deployed-applications,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4d562ebb0d80a031a0,FALSE,FALSE,Thu May 22 2025 14:57:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Prasanna Ravichandran,azure,azure-app-service,unused-resource,compute,"<p id="""">  App Service Plans continue to incur charges even when no applications are deployed. This can occur when applications are deleted, migrated, or retired, but the associated App Service Plan remains active. Without ongoing workloads, these idle plans become silent cost contributors — especially in higher-cost SKUs like Premium v3 or Isolated v2.</p><p id="""">In large or decentralized environments, unused plans can accumulate quickly if cleanup is not automated or routinely enforced. These idle plans offer no functional value but continue to consume compute resources and generate operational expense.</p>","<ul id=""""><li id="""">App Service Plans are billed based on:</li><li id=""""><strong id="""">Instance size and tier</strong> (e.g., Premium v3, Isolated v2)</li><li id=""""><strong id="""">Provisioned capacity</strong> (number of instances)</li><li id="""">Charges apply even if <strong id="""">no applications</strong> are deployed to the plan</li><li id="""">Shared tier plans (e.g., Free or Shared) are not affected, but most production workloads use dedicated SKUs</li></ul>","<ul id=""""><li id="""">Identify App Service Plans with zero deployed applications</li><li id="""">Determine whether the plan was previously associated with a workload that has been deleted or migrated</li><li id="""">Review tagging, naming, and ownership metadata to validate whether the plan is still in use or intended for future deployments</li><li id="""">Flag plans in higher-cost tiers without active utilization</li></ul>","<ul id=""""><li id="""">Decommission App Service Plans with no active applications unless a future use case is explicitly confirmed</li><li id="""">In cases with low utilization, consider consolidating multiple lightly used plans into a single plan to reduce spend</li><li id="""">Establish governance practices to routinely identify and remove orphaned plans after application lifecycle events</li><li id="""">Update provisioning workflows to clean up unused plans automatically or via policy</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/app-service/overview-hosting-plans"" id="""">Azure App Service Plans Overview</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/app-service/"" id="""">App Service Pricing</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Compute-1438</p>
Idle Azure SQL Elastic Pool Without Databases,idle-azure-sql-elastic-pool-without-databases,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4dae00e2b0e2a0a353,FALSE,FALSE,Thu May 22 2025 14:57:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Prasanna Ravichandran,azure,azure-sql,unused-resource,databases,"<p id="""">  An Azure SQL Elastic Pool continues to incur costs even if it contains no databases. This can occur when databases are deleted, migrated to single-instance configurations, or consolidated elsewhere — but the pool itself remains provisioned. In such cases, the pool becomes an idle resource consuming budget without delivering value.</p><p id="""">This inefficiency often goes undetected in large or decentralized environments where cleanup workflows are manual or inconsistent. Since the pool reserves compute and storage regardless of usage, it silently contributes to unnecessary spend over time.</p>","<ul id=""""><li id="""">Azure SQL Elastic Pools are billed based on:</li><li id="""">Provisioned compute (eDTUs or vCores)</li><li id="""">Allocated storage</li><li id="""">Charges continue even when no databases are attached to the pool</li></ul>","<ul id=""""><li id="""">Identify SQL Elastic Pools with zero active databases</li><li id="""">Review recent database migration or deletion activity to determine whether the pool was previously used</li><li id="""">Evaluate whether the pool is intentionally retained for future deployment or placeholder provisioning</li><li id="""">Confirm that no scheduled reallocation or redeployment is planned</li></ul>","<ul id=""""><li id="""">Decommission any Elastic Pool with no active databases unless a valid business case exists for retaining it</li><li id="""">Review infrastructure-as-code templates and automation pipelines to ensure pool cleanup is included in deprovisioning workflows</li><li id="""">Establish periodic audits to catch and remove idle pools across subscriptions and teams</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview"" id="""">What are Azure SQL Elastic Pools?</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/azure-sql-database/elastic/"" id="""">Azure SQL Database Pricing</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Databases-7459</p>
Idle Cloud Memorystore Redis Instance,idle-cloud-memorystore-redis-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941a8aaf395b8dedfcf2,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),ChatGPT (Validated by Taylor Houck),gcp,gcp-cloud-memorystore,inactive-resource,databases,"<p id="""">Cloud Memorystore instances that remain idle—i.e., not receiving read or write requests—continue to incur full costs based on provisioned size. In test environments, migration scenarios, or deprecated application components, Redis instances are often left running unintentionally. Since Redis does not autoscale or suspend, unused capacity results in 100% waste until explicitly deleted.</p>","<p id="""">Billed based on:  * Provisioned instance size (memory capacity)     Instance tier (Standard or Basic)   * Uptime (per second billing). Costs accrue regardless of actual data stored or traffic served.</p>","<ul id=""""><li id="""">Review memory usage and operation count metrics over a recent period</li><li id="""">Identify instances with zero or near-zero read/write commands</li><li id="""">Check for stale endpoints not referenced in active application code or configuration</li><li id="""">Validate whether the instance serves any production dependencies</li><li id="""">Confirm environment tags (e.g., dev, staging) that may indicate non-critical usage</li></ul>","<ul id=""""><li id="""">Decommission idle Redis instances no longer in use</li><li id="""">Consider scaling down instance size if usage is expected to remain minimal</li><li id="""">Use labels to track instance ownership and business purpose for easier future audits</li><li id="""">Establish a periodic review process to catch long-idle memory store resources</li></ul>","<p id="""">https://cloud.google.com/memorystore/docs/redis/pricing  https://cloud.google.com/memorystore/docs/redis/monitoring-metrics  https://cloud.google.com/memorystore/docs/redis/creating-managing-instances</p>",FALSE,FALSE,,,,<p>GCP-Databases-8659</p>
Idle Cloud NAT Gateway Without Active Traffic,idle-cloud-nat-gateway-without-active-traffic,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941a0331c6da9b0e7343,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),,gcp,gcp-cloud-nat,idle-resource-with-baseline-cost,networking,"<p id="""">Each Cloud NAT gateway provisioned in GCP incurs hourly charges for each external IP address attached, regardless of whether traffic is flowing through the gateway. In many environments, NAT configurations are created for temporary access (e.g., one-off updates, patching windows, or ephemeral resources) and are never cleaned up.  If no traffic is flowing, these NAT gateways remain idle yet continue to generate charges due to reserved IPs and persistent gateway configuration. This is especially common in non-production environments or when legacy configurations are forgotten.</p>","<p id="""">Billed based on:  * Number of NAT IP addresses used per hour   * Gigabytes of egress traffic processed through the gateway  Idle Cloud NATs incur baseline hourly costs for reserved IPs, even if no data is flowing.</p>","<ul id=""""><li id="""">Identify Cloud NAT gateways with active configurations but no recent egress traffic</li><li id="""">Cross-reference NAT configurations with active VM instances or workloads that require external access</li><li id="""">Evaluate whether any static IPs are still reserved for inactive NATs</li><li id="""">Review staging, test, or decommissioned environments where NATs may remain unintentionally</li></ul>","<ul id=""""><li id="""">Decommission unused Cloud NAT gateways with no associated traffic</li><li id="""">Release reserved external IP addresses if no longer needed</li><li id="""">Consolidate NAT configurations where feasible across shared VPCs or regions</li><li id="""">Implement tagging and lifecycle policies for temporary NAT configurations to ensure cleanup</li></ul>","<p id=""""><a href=""https://cloud.google.com/nat/pricing"" id="""">Cloud NAT Pricing</a><br><a href=""https://cloud.google.com/nat/docs/overview"" id="""">Cloud NAT Overview</a></p>",FALSE,FALSE,,,,<p>GCP-Networking-1319</p>
Idle Dataflow Workers Running After Pipeline Failure,idle-dataflow-workers-running-after-pipeline-failure-f6b1a,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80dfca8351ad686ec080,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:13 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Damian Ohienmhen,gcp,gcp-dataflow,unreleased-compute-resources-after-failure,compute,"<p id="""">When a Dataflow pipeline fails—often due to dependency issues, misconfigurations, or data format mismatches—its worker instances may remain active temporarily until the service terminates them. In some cases, misconfigured jobs, stuck retries, or delayed monitoring can cause workers to continue running for extended periods. These idle workers consume vCPU, memory, and storage resources without performing useful work. The inefficiency is compounded in large or high-frequency batch environments where repeated failures can leave many orphaned workers running concurrently.</p>","<p id="""">Dataflow charges for the compute time of active workers, as well as associated resources such as persistent disks and networking. If pipeline failures prevent graceful shutdown or cleanup, these workers continue incurring compute charges even though no processing occurs.</p>","<ul id=""""><li id="""">Review Dataflow job logs for failed or cancelled jobs that show prolonged worker activity afterward</li><li id="""">Assess cost and utilization metrics to identify worker instances continuing to accrue compute charges after pipeline termination</li><li id="""">Evaluate the frequency of job restarts or repeated retries that result in idle worker time</li><li id="""">Confirm whether monitoring and alerting mechanisms detect failed or stalled pipelines promptly</li></ul>","<ul id=""""><li id="""">Implement automated job monitoring and alerting to detect pipeline failures and trigger termination of orphaned workers</li><li id="""">Set timeouts or retry limits within Dataflow job configurations to prevent indefinite retry loops</li><li id="""">Regularly review active Dataflow jobs and terminate those that are stuck, failed, or idle</li><li id="""">Use error handling and pipeline health checks to ensure worker cleanup occurs after job failures</li></ul>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/dataflow/docs/guides/using-monitoring-intf"" id="""">Dataflow Monitoring and Troubleshooting</a></li><li id=""""><a href=""https://cloud.google.com/dataflow/pricing"" id="""">Dataflow Pricing</a></li></ul>",FALSE,FALSE,,,,GCP-Compute-2163
Idle ECS Container Instances Due to ASG Minimum Capacity,idle-ecs-container-instances-due-to-asg-minimum-capacity,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4a02e839f6de1b60e7,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Akarsh Inuganti,aws,aws-ecs,inefficient-configuration,compute,"<p id="""">When ECS clusters are configured with an Auto Scaling Group that maintains a minimum number of EC2 instances (e.g., min = 1 or higher), the instances remain active even when there are no tasks scheduled. This leads to idle compute capacity and unnecessary EC2 charges.Instead, ECS Capacity Providers support target tracking scaling policies that can scale the ASG to zero when idle and automatically increase capacity when new tasks or services are scheduled. Failing to adopt this pattern results in persistent idle infrastructure and unnecessary costs in ECS environments that do not require always-on compute.</p>","<ul id=""""><li id="""">EC2 container instances in ECS are billed like regular EC2 instances — by the second, based on type and size</li><li id="""">ASGs that maintain a minimum capacity &gt; 0 will keep EC2 instances running even if no ECS services or tasks are scheduled</li><li id="""">ECS Capacity Providers can scale ASGs dynamically using target tracking, enabling min = 0 when idle</li></ul>","<ul id=""""><li id="""">Identify ECS clusters using EC2 launch type</li><li id="""">Check the associated ASG for a minimum capacity &gt; 0</li><li id="""">Analyze task scheduling patterns to confirm periods of inactivity</li><li id="""">Correlate EC2 uptime with lack of ECS service or task activity</li></ul>","<ul id=""""><li id="""">Configure an ECS Capacity Provider for the cluster and attach a target tracking scaling policy</li><li id="""">Set the ASG minimum capacity to 0 to allow scale-down during idle periods</li><li id="""">Ensure ECS services are configured with appropriate scaling triggers (e.g., CPU or memory utilization)</li><li id="""">Monitor for unintended delays when new tasks are scheduled to validate scaling responsiveness</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-capacity-providers.html\&quot;"" id="""">Amazon ECS Capacity Providers</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonECS/latest/userguide/asg-capacity-providers.html\"" id="""">Using Auto Scaling with ECS</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-7739</p>
Idle EMR Cluster Without Auto-Termination Policy,idle-emr-cluster-without-auto-termination-policy,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941ce31d040ea991ebbb,FALSE,FALSE,Sun Jun 22 2025 23:39:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Kyler Rupe,aws,aws-emr,inactive-resource,compute,"<p id="""">Amazon EMR clusters often run on large, multi-node EC2 fleets, making them costly to leave running unnecessarily. If a cluster becomes idle—no longer processing jobs—but is not terminated, it continues accruing EC2 and EMR service charges. Many teams forget to shut down clusters manually or leave them running for debugging, staging, or future job use. Without an auto-termination policy, this oversight leads to significant unnecessary spend.</p>","<p id="""">Billed based on the EC2 instances provisioned for the EMR cluster (by instance-hours), along with additional EMR service fees per instance-hour</p>","<ul id=""""><li id="""">Check for EMR clusters with long durations of idle time and no recent job activity</li><li id="""">Identify clusters with “IsIdle \= true” within CloudWatch logs for extended periods</li><li id="""">Review clusters that have completed their last step but remain active</li><li id="""">Evaluate whether auto-termination policies are defined for transient or single-use clusters</li><li id="""">Confirm whether clusters are persistent for valid operational reasons (e.g., interactive workloads, shared notebooks)</li></ul>","<ul id=""""><li id="""">Enable an auto-termination policy on EMR clusters that are intended to be short-lived or batch-oriented</li><li id="""">Review and shut down idle clusters that are no longer actively running jobs</li><li id="""">Educate data engineering teams on the cost implications of leaving clusters running</li><li id="""">Consider moving toward EMR Serverless or ephemeral workflows for transient workloads</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html"" id="""">EMR Monitoring with CloudWatch</a><br><a href=""https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-auto-termination-policy.html"" id="""">Auto-Termination Policy for EMR</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-1832</p>
Idle GKE Autopilot Clusters with Always-On System Overhead,idle-gke-autopilot-clusters-with-always-on-system-overhead,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894197bfcad97adf0ea60,FALSE,FALSE,Sun Jun 22 2025 23:39:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),,gcp,gcp-gke,inactive-resource-consuming-baseline-costs,compute,"<p id="""">Even when no user workloads are active, GKE Autopilot clusters continue running system-managed pods that accrue compute and storage charges. These include control plane components and built-in agents for observability and networking. If Autopilot clusters are deployed in non-production or experimental environments and left idle, they may silently accrue ongoing charges unrelated to application activity.  This inefficiency often occurs in:  * Dev/test clusters that are spun up temporarily but not deleted   * Clusters used for one-time jobs or training workloads   * Scheduled workloads that run infrequently but don't trigger downscaling</p>","<p id="""">* Billed per vCPU, memory, and ephemeral storage requested by running pods   * Baseline system pods (e.g., logging agents, kube-system) incur cost even with zero user workloads   * Clusters themselves do not scale to zero — an idle Autopilot cluster still incurs a minimum charge</p>","<ul id=""""><li id="""">Identify GKE Autopilot clusters with no active user-deployed workloads over a representative time window</li><li id="""">Confirm that cluster billing continues despite low or no pod activity</li><li id="""">Assess whether any scheduled workloads justify keeping the cluster online</li><li id="""">Review tagging or naming conventions to isolate non-production or experimental environments</li><li id="""">Evaluate whether clusters are still needed or can be replaced with on-demand environments (e.g., Cloud Run)</li></ul>","<ul id=""""><li id="""">Delete unused Autopilot clusters in dev, test, or sandbox environments</li><li id="""">Replace infrequently used workloads with serverless alternatives like Cloud Run or Cloud Functions</li><li id="""">Implement automation to tear down unused clusters after inactivity thresholds</li><li id="""">Consider switching to Standard mode with custom node pools if greater control over scaling and cost is needed</li></ul>","<p id=""""><a href=""https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview"" id="""">GKE Autopilot Overview</a><br><a href=""https://cloud.google.com/kubernetes-engine/pricing?hl=en#autopilot_mode"" id="""">GKE Pricing</a></p>",FALSE,FALSE,,,,<p>GCP-Compute-2813</p>
Idle Load Balancer,idle-load-balancer,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894172610ccf42b2e0093,FALSE,FALSE,Sun Jun 22 2025 23:39:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Dann Berg,gcp,gcp-load-balancers,idle-resource,networking,"<p id="""">Provisioned load balancers continue to generate costs even when they are no longer serving meaningful traffic. This often occurs when applications are decommissioned, testing infrastructure is left behind, or backend services are removed without deleting the associated frontend configurations. Without ingress or egress traffic, these load balancers offer no functional value but still consume billable resources, including forwarding rules and reserved external IPs.</p>","<p id="""">Load balancers in GCP incur ongoing costs based on provisioned forwarding rules, reserved IP addresses, and the amount of data processed—even if they are not actively routing traffic.</p>","<ul id=""""><li id="""">Identify load balancers with little to no ingress or egress traffic over a representative time period</li><li id="""">Review whether the load balancer is associated with active backend services or instance groups</li><li id="""">Check if associated IP addresses are actively used or referenced by DNS records</li><li id="""">Correlate creation or last-updated dates with known application lifecycle events (e.g., shutdowns or migrations)</li><li id="""">Confirm with application or network owners whether the load balancer still serves an active purpose</li></ul>","<ul id=""""><li id="""">Decommission load balancers that no longer serve traffic or lack associated backend services</li><li id="""">Release reserved IP addresses tied to unused load balancers</li><li id="""">Incorporate lifecycle tagging and auditing practices to flag test or temporary load balancers for removal</li><li id="""">Review usage regularly to avoid accumulating idle network resources</li></ul>","<p id="""">Google Cloud Load Balancing Pricing  Load Balancer Monitoring in Cloud Monitoring</p>",FALSE,FALSE,,,,<p>GCP-Networking-5478</p>
Imbalanced Data Transfer Between Availability Zones,imbalanced-data-transfer-between-availability-zones,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b41905575ca69313938,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-data-transfer,inefficient-architecture,networking,"<p id="""">Some architectures unintentionally route large volumes of traffic between resources that reside in different Availability Zones—such as database queries, service calls, replication, or logging. While these patterns may be functionally correct, they can lead to unnecessary data transfer charges when the traffic could be contained within a single AZ. Over time, this can become a silent cost driver, especially for chatty microservices, replicated storage layers, or high-throughput pipelines. Re-architecting for AZ-locality—when possible—can reduce these charges without affecting availability in environments where high resilience isn’t required.</p>","<p id="""">Data transferred between AWS resources is billed based on the source and destination of the traffic. While data transferred within the same Availability Zone is typically free, data transferred between Availability Zones within the same region incurs per-GB charges in both directions, even if the traffic stays within a single VPC. This pricing model applies across many AWS services, including EC2, RDS, and Load Balancers. Although inter-region traffic is charged at higher rates, cross-AZ transfer within a region is a common source of silent, avoidable cost, especially in architectures that weren’t designed with AZ-locality in mind.</p>","<ul id=""""><li id="""">Identify resources that receive or send high volumes of traffic to other Availability Zones within the same region</li><li id="""">Review VPC flow logs, CloudWatch metrics, or billing data to assess regional data transfer patterns</li><li id="""">Determine whether the resource acts as a centralized destination for data aggregation, storage, or processing</li><li id="""">Evaluate whether AZ locality can be preserved (e.g., by colocating compute and storage in the same AZ)</li><li id="""">Confirm that high availability requirements wouldn’t be compromised by reducing cross-AZ replication or communication</li><li id="""">Engage with application or infrastructure teams to assess whether architectural changes could reduce AZ-to-AZ traffic without impacting service goals</li></ul>","<p id="""">Where feasible, update the architecture to localize traffic within a single Availability Zone. This may include placing dependent resources (e.g., compute, cache, storage) in the same AZ, using zonal services, or avoiding unnecessary replication across zones. In cases where high availability is required, assess whether the added cost of cross-AZ traffic is justified, or whether more efficient patterns—such as reducing the volume or frequency of cross-AZ communication—can help contain costs without compromising resilience.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer"" id="""">AWS Data Transfer Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/vpc/latest/userguide/working-with-aws-networks.html"" id="""">Optimizing AWS Network Costs</a></li></ul>",FALSE,FALSE,,44,,<p>AWS-Networking-4131</p>
Inactive AWS WorkSpace,inactive-aws-workspace,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894169bf2d43807db05d6,FALSE,FALSE,Sun Jun 22 2025 23:39:02 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Taylor Houck,aws,aws-workspaces,inactive-resource,compute,"<p id="""">If an AWS WorkSpace has been provisioned but not accessed in a meaningful timeframe, it may represent waste—particularly if it is set to monthly billing. Many organizations leave WorkSpaces active for users who no longer need them or have shifted roles, leading to persistent charges without corresponding business value. Even in hourly mode, costs can accrue if WorkSpaces are left in a running state.</p>","<p id="""">Amazon WorkSpaces supports two primary billing options: <strong id="""">monthly</strong> (always-on) and <strong id="""">hourly (auto-stop)</strong>. In monthly mode, you are billed a flat rate regardless of usage. In hourly mode, billing is based on active usage with a lower base fee. Unused or inactive WorkSpaces in monthly mode result in unnecessary recurring charges.</p>","<ul id=""""><li id="""">Review the last login date for all provisioned WorkSpaces to identify users with prolonged inactivity</li><li id="""">Confirm whether inactive WorkSpaces are set to monthly billing (always-on) or hourly (auto-stop)</li><li id="""">Evaluate if inactive WorkSpaces are associated with users who have left the organization or changed roles</li><li id="""">Check whether a WorkSpace remains in a running state even in hourly mode, causing unnecessary hourly charges</li><li id="""">Review tagging metadata or directory assignments to determine if the WorkSpace is part of a temporary, staging, or unused environment</li></ul>","<ul id=""""><li id="""">Decommission WorkSpaces that are no longer needed</li><li id="""">Switch billing mode from monthly to auto-stop for WorkSpaces with intermittent usage</li><li id="""">Automate reviews of WorkSpaces based on last login data to flag stale resources for cleanup or conversion</li><li id="""">Use lifecycle management policies to standardize default billing modes and enforce deactivation schedules</li></ul>","<p id=""""><a href=""https://aws.amazon.com/workspaces/pricing/"" id="""">Amazon WorkSpaces Pricing</a><br><a href=""https://docs.aws.amazon.com/workspaces/latest/adminguide/usage.html"" id="""">Manage Amazon WorkSpaces Usage</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-2606</p>
Inactive AppStream Image Builder or App Block Builder Instances,inactive-appstream-image-builder-or-app-block-builder-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894143538fc9ed8f1fca6,FALSE,FALSE,Sun Jun 22 2025 23:39:00 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Jason DiDomenico,aws,aws-appstream-2-0,unused-resource,compute,"<p id="""">When AppStream builder instances are left running but unused, they continue to generate compute charges without delivering any value. These instances are commonly left active after configuration or image creation is completed but can be safely stopped or terminated when not in use. Identifying and decommissioning inactive builders helps reduce unnecessary compute costs.</p>","<p id="""">AppStream 2.0 Image Builder and App Block Builder instances are billed per hour while running, regardless of whether they are actively in use. Charges accrue continuously while the instance is in the running state, even if no users are connected or no configuration is taking place.</p>","<ul id=""""><li id="""">Identify AppStream Image Builder or App Block Builder instances that have remained in a running state without recent user connections or image-related activity</li><li id="""">Review historical usage patterns to determine if the instance was left on after setup or testing</li><li id="""">Check for absence of scheduled image builds, configuration updates, or automation tied to the builder</li><li id="""">Validate with application administrators whether the builder instance is still needed</li></ul>","<ul id=""""><li id="""">Stop or decommission builder instances that are no longer required.</li><li id="""">Implement an automated workflow—such as a scheduled Lambda function—that stops builder instances after a defined period of inactivity.</li><li id="""">Establish operational guidelines to ensure builder instances are shut down after image creation or testing tasks are completed.</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/appstream2/latest/developerguide/image-builder.html"" id="""">AppStream 2.0 Image Builder Overview</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-4680</p>
Inactive Application Load Balancer (ALB),inactive-application-load-balancer-alb,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b45ea589e71c845b1c7,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-elb,unused-resource,networking,"<p id="""">Application Load Balancers that no longer serve active workloads may persist after application migrations, architecture changes, or testing activities. When no incoming requests are processed through the ALB, it continues to generate baseline hourly and LCU charges. Identifying and decommissioning unused ALBs helps reduce networking expenses without impacting operational environments.</p>","<p id="""">Application Load Balancers are billed per hour of operation and per Load Balancer Capacity Unit (LCU) used. Charges continue to accrue even if the load balancer is not actively handling requests. Maintaining idle ALBs results in unnecessary ongoing costs without delivering operational value.</p>","<ul id=""""><li id="""">Identify Application Load Balancers with no active HTTP/HTTPS requests or minimal LCU consumption over a representative time period</li><li id="""">Confirm there are no listener rules, target groups, or backend services depending on the load balancer</li><li id="""">Review application dependencies, DNS records, and security group configurations to validate safe removal</li><li id="""">Check tagging and metadata to determine whether the ALB was associated with a temporary, testing, or deprecated environment</li><li id="""">Validate findings with application owners or infrastructure teams before deletion</li></ul>","<p id="""">If confirmed to be inactive, delete the load balancer through the AWS Console, CLI, or API to stop further billing. Where possible, document the reason for decommissioning and update tagging or inventory systems to prevent reaccumulation of unused resources.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\&quot;"" id="""">Application Load Balancer Documentation</a></li><li id=""""><a href=""https://aws.amazon.com/elasticloadbalancing/pricing/\"" id="""">Elastic Load Balancing Pricing</a></li></ul>",FALSE,FALSE,,48,,<p>AWS-Networking-8365</p>
Inactive Azure Load Balancer,inactive-azure-load-balancer,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b48f64ac241a1ad8d7a,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Mathijs Hendriks,azure,azure-load-balancer,unused-resource,networking,"<p id="""">In dynamic environments — especially during autoscaling, testing, or infrastructure changes — it's common for load balancers to remain provisioned after their backend resources have been decommissioned. When this happens, the load balancer continues to incur hourly charges despite serving no functional purpose. These inactive resources often go unnoticed, particularly in dev/test environments or when deployment pipelines fail to include proper cleanup logic. Over time, the accumulation of unused load balancers contributes to unnecessary recurring costs with no operational benefit.</p>","<p id="""">Azure Load Balancer (Standard SKU) is billed based on:</p><ul id=""""><li id="""">Per-rule hourly charge — billed continuously, regardless of backend activity</li><li id="""">Data processing charges — billed per GB, if traffic is present</li></ul><p id="""">Cost persists even when there are no active backend pool members, and no traffic is flowing.</p>","<ul id=""""><li id="""">Identify Azure Load Balancers with empty backend address pools</li><li id="""">Verify that there are no associated virtual machines or scale sets</li><li id="""">Check network traffic metrics to confirm zero inbound or outbound flow</li><li id="""">Cross-reference with naming conventions or tags to flag decommissioned environments</li></ul>","<ul id=""""><li id="""">Delete Azure Load Balancers that have no backend pool members and no observed traffic</li><li id="""">Implement automation or tagging policies to detect and flag inactive networking resources</li><li id="""">Update infrastructure-as-code or deployment scripts to ensure load balancers are removed alongside their dependent compute resources</li></ul>","<p id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/load-balancer/"" id="""">Azure Load Balancer Pricing</a></p>",FALSE,FALSE,,,,
Inactive Azure Load Balancer,inactive-azure-load-balancer-0fe41,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4af7704f33ce49cc57,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Anderson Oliveira,azure,azure-load-balancer,unused-resource,networking,"<p id="""">Standard Load Balancers are frequently provisioned for internal services, internet-facing applications, or testing environments. When a workload is decommissioned or moved, the load balancer may be left behind without any active backend pool or traffic — but continues to incur hourly charges for each frontend IP configuration.Because Azure does not automatically remove or alert on inactive load balancers, and because they may not show significant outbound traffic, these resources often persist unnoticed. In dynamic or multi-team environments, this can result in a growing number of unused Standard Load Balancers generating silent, recurring costs.</p>","<ul id=""""><li id="""">Standard SKU incurs hourly charges per front-end IP configuration, regardless of traffic or backend association</li><li id="""">Additional per-GB data processing charges apply to outbound traffic routed through the load balancer</li><li id="""">Basic SKU is free but supports only limited functionality and scope</li></ul>","<ul id=""""><li id="""">Review whether the load balancer has any backend pool associations</li><li id="""">Assess whether any virtual machines or scale sets are currently linked to the backend pool</li><li id="""">Evaluate if the load balancer is receiving or routing any traffic over a representative time period</li><li id="""">Determine whether the resource was provisioned for a workload that has since been decommissioned</li><li id="""">Confirm whether the load balancer serves a current business or operational purpose</li></ul>","<ul id=""""><li id="""">Delete load balancers that have no active backend pool and are no longer needed</li><li id="""">Review associated resources (e.g., front-end IP configurations, probes, rules) to ensure they can be safely removed</li><li id="""">Establish tagging or documentation standards to track ownership and intended usage</li><li id="""">Incorporate load balancer cleanup into deprovisioning and environment hygiene workflows</li></ul>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/load-balancer/\&quot;"" id="""">Azure Load Balancer Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics\"" id="""">Azure Load Balancer Diagnostics and Metrics</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Networking-9868</p>
Inactive Blobs in Storage Account,inactive-blobs-in-storage-account,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b41d364e73fcbd4345b,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,azure,azure-blob-storage,inefficient-configuration,storage,"<p id="""">Storage accounts can accumulate blob data that is no longer actively accessed—such as legacy logs, expired backups, outdated exports, or orphaned files. When these blobs remain in the Hot tier, they continue to incur the highest storage cost, even if they have not been read or modified for an extended period. Without lifecycle management in place, these inactive blobs often go unnoticed and accumulate cost. In many cases, the data could be safely transitioned to a lower-cost tier or deleted altogether, depending on retention needs. Additionally, misconfigured default tier settings at the account or container level can cause even new uploads to be stored in the Hot tier unnecessarily. Azure lifecycle transitions do not incur additional fees, making automation a low-risk optimization method.</p>","<p id="""">Azure Blob Storage is billed based on a combination of:</p><ul id=""""><li id="""">Storage used (per GB/month) — Charges vary by access tier:</li><li id="""">Hot (highest storage cost, lowest access cost)</li><li id="""">Cool (lower storage cost, higher access and operation charges)</li><li id="""">Archive (lowest storage cost, highest retrieval and rehydration cost, with delayed access)</li><li id="""">Data access and operations — Read/write operations and metadata access are billed separately and increase in cost for cooler tiers</li><li id="""">Early deletion charges — Cool and Archive tiers have minimum retention durations (e.g., 30 days for Cool, 180 days for Archive), with penalties for early deletion</li><li id="""">Rehydration — Retrieving data from Archive to an online tier (Hot or Cool) incurs a separate per-GB retrieval charge</li></ul>","<ul id=""""><li id="""">Identify storage accounts with large amounts of data in the Hot tier</li><li id="""">Analyze blob-level access patterns using logs or metrics to confirm that data has not been read or written over a defined lookback period</li><li id="""">Determine whether the data is still relevant to any active workload, process, or compliance requirement</li><li id="""">Check whether lifecycle management rules are configured to transition or delete stale data, and whether they are actively being applied</li><li id="""">Review the default access tier setting for each storage account or container to identify misaligned defaults (e.g., Hot for archival use cases)</li><li id="""">Engage with application or data owners to validate whether inactive blobs can be tiered or deleted without impact</li></ul>","<p id="""">For inactive data that must be retained, transition blobs to lower-cost tiers such as Cool or Archive using Azure Blob lifecycle policies. If the data is no longer needed, delete it to eliminate costs. Review and enforce lifecycle rules across storage accounts to automate ongoing management. Ensure that default access tier settings are aligned with data usage patterns to prevent accumulation of new blobs in the wrong tier.</p>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/storage/blobs/"" id="""">Azure Blob Storage Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview"" id="""">Azure Blob Lifecycle Management</a></li></ul>",FALSE,FALSE,,118,,<p>Azure-Storage-8957</p>
Inactive Classic Load Balancer (CLB),inactive-classic-load-balancer-clb,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b453664b2e5e107c303,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-elb,unused-resource,networking,"<p id="""">Classic Load Balancers that no longer serve active workloads will persist if they are not properly decommissioned. This often happens after application migrations, architecture changes, or testing activities. Even if no connections or traffic are passing through the CLB, it continues to incur baseline charges until manually deleted. Identifying and removing unused load balancers helps eliminate waste without impacting operations.</p>","<p id="""">Classic Load Balancers are billed per hour of operation and per GB of data processed, whether or not they are actively handling traffic. Inactive CLBs that continue to run without handling meaningful traffic still accrue hourly charges, resulting in unnecessary ongoing costs.</p>","<ul id=""""><li id="""">Identify Classic Load Balancers with no active connections or data transfer over a representative time period</li><li id="""">Confirm there are no health checks, listener rules, or target instances relying on the load balancer</li><li id="""">Review application and infrastructure dependencies to ensure decommissioning will not disrupt services</li><li id="""">Check metadata, tags, or configuration history to determine whether the CLB was associated with a temporary or deprecated workload</li><li id="""">Validate findings with application owners or system architects before deletion</li></ul>","<p id="""">If confirmed to be inactive, delete the load balancer through the AWS Console, CLI, or API to stop further billing. Where possible, document the reason for decommissioning and update tagging or inventory systems to prevent reaccumulation of unused resources.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/introduction.html\&quot;"" id="""">Classic Load Balancer Documentation</a></li><li id=""""><a href=""https://aws.amazon.com/elasticloadbalancing/pricing/\"" id="""">Elastic Load Balancing Pricing</a></li></ul>",FALSE,FALSE,,45,,<p>AWS-Networking-3251</p>
Inactive CloudWatch Log Group,inactive-cloudwatch-log-group,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b3f866414fe44b72715,FALSE,FALSE,Thu May 22 2025 14:57:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-cloudwatch,unused-resource,other,"<p id="""">CloudWatch log groups often persist long after their usefulness has expired. In some cases, they are associated with applications or resources that are no longer active. In other cases, the systems may still be running, but the log data is no longer being reviewed, analyzed, or used by any team. Regardless of the reason, retaining logs that no one is monitoring or using results in unnecessary storage costs. If log data is not needed for operational visibility, debugging, compliance, or auditing purposes, it should either be deleted or managed with a shorter retention policy.</p>","<p id="""">CloudWatch Logs are billed on a usage-based model, with charges incurred for:</p><ul id=""""><li id="""">Log ingestion – Cost per GB of log data sent to CloudWatch</li><li id="""">Log storage – Ongoing cost per GB-month for data retained in log groups, regardless of access or usage</li><li id="""">Log queries and insights – Additional charges apply when querying or analyzing logs</li></ul>","<ul id=""""><li id="""">Identify log groups with significant stored log volume but no recent ingestion activity</li><li id="""">Review historical usage to determine if log data is still being used for operational, security, or compliance purposes</li><li id="""">Evaluate whether the log group is associated with an active application or AWS resource</li><li id="""">Check whether the log group has a defined retention policy or is defaulting to indefinite storage</li><li id="""">Confirm there are no current alerts, dashboards, or queries relying on data within the log group</li><li id="""">Validate with relevant stakeholders (e.g., DevOps, security, compliance) whether the log group is still required</li></ul>","<p id="""">Apply appropriate retention policies to automatically expire old logs or manually delete inactive log groups that are no longer needed. Where appropriate, implement automated cleanup routines to routinely identify and remove unused log groups.</p>","<p id=""""><a href=""https://aws.amazon.com/cloudwatch/pricing/\&quot;"" id="""">CloudWatch Pricing</a></p>",FALSE,FALSE,,37,,<p>AWS-Other-8098</p>
Inactive DMS Replication Instance,inactive-dms-replication-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4ad1ac1e662f805476,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Tom Cross,aws,aws-dms,orphaned-resource,databases,"<p id="""">Replication instances are commonly left running after migration tasks are completed, especially when DMS is used for one-time or project-based migrations. Without active replication tasks, these instances no longer serve any purpose but continue to incur hourly compute costs. In large organizations or decentralized migration projects, these idle instances may go unnoticed, contributing to persistent background spend.</p>","<ul id=""""><li id="""">DMS Replication Instances are charged per hour of uptime, based on instance size</li><li id="""">Charges accrue even if no replication tasks are actively running</li><li id="""">Additional storage charges may apply for logs and cached data, but the bulk of cost comes from the instance itself</li></ul>","<ul id=""""><li id="""">List all DMS replication instances in the account or region</li><li id="""">Check whether each instance has active replication tasks associated with it</li><li id="""">Flag instances with zero tasks or tasks in a stopped or completed state</li><li id="""">Review instance creation and last activity timestamps to identify long-idle resources</li></ul>","<ul id=""""><li id="""">Stop and delete DMS replication instances that are no longer associated with active or planned tasks</li><li id="""">Tag replication instances by project or migration wave to enable future clean-up</li><li id="""">Incorporate DMS instance lifecycle checks into post-migration workflows</li><li id="""">Use infrastructure-as-code where possible to ensure temporary resources are cleaned up automatically</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/dms/pricing/\&quot;"" id="""">AWS DMS Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html\"" id="""">Monitoring AWS DMS Tasks and Instances</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Databases-4018</p>
Inactive DynamoDB Table,inactive-dynamodb-table,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b44ec1d6334aa695c31,FALSE,FALSE,Thu May 22 2025 14:57:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),,aws,aws-dynamodb,unused-resource,databases,"<p id="""">This inefficiency occurs when a DynamoDB table is no longer accessed by any active workload but continues to accumulate storage charges. These tables often remain after a project ends, a feature is retired, or data is migrated elsewhere. Without any read or write activity, the table provides no functional value and becomes a cost liability.</p>","<p id="""">DynamoDB charges for data storage (per GB-month) and for read/write activity (either per request or via provisioned capacity). Tables incur storage charges continuously, regardless of usage.</p>","<ul id=""""><li id="""">Identify tables with zero read or write activity over a defined lookback period (e.g. 7, 14, 30+ days)</li><li id="""">Confirm no dependencies exist from applications, analytics jobs, backup processes, or event streams</li><li id="""">Check metadata (tags, naming, creation date) to determine purpose and ownership</li><li id="""">Validate with data owners whether the table’s content must be retained for compliance or reference</li></ul>","<p id="""">Delete the table if it’s no longer needed. If data retention is required, export the table contents (e.g., to S3) before deletion. Use lifecycle tagging or ownership metadata to track tables going forward and avoid orphaned storage. For tables that may become inactive again in the future, consider archiving the data or restructuring workflows to avoid indefinite retention.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/dynamodb/pricing/\&quot;"" id="""">DynamoDB Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ManagingTables.html\"" id="""">Managing Tables in DynamoDB</a></li></ul>",FALSE,FALSE,,86,,<p>AWS-Databases-4839</p>
Inactive EC2 Instance,inactive-ec2-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b431b1adbc6d1f05406,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-ec2,unused-resource,compute,"<p id="""">This inefficiency occurs when an EC2 instance remains in a running state but is not actively utilized. These instances may be remnants of past projects, forgotten development environments, or temporarily created for testing and never decommissioned. If an instance shows consistently low or no CPU, network, or disk activity—and no active connections—it likely serves no operational purpose but continues to generate ongoing compute and storage charges.</p>","<p id="""">EC2 instances are billed per second (Linux) or per hour (Windows/legacy types) while in the running state, based on instance type and size. Charges apply continuously unless the instance is stopped or terminated. Associated resources like EBS volumes, Elastic IPs, and Load Balancers may continue to incur charges even after the instance is stopped.</p>","<ul id=""""><li id="""">Identify EC2 instances that have been running throughout the lookback period</li><li id="""">Review CPU utilization, network throughput, and disk activity for sustained inactivity</li><li id="""">Check for the absence of inbound or outbound connections over the same period</li><li id="""">Evaluate whether the instance is part of a deprecated or paused workload</li><li id="""">Confirm with application or infrastructure owners whether the instance is still required</li></ul>","<p id="""">Decommission the instance after confirming it is no longer needed. Prior to termination, consider taking a snapshot or image if the data must be retained. Delete or clean up associated resources such as EBS volumes, Elastic IPs, and security groups. Schedule regular audits to catch inactive compute resources early.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/\&quot;"" id="""">EC2 Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-overview.html\"" id="""">Monitoring EC2 Instances</a></li></ul>",FALSE,FALSE,,128,,<p>AWS-Compute-6784</p>
Inactive EKS Cluster,inactive-eks-cluster,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b46d521f398e9e19a73,FALSE,FALSE,Thu May 22 2025 14:57:10 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-eks,unused-resource,compute,"<p id="""">Clusters that no longer run active workloads but remain provisioned continue incurring hourly control plane costs and may also maintain associated infrastructure like node groups or VPC components. Inactive clusters often persist after environment decommissioning, project shutdowns, or migrations. Decommissioning unused clusters eliminates unnecessary operational costs and simplifies infrastructure management.</p>","<p id="""">EKS control planes are billed per cluster per hour, regardless of whether any workloads are actively running. In addition, any underlying compute resources (EC2 nodes or Fargate tasks) continue to accrue charges separately. Maintaining unused EKS clusters results in ongoing hourly billing even when no active workloads are present.</p>","<ul id=""""><li id="""">Identify EKS clusters with no active Deployments, StatefulSets, DaemonSets, CronJobs, or running pods over a representative time window</li><li id="""">Review node group activity and verify whether any EC2 instances or Fargate tasks are currently attached to the cluster</li><li id="""">Analyze cluster API server logs or CloudWatch metrics to confirm minimal API usage and cluster activity</li><li id="""">Check tagging, cluster creation metadata, and documentation to assess whether the cluster was associated with a deprecated project or environment</li><li id="""">Validate findings with infrastructure owners, DevOps teams, or platform engineering groups before decommissioning</li></ul>","<p id="""">Decommission EKS clusters confirmed to be inactive by deleting the cluster through the AWS Console, CLI, or API. Ensure that any dependent resources, such as node groups, VPCs, security groups, or service-linked roles, are also reviewed for cleanup. If a lightweight future usage model is anticipated, consider rearchitecting with AWS EKS Fargate profiles to avoid managing EC2 instances directly.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\&quot;"" id="""">Amazon EKS Documentation</a></li><li id=""""><a href=""https://docs.aws.amazon.com/eks/latest/userguide/fargate.html\"" id="""">AWS EKS Fargate Overview</a></li></ul>",FALSE,FALSE,,97,,<p>AWS-Compute-6372</p>
Inactive Files in Storage Account,inactive-files-in-storage-account,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b46cb53594d62ed7e9b,FALSE,FALSE,Thu May 22 2025 14:57:10 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),,azure,azure-blob-storage,unused-resource,storage,"<p id="""">Files that show no read or write activity over an extended period often indicate redundant or abandoned data. Keeping inactive files in higher-cost storage classes unnecessarily increases monthly spend. Implementing proactive archiving, deletion workflows, and safety features like Blob Soft Delete or Versioning improves cost efficiency while protecting against accidental data loss.</p>","<p id="""">Azure Storage charges based on the volume of data stored per GB, with tiered pricing across Hot, Cool, and Archive classes. Stored data continues to incur charges regardless of access frequency unless automated lifecycle policies or clean-up processes are applied.</p>","<ul id=""""><li id="""">Identify storage accounts or containers containing blobs with no reads or modifications over a defined lookback period</li><li id="""">Analyze blob access logs and object metadata to validate inactivity</li><li id="""">Review creation timestamps, tags, and business ownership metadata to assess ongoing relevance</li><li id="""">Check for compliance, legal hold, or retention requirements before deletion or archival</li><li id="""">Validate findings with data owners, application teams, or governance stakeholders before proceeding</li></ul>","<p id="""">Delete redundant files and archive infrequently accessed data by transitioning to Cool or Archive tiers. Enable Blob Soft Delete or Versioning to safeguard deletions during the clean-up process. Establish regular audits of inactive storage to maintain a lean and cost-optimized environment.</p>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction\&quot;"" id="""">Azure Blob Storage Overview</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/storage/blobs/\"" id="""">Azure Storage Pricing</a></li></ul>",FALSE,FALSE,,119,,<p>Azure-Storage-8736</p>
Inactive GCS Bucket,inactive-gcs-bucket,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b40ece527739c06c01f,FALSE,FALSE,Thu May 22 2025 14:57:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,gcp,gcp-gcs,unused-resource,storage,"<p id="""">GCS buckets often persist after applications are retired or data is no longer in active use. Without access activity, these buckets generate storage charges without providing ongoing value. Leaving stale data in Standard storage—designed for frequent access—results in unnecessary cost. If the data must be retained for compliance or future reference, colder tiers offer substantial savings. If it is no longer needed, the data should be deleted.</p>","<p id="""">GCS charges are based on:</p><ul id=""""><li id="""">Storage usage (per GB/month), with rates varying by storage class (e.g., Standard, Nearline, Coldline, Archive)</li><li id="""">Data retrieval fees for Nearline, Coldline, and Archive classes</li><li id="""">Operation charges for requests like reads, writes, and deletes</li><li id="""">Minimum storage durations for colder tiers (e.g., 30 days for Nearline, 90 for Coldline, 365 for Archive)</li></ul><p id="""">Buckets accrue charges for all stored data, regardless of access frequency. Even when inactive, the data continues to generate storage and metadata costs.</p>","<ul id=""""><li id="""">Identify GCS buckets that have had no read or write activity over a representative lookback period</li><li id="""">Review object access logs and storage metrics to confirm inactivity</li><li id="""">Assess whether the bucket is tied to any active workload, automated workflow, or scheduled task</li><li id="""">Determine whether the data must be retained for compliance, legal hold, or long-term archival</li><li id="""">Check for the presence (or absence) of lifecycle management policies to transition or delete stale data</li><li id="""">Coordinate with data owners or teams responsible for the bucket to confirm its relevance and required retention</li></ul>","<p id="""">Delete buckets that are no longer needed to eliminate ongoing storage charges. For data that must be retained but is rarely accessed, transition objects to colder storage classes like Nearline, Coldline, or Archive. Enable lifecycle policies to automatically transition or expire inactive data based on age or last access timestamp.</p>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/storage/pricing\&quot;"" id="""">GCS Pricing</a></li><li id=""""><a href=""https://cloud.google.com/storage/docs/lifecycle\"" id="""">GCS Lifecycle Management</a></li></ul>",FALSE,FALSE,,106,,<p>GCP-Storage-5248</p>
Inactive Gateway Load Balancer (GLB),inactive-gateway-load-balancer-glb,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b45ac6f9746d211b7a4,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-elb,unused-resource,networking,"<p id="""">Gateway Load Balancers that no longer have active traffic flows can continue to exist indefinitely unless proactively decommissioned. This often happens after network topology changes, security architecture updates, or environment deprecations. Without active packet forwarding, the GLB provides no functional benefit but still incurs hourly and data transfer costs.</p>","<p id="""">Gateway Load Balancers are billed per hour of operation and per GB of data processed. Charges continue to accrue even if the load balancer is not actively forwarding traffic. Maintaining idle GLBs results in unnecessary ongoing costs without delivering operational value.</p>","<ul id=""""><li id="""">Identify Gateway Load Balancers with no active traffic or minimal packet forwarding over a representative time window</li><li id="""">Confirm there are no attached target appliances or ongoing inspection flows depending on the load balancer</li><li id="""">Review networking configurations, route tables, and security group dependencies to validate safe removal</li><li id="""">Check tagging and documentation to identify whether the GLB was associated with a retired or migrated environment</li><li id="""">Validate findings with security or network operations teams before deletion</li></ul>","<p id="""">If confirmed to be inactive, delete the load balancer through the AWS Console, CLI, or API to stop further billing. Where possible, document the reason for decommissioning and update tagging or inventory systems to prevent reaccumulation of unused resources.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html\&quot;"" id="""">Gateway Load Balancer Documentation</a></li><li id=""""><a href=""https://aws.amazon.com/elasticloadbalancing/pricing/\"" id="""">Elastic Load Balancing Pricing</a></li></ul>",FALSE,FALSE,,46,,<p>AWS-Networking-1244</p>
Inactive Kubernetes Workload,inactive-kubernetes-workload,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b463674d922d5950c05,FALSE,FALSE,Thu May 22 2025 14:57:10 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-eks,unused-resource,compute,"<p id="""">Workloads with consistently low CPU and memory usage may no longer serve active traffic or scheduled tasks, but continue reserving resources within the cluster. These idle deployments often remain after project migrations, feature deprecations, or experimentation. Removing inactive workloads allows node groups to scale down, reducing infrastructure costs without impacting active services.</p>","<p id="""">EKS control planes are billed hourly, and compute resources (EC2 nodes or Fargate) are billed separately based on usage. Kubernetes workloads that reserve CPU and memory continue consuming node capacity, even if they are idle, driving unnecessary compute costs.</p>","<ul id=""""><li id="""">Identify Kubernetes workloads (Deployments, StatefulSets, DaemonSets, or CronJobs) with minimal CPU and memory usage over a representative time window</li><li id="""">Review service exposure, pod restart patterns, and ingress configurations to validate inactivity</li><li id="""">Assess workload labels, annotations, and namespaces to determine original ownership and purpose</li><li id="""">Check application monitoring or logs to verify that no meaningful interactions are occurring</li><li id="""">Confirm findings with application owners or service stakeholders before proceeding with removal</li></ul>","<p id="""">Delete workloads confirmed to be inactive to reclaim node capacity and reduce cluster size. Review associated Kubernetes resources such as Services, ConfigMaps, and Persistent Volume Claims to ensure complete cleanup. Incorporate tagging and lifecycle management practices to better track workload ownership and reduce future accumulation of unused resources.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\&quot;"" id="""">Amazon EKS Documentation</a></li><li id="""">Kubernetes Workloads Overview</li></ul>",FALSE,FALSE,,63,,<p>AWS-Compute-8474</p>
Inactive Memorystore Instance,inactive-memorystore-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941ab3181605d2055f1a,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,gcp,gcp-memorystore,inactive-resource,databases,"<p id="""">Memorystore instances that are provisioned but unused — whether due to deprecated services, orphaned environments, or development/testing phases ending — continue to incur memory and infrastructure charges.  Because usage-based metrics like client connections or cache hit ratios are not tied to billing, an idle instance costs the same as a heavily used one. This makes it critical to identify and decommission inactive caches.</p>","<p id="""">Billed based on:  * Provisioned memory capacity per hour   * Network egress (if applicable)   * Optional HA configurations incur additional cost  Charges accrue even if the instance is not actively serving traffic or receiving connections.</p>","<ul id=""""><li id="""">Identify instances with zero or near-zero connected clients over time</li><li id="""">Evaluate cache hit ratio and operation throughput (gets/sets) for inactivity</li><li id="""">Check if the instance is tied to an application still in use</li><li id="""">Review environments (e.g., test, staging) for abandoned or duplicated caches</li><li id="""">Assess whether data stored in the cache is static or outdated</li></ul>","<ul id=""""><li id="""">Decommission inactive or obsolete Memorystore instances</li><li id="""">Consolidate fragmented caching layers across services or environments</li><li id="""">Use automated tagging and monitoring to flag long-idle instances</li><li id="""">For intermittent workloads, consider re-creating the cache on demand via IaC or CI/CD pipelines</li></ul>","<p id="""">* Memorystore Pricing  * Monitoring Redis Instances  * Best Practices for Memorystore</p>",FALSE,FALSE,,,,<p>GCP-Databases-7888</p>
Inactive NAT Gateway,inactive-nat-gateway,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b41f77ba902a30b3181,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-nat-gateway,unused-resource,networking,"<p id="""">NAT Gateways are frequently left running after environments are re-architected, workloads are shut down, or connectivity patterns change. In many cases, they continue to incur hourly charges despite no active traffic flowing through them. Because hourly fees are not tied to whether the gateway is needed—just whether it exists—these resources can quietly drive recurring costs without delivering ongoing value. Identifying and removing unused gateways is a simple way to reduce waste.</p>","<p id="""">NAT Gateway charges include:</p><ul id=""""><li id="""">Hourly cost per deployed gateway per Availability Zone — charged as long as the gateway exists, regardless of usage</li><li id="""">Per-GB data processing fee — charged for all data passing through the NAT Gateway</li></ul><p id="""">There is no minimum usage threshold. Even an idle NAT Gateway accrues full hourly charges.</p>","<ul id=""""><li id="""">List all NAT Gateways currently provisioned in each region</li><li id="""">Review flow logs, CloudWatch metrics, or billing data to confirm whether any data has been processed through the gateway during the lookback period</li><li id="""">Validate that no private subnet or route table is actively routing traffic through the NAT Gateway</li><li id="""">Determine whether the gateway was created as part of a dev, staging, or legacy environment that is no longer active</li><li id="""">Confirm with network or infrastructure owners whether the gateway is still required for any known process or dependency</li></ul>","<p id="""">Delete NAT Gateways that have shown no data transfer activity and are no longer required. Review associated route tables to ensure removal will not disrupt network connectivity. Consider automating periodic audits of NAT Gateway usage across environments.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/vpc/pricing/#nat_gateway"" id="""">AWS NAT Gateway Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"" id="""">Working with NAT Gateways</a></li></ul>",FALSE,FALSE,,13,,<p>AWS-Networking-7739</p>
Inactive Network Load Balancer (NLB),inactive-network-load-balancer-nlb,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b45d364e73fcbd437a2,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-elb,unused-resource,networking,"<p id="""">Network Load Balancers that are no longer needed often persist after architecture changes, service decommissioning, or migration projects. When no active TCP connections or traffic flow through the NLB, it still generates hourly operational costs. Identifying and removing these idle resources helps reduce unnecessary networking expenses without affecting service availability.</p>","<p id="""">Network Load Balancers are billed per hour of operation and per GB of data processed. Inactive NLBs that remain provisioned without traffic continue to incur hourly charges, even when no connections are being handled.</p>","<ul id=""""><li id="""">Identify Network Load Balancers with no active connections or minimal data processing over a defined monitoring window</li><li id="""">Confirm there are no registered targets, listener rules, or backend services depending on the NLB</li><li id="""">Review networking and security configurations to ensure the load balancer is not being used for future failover or redundancy scenarios</li><li id="""">Check tagging and documentation history to assess whether the NLB was tied to a deprecated workload or temporary environment</li><li id="""">Validate findings with infrastructure or application teams before initiating removal</li></ul>","<p id="""">If confirmed to be inactive, delete the load balancer through the AWS Console, CLI, or API to stop further billing. Where possible, document the reason for decommissioning and update tagging or inventory systems to prevent reaccumulation of unused resources.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\&quot;"" id="""">Network Load Balancer Documentation</a></li><li id=""""><a href=""https://aws.amazon.com/elasticloadbalancing/pricing/\"" id="""">Elastic Load Balancing Pricing</a></li></ul>",FALSE,FALSE,,47,,<p>AWS-Networking-7760</p>
Inactive Object Storage Bucket,inactive-object-storage-bucket,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894174c7041f2f2bd3537,FALSE,FALSE,Sun Jun 22 2025 23:39:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),,oci,oci-object-storage,inactive-storage-resource,storage,"<p id="""">OCI Object Storage buckets accrue charges based on data volume stored, even if no activity has occurred. Buckets that haven't been read from or written to in months may contain outdated data or artifacts from discontinued projects.</p>","<p id="""">Charged per GB stored per month, regardless of access frequency.</p>","<ul id=""""><li id="""">Review access logs or object metadata to identify buckets with no recent read/write activity</li><li id="""">Confirm whether the bucket serves an archival or active workload purpose</li><li id="""">Evaluate organizational retention policies to determine if long-term storage is required</li><li id="""">Check for object versioning or lifecycle policies that may be retaining stale data</li></ul>","<ul id=""""><li id="""">Archive or delete data from inactive buckets after stakeholder confirmation</li><li id="""">Apply lifecycle rules to transition or expire infrequently accessed data</li><li id="""">Migrate cold data to OCI Archive Storage for reduced cost</li><li id="""">Delete unused buckets to stop further storage accrual</li></ul>","<p id=""""><a href=""https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/managingbuckets.htm"" id="""">Managing Buckets in Object Storage</a><br><a href=""https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/usinglifecyclepolicies.htm"" id="""">Object Lifecycle Policies</a></p>",FALSE,FALSE,,,,<p>OCI-Storage-5856</p>
Inactive RDS Cluster,inactive-rds-cluster,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b42d036f316acffa2e9,FALSE,FALSE,Thu May 22 2025 14:57:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-rds,unused-resource,databases,"<p id="""">This inefficiency occurs when an RDS cluster remains provisioned but is no longer serving any workloads and has no active database connections. Unlike underutilized resources, these clusters are completely idle—showing no query activity, background processing, or usage over time. They often persist in dev, staging, or legacy environments where the workload has been retired or moved, yet the cluster remains active and continues to generate ongoing compute and storage costs.</p>","<p id="""">RDS clusters are billed per hour based on the instance types used for each node (primary and replicas), along with additional charges for storage, I/O, backups, monitoring, and data transfer. Charges continue to accrue even if the cluster is idle or not serving traffic.</p>","<ul id=""""><li id="""">Identify RDS clusters with no active connections or query activity during the lookback period</li><li id="""">Confirm whether the cluster is receiving any traffic from applications or internal services</li><li id="""">Check for sustained low or zero CPU utilization, network throughput, and read/write IOPS</li><li id="""">Evaluate whether the cluster is part of a dev/test environment that is no longer in use</li><li id="""">Consult application and database owners to confirm that the cluster is no longer needed</li></ul>","<p id="""">Decommission the RDS cluster after confirming it is no longer in use. Take a final snapshot if the data may be needed later, and ensure that associated backups and read replicas are also deleted if not required. Incorporate regular reviews of cluster activity to identify and clean up idle infrastructure.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/rds/pricing/\&quot;"" id="""">RDS Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\"" id="""">RDS Monitoring</a></li></ul>",FALSE,FALSE,,105,,<p>AWS-Databases-9353</p>
Inactive RDS Instance,inactive-rds-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b44bfeaba01c68128eb,FALSE,FALSE,Thu May 22 2025 14:57:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-rds,unused-resource,databases,"<p id="""">This inefficiency occurs when an RDS instance remains in the running state but is no longer actively serving application traffic. These instances may be remnants of retired applications, paused development environments, or workloads that were migrated elsewhere. If an instance shows no active connections and sustained inactivity across CPU and memory metrics, it is likely idle and generating unnecessary costs.</p>","<p id="""">RDS instances are billed based on the selected instance class (vCPU and memory), per hour (or per second in some cases) while running. Additional charges may apply for storage, backup retention, Multi-AZ replication, and provisioned IOPS—regardless of workload activity.</p>","<ul id=""""><li id="""">Identify RDS instances that have been running continuously during the lookback period</li><li id="""">Review performance metrics to confirm low or zero CPU and memory usage</li><li id="""">Check connection logs and query metrics to validate the absence of active database traffic</li><li id="""">Confirm with owners or application teams whether the instance is still in use</li><li id="""">Evaluate whether snapshots or backups exist that could support future recovery needs</li></ul>","<p id="""">Shut down or delete the inactive instance after validating that it is no longer required. Take a snapshot if data retention is necessary. Review and remove associated resources such as storage, backups, or read replicas to fully eliminate residual costs.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/rds/pricing/\&quot;"" id="""">RDS Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\"" id="""">Monitoring RDS with Amazon CloudWatch</a></li></ul>",FALSE,FALSE,,41,,<p>AWS-Databases-3583</p>
Inactive RDS Read Replica,inactive-rds-read-replica,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b40d036f316acffa23c,FALSE,FALSE,Thu May 22 2025 14:57:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-rds,unused-resource,databases,"<p id="""">Read replicas are intended to improve performance for read-heavy workloads or support cross-region redundancy. However, it's common for replicas to remain in place even after their intended purpose has passed. In some cases, they were provisioned for scaling or analytics workloads that no longer exist; in others, they are tied to live environments but not actively receiving queries. Since each replica incurs full RDS costs, retaining one that is no longer used leads to unnecessary ongoing expenses.</p>","<p id="""">Amazon RDS pricing is based on several components:</p><ul id=""""><li id="""">Compute – Billed hourly based on the instance type and size of each database instance, including read replicas</li><li id="""">Storage – Charged per GB provisioned, regardless of usage</li><li id="""">I/O Operations – Additional charges for reads and writes, depending on storage type</li><li id="""">Data Transfer – Especially relevant for cross-region replication</li></ul><p id="""">Read replicas are billed as independent RDS instances, meaning they incur the same base costs as a standard instance—whether or not they are actively serving traffic.</p>","<ul id=""""><li id="""">Identify all existing RDS read replicas and their associated primary instances</li><li id="""">Assess whether read traffic is being actively routed to each replica</li><li id="""">Review recent query activity to determine if the replica is used for reporting, analytics, or scaling</li><li id="""">Evaluate whether the replica is still needed for its original purpose (e.g., disaster recovery, cross-region availability, performance optimization)</li><li id="""">Confirm there are no active failover configurations or application connections relying on the replica</li><li id="""">Validate with relevant teams (e.g., DBA, application owners) that decommissioning the replica will not impact availability, performance, or redundancy</li></ul>","<p id="""">If a read replica is no longer receiving queries and has no clear operational or redundancy purpose, it should be deleted to avoid unnecessary compute and storage costs. Coordinate with database and application teams before removal to ensure that dependencies or failover plans are not disrupted.</p>","<p id=""""><a href=""https://aws.amazon.com/rds/pricing/\&quot;"" id="""">RDS Pricing</a></p>",FALSE,FALSE,,39,,<p>AWS-Databases-2996</p>
Inactive S3 Bucket,inactive-s3-bucket,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b42e09e434112c2f371,FALSE,FALSE,Thu May 22 2025 14:57:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-s3,unused-resource,storage,"<p id="""">S3 buckets often persist after projects complete or when the associated workloads have been retired. If a bucket is no longer being read from or written to—and its contents are not required for compliance, backup, or retention purposes—it represents ongoing cost without delivering value. Many organizations overlook these idle buckets, especially in shared or legacy accounts, leading to unnecessary storage costs over time.</p>","<p id="""">S3 charges based on the total storage used (per GB per month), the selected storage class, and the number and type of requests made (PUT, GET, LIST, etc.). Storage costs accrue continuously, even if the data is never accessed. Buckets with no read or write activity still incur storage charges for all retained data.</p>","<ul id=""""><li id="""">Identify S3 buckets with no read or write activity during the defined lookback period</li><li id="""">Review object access patterns to confirm that stored data is not being queried or updated</li><li id="""">Check whether the bucket was associated with a decommissioned workload, environment, or application</li><li id="""">Assess whether the data is still needed for compliance, reference, or future recovery</li><li id="""">Validate with data owners or security stakeholders before making changes</li></ul>","<p id="""">If a bucket is confirmed to be inactive and nonessential, delete the bucket and all stored objects. Alternatively, move the data to a lower-cost storage class such as Glacier Flexible Retrieval or Deep Archive for long-term retention at reduced cost. Consider applying lifecycle policies going forward to automatically clean up inactive data.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/s3/pricing/\&quot;"" id="""">S3 Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\"" id="""">S3 Storage Class Overview</a></li></ul>",FALSE,FALSE,,1,,<p>AWS-Storage-8589</p>
Inactive Tables in Storage Account,inactive-tables-in-storage-account,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b460606304f3b580192,FALSE,FALSE,Thu May 22 2025 14:57:10 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),,azure,azure-table-storage,unused-resource,storage,"<p id="""">Tables with no read or write activity often represent deprecated applications, obsolete telemetry, or abandoned development artifacts. Retaining inactive tables increases storage costs and operational complexity. Regularly auditing and cleaning up unused tables helps maintain a streamlined, cost-effective storage environment.</p>","<p id="""">Azure Table Storage charges for stored data per GB and separately for read, write, and delete operations. Tables incur ongoing storage costs even if no activity occurs, contributing to unnecessary baseline expenses.</p>","<ul id=""""><li id="""">Identify Azure Table Storage tables with no read or write operations over a defined lookback period</li><li id="""">Review table creation dates, metadata, and ownership tags to assess relevance and intended retention</li><li id="""">Check for compliance, legal hold, or audit requirements before initiating deletions or exports</li><li id="""">Evaluate whether table contents require archival, and plan export strategies where needed</li><li id="""">Validate findings with application owners, data stewards, or governance teams before proceeding</li></ul>","<p id="""">Delete tables confirmed to be redundant or obsolete. If long-term retention is required, export the table contents (e.g., to CSV or JSON format) and store them in lower-cost services like Azure Blob Storage Archive tier. Incorporate tagging and lifecycle management practices to prevent reaccumulation of unused tables.</p>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-overview\&quot;"" id="""">Azure Table Storage Overview</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/storage/tables/\"" id="""">Azure Storage Pricing</a></li></ul>",FALSE,FALSE,,120,,<p>Azure-Storage-9154</p>
Inactive VPC Interface Endpoint,inactive-vpc-interface-endpoint,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b41c0bd0856e23274e4,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-vpc,unused-resource,networking,"<p id="""">VPC Interface Endpoints are commonly deployed to meet network security or compliance requirements by enabling private access to AWS services. However, these endpoints often remain provisioned even after the original use case is deprecated. In some cases, the applications have been decommissioned; in others, traffic routing has changed and the endpoint is no longer used. Since interface endpoints generate hourly charges whether or not they are used, identifying and removing inactive ones can eliminate unnecessary costs.</p>","<p id="""">VPC Interface Endpoints incur costs based on:</p><ul id=""""><li id="""">Hourly Charges: Billed per hour for each provisioned interface endpoint, in each Availability Zone, regardless of traffic.</li><li id="""">Data Processing Charges: Billed per gigabyte (GB) of data processed through the endpoint.</li></ul><p id="""">Because hourly charges continue to accrue even when no data flows through the endpoint, idle endpoints result in ongoing costs with no business value.</p>","<ul id=""""><li id="""">Identify all VPC Interface Endpoints currently provisioned in your account</li><li id="""">Review data transfer activity to determine whether any data has flowed through the endpoint over a representative time period</li><li id="""">Confirm whether the associated AWS service or endpoint service is still used by any workloads in the environment</li><li id="""">Evaluate whether the endpoint is tied to any running EC2 instances, Lambda functions, or container workloads</li><li id="""">Check whether the endpoint is linked to a deprecated environment or redundant architecture</li><li id="""">Engage with networking or application teams to verify that no current dependencies exist</li></ul>","<p id="""">Decommission VPC Interface Endpoints that show no recent usage and are not tied to any active application dependencies. Since endpoints are billed hourly, removing unused ones immediately eliminates ongoing charges. This change has no impact on availability or connectivity as long as no resources are routing traffic through the endpoint. Coordinate with application and networking teams to ensure safe removal.</p>","<p id=""""><a href=""https://aws.amazon.com/privatelink/pricing/\&quot;"" id="""">AWS PrivateLink Pricin</a>g</p>",FALSE,FALSE,,10,"<p id="""">Illustrative savings for removing idle interface endpoints:</p><ul id=""""><li id="""">A single inactive interface endpoint in <em id="""">us-east-1</em> costs $0.01/hour = $7.30/month</li><li id="""">Removing 100 unused endpoints = $730/month or $8,760/year</li><li id="""">Even greater savings accrue if endpoints were also incurring data-processing fees</li></ul>",<p>AWS-Networking-1988</p>
Inactive Web Application Firewall (WAF),inactive-web-application-firewall-waf,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4747b0f2b3ca307c17,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-waf,unused-resource,networking,"<p id="""">Azure WAF configurations attached to Application Gateways can persist after their backend pool resources have been removed — often during environment reconfiguration or application decommissioning. In these cases, the WAF is no longer serving any functional purpose but continues to incur fixed hourly costs. Because no traffic is routed and no applications are protected, the WAF is effectively inactive. These orphaned WAFs are easy to overlook without regular cleanup processes and can quietly accumulate unnecessary charges over time.</p>","<p id="""">Azure WAF on Application Gateway is billed based on:</p><ul id=""""><li id="""">Per-hour instance charges — incurred regardless of traffic volume</li><li id="""">Data processing charges — based on traffic inspected, only applicable when the WAF is actively routing requests</li></ul><p id="""">A WAF remains billable as long as it is provisioned, even if there are no active backend pool resources or traffic flowing through it.</p>","<ul id=""""><li id="""">Identify WAF-enabled Application Gateways with no associated backend pool targets</li><li id="""">Review traffic metrics or diagnostic logs to confirm that no requests are being processed</li><li id="""">Validate that the WAF is not in use by other services such as Front Door or CDN endpoints</li><li id="""">Consult with application teams to determine whether the WAF is still needed for future use</li></ul>","<ul id=""""><li id="""">Delete WAF configurations that are no longer routing traffic or protecting active applications</li><li id="""">Establish periodic audits to flag and review WAFs with empty backend pools</li><li id="""">Use automated checks to detect and alert on WAF deployments with no active use</li></ul>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/web-application-firewall/"" id="""">Azure WAF Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/web-application-firewall/ag/ag-overview"" id="""">Application Gateway WAF Overview</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Networking-5932</p>
Inactive and Detached EBS Volume,inactive-and-detached-ebs-volume,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b40c0b8688d517a8f58,FALSE,FALSE,Thu May 22 2025 14:57:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-ebs,unused-resource,storage,"<p id="""">EBS volumes frequently remain detached after EC2 instances are terminated, replaced, or reconfigured. Some may be intentionally retained for reattachment or backup purposes, but many persist unintentionally due to the lack of automated cleanup. When these detached volumes are also inactive—showing no read or write activity—they represent unnecessary ongoing costs. Identifying and removing these orphaned volumes can produce meaningful savings without affecting running workloads.</p>","<p id="""">EBS volumes are billed based on the provisioned volume type, size, and performance configuration, regardless of usage or attachment</p>","<ul id=""""><li id="""">Identify EBS volumes that are not attached to any EC2 instance (“available”)</li><li id="""">Review usage data to confirm that no read or write activity has occurred over a defined lookback period</li><li id="""">Check whether the volume is intentionally retained for manual recovery, reattachment, or snapshot purposes, especially if it's part of a known backup or disaster recovery process managed outside of AWS</li><li id="""">Evaluate the cost contribution of the volume based on size, type, and provisioned IOPS</li><li id="""">Determine whether a recent snapshot exists or is needed before deletion</li><li id="""">Confirm with application or infrastructure owners whether the volume still serves a purpose</li></ul>","<p id="""">Delete detached and inactive EBS volumes that are no longer required. Before deletion, take a snapshot if the data must be retained for backup or compliance purposes. Consider setting up scheduled audits or automated cleanup routines to regularly identify and remove unused volumes.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/ebs/pricing/\&quot;"" id="""">EBS Pricing</a></li><li id=""""><a href=""https://aws.amazon.com/blogs/mt/controlling-your-aws-costs-by-deleting-unused-amazon-ebs-volumes/\"" id="""">Controlling EBS Costs</a></li></ul>",FALSE,FALSE,,11,,<p>AWS-Storage-6653</p>
Inactive and Detached Managed Disk,inactive-and-detached-managed-disk,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b407db021f413aa3dd6,FALSE,FALSE,Thu May 22 2025 14:57:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,azure,azure-managed-disks,unused-resource,storage,"<p id="""">Managed Disks frequently remain detached after Azure virtual machines are deleted, reimaged, or reconfigured. Some may be intentionally retained for reattachment, backup, or migration purposes, but many persist unintentionally due to the lack of automated cleanup processes. When these detached disks are also inactive—showing no read or write activity—they represent unnecessary ongoing costs. Identifying and removing these orphaned disks can produce meaningful savings without affecting any active workloads.</p>","<p id="""">Managed Disks are billed based on:</p><ul id=""""><li id="""">Provisioned disk size (per GB/month) — billed regardless of attachment or usage</li><li id="""">Disk types include:</li><li id="""">Premium SSD (v1/v2) – High-performance workloads</li><li id="""">Standard SSD – Balanced cost and performance</li><li id="""">Standard HDD – Lower-cost for infrequent access</li><li id="""">Ultra Disk – Provisioned with specific IOPS and throughput, billed separately</li><li id="""">Snapshots – Billed separately based on storage consumed</li></ul><p id="""">Unattached disks are fully billable until explicitly deleted</p>","<ul id=""""><li id="""">Identify Managed Disks that are in an unattached state (not linked to any VM)</li><li id="""">Review metrics or activity logs to determine whether the disk has seen any read or write operations during the lookback period</li><li id="""">Check whether the disk is intentionally retained for recovery, migration, or reattachment</li><li id="""">Determine whether a snapshot has already been created or is needed prior to deletion</li><li id="""">Consult with workload owners or infrastructure teams to verify whether the disk still serves a business or technical purpose</li></ul>","<p id="""">Delete inactive and detached Managed Disks that are no longer needed. If data retention is required, create a snapshot before deletion. Establish periodic reviews or automate cleanup of unused resources to reduce persistent storage costs, especially for higher-tier disk types.</p>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/managed-disks/"" id="""">Azure Managed Disks Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types"" id="""">Azure Disk Storage Overview</a></li></ul>",FALSE,FALSE,,110,,<p>Azure-Storage-5257</p>
Inactive and Stopped VM,inactive-and-stopped-vm,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4347bdcbdaaafbf32d,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,azure,azure-virtual-machines,unused-resource,compute,"<p id="""">This inefficiency arises when a virtual machine is left in a stopped (deallocated) state for an extended period but continues to incur costs through attached storage and associated resources. These idle VMs are often remnants of retired workloads, temporary environments, or paused projects that were never fully cleaned up. Without clear ownership or automated cleanup, they can persist unnoticed and accumulate avoidable charges.</p>","<p id="""">While compute charges stop when a VM is in a deallocated (stopped) state, storage costs for the attached OS and data disks continue to accrue. Public IP addresses (if static), snapshots, and backup configurations may also generate charges.</p>","<ul id=""""><li id="""">Identify virtual machines that have remained in a stopped (deallocated) state for the entire lookback period</li><li id="""">Review whether any activity has occurred from the associated managed disks, network interfaces, or backup processes</li><li id="""">Evaluate whether the VM is part of a dev/test or legacy environment with no recent usage</li><li id="""">Check for active reservations or static public IPs still assigned to the VM</li><li id="""">Validate with the application owner whether the VM or its data is still needed</li></ul>","<p id="""">Delete stopped VMs that are confirmed to be inactive and no longer required. Ensure that managed disks, static IPs, and snapshots are reviewed and cleaned up accordingly. If the data may be needed later, consider creating a snapshot or exporting the disk before deletion. Implement scheduled reviews of deallocated VMs to minimize waste across environments.</p>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/virtual-machines/\&quot;"" id="""">Azure VM Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/states-lifecycle\"" id="""">Azure VM States</a></li></ul>",FALSE,FALSE,,121,,<p>Azure-Compute-7500</p>
Inactive and Unmounted EFS File System,inactive-and-unmounted-efs-file-system,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b486009684d2ff081d2,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Shireen Maini,aws,aws-efs,unused-resource,storage,"<p id="""">EFS file systems that are no longer attached to any running services — such as EC2 instances or Lambda functions — continue to incur storage charges. This often occurs after workloads are decommissioned but the file system is left behind. A quick indicator of this state is when the EFS file system has no mount targets configured. Without active usage or connection, these orphaned file systems represent pure cost with no functional value. Unlike block storage, EFS does not require an attached instance to incur billing, making it easy for unused resources to go unnoticed.</p>","<p id="""">EFS is billed based on:</p><ul id=""""><li id="""">Total data stored (per GB/month)</li><li id="""">Storage tier (Standard or Infrequent Access, if lifecycle policies are enabled)</li><li id="""">Optional provisioned throughput (if configured)</li></ul><p id="""">Even when unmounted, EFS continues to accrue storage cost until deleted.</p>","<ul id=""""><li id="""">Identify EFS file systems with no mount targets present</li><li id="""">Check for file systems with no recent read/write activity over a representative time window</li><li id="""">Correlate with decommissioned EC2 instances or workloads that previously used the file system</li><li id="""">Confirm with workload owners whether the data is still required or can be deleted</li></ul>","<ul id=""""><li id="""">Delete EFS file systems that are no longer in use and have no attached mount targets</li><li id="""">If data must be retained, consider exporting it to a lower-cost storage service (e.g., S3 Glacier) before deletion</li><li id="""">Establish periodic audits to identify and clean up orphaned file systems</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/efs/pricing/"" id="""">Amazon EFS Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/efs/latest/ug/mount-targets.html"" id="""">Working with Amazon EFS Mount Targets</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Storage-2770</p>
Inefficient Autotermination Configuration for Interactive Clusters,inefficient-autotermination-configuration-for-interactive-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b47562ebb0d80a02f80,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Matt Weingarten,databricks,databricks-clusters,misconfiguration,compute,"<p id="""">Interactive clusters are often left running between periods of active use. To mitigate idle charges, Databricks provides an “autotermination” setting that shuts down clusters after a period of inactivity. However, if the termination period is set too high, or if policies do not enforce reasonable thresholds, idle clusters can persist for long durations without performing any work—resulting in wasted compute spend. Lowering the termination window reduces exposure to idle time while preserving user flexibility.</p>","<p id="""">Databricks compute is billed via:</p><ul id=""""><li id="""">Databricks Units (DBUs): Billed per hour based on cluster type and node configuration</li><li id="""">Underlying Cloud Infrastructure: Charged based on VM runtime, billed per second or minute</li></ul><p id="""">Interactive clusters continue accruing DBU and VM costs while running, including during idle periods. The autotermination setting determines how long idle clusters persist before being shut down automatically. Longer termination windows result in more idle time and unnecessary cost.</p>","<ul id=""""><li id="""">List all interactive clusters and review their configured autotermination settings</li><li id="""">Compare termination time thresholds to workspace policy guidelines or best practices</li><li id="""">Confirm whether any long windows are tied to justified use cases (e.g., infrequent but heavy dev work)</li></ul>","<ul id=""""><li id="""">Lower the autotermination threshold for interactive clusters</li><li id="""">Apply workspace compute policies to cap the maximum idle time for clusters</li><li id="""">Grant exceptions only when use cases are documented and cost impact is understood</li></ul>","<ul id=""""><li id="""">Configure Autotermination Settings</li><li id="""">Databricks Compute Policies</li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-1898</p>
Inefficient BI Queries Driving Excessive Compute Usage,inefficient-bi-queries-driving-excessive-compute-usage,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941a3c06b6092977ac29,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Scott Shulman,databricks,interactive-clusters,inefficient-query-patterns,compute,"<p id="""">Business Intelligence dashboards and ad-hoc analyst queries frequently drive Databricks compute usage — especially when:  * Dashboards are auto-refreshed too frequently   * Queries scan full datasets instead of leveraging filtered views or materialized tables   * Inefficient joins or large broadcast operations are used   * Redundant or exploratory queries are triggered during interactive exploration  This often results in clusters staying active for longer than necessary, or being autoscaled up to handle inefficient workloads, leading to unnecessary DBU consumption.</p>","<p id="""">Databricks usage is billed per DBU (Databricks Unit) per hour, depending on the cluster type and size. BI dashboards and ad-hoc SQL queries can keep interactive or all-purpose clusters running, consuming DBUs even when queries are poorly optimized or overly frequent.</p>","<ul id=""""><li id="""">Review query logs from SQL Analytics or interactive clusters to identify:</li><li id="""">Query patterns that scan large volumes of data repeatedly</li><li id="""">Dashboards with auto-refresh settings that exceed business needs</li><li id="""">Repeated ad-hoc queries with similar structure but different filters</li><li id="""">Long-running queries due to unoptimized joins, filters, or aggregations</li><li id="""">High DBU usage attributed to BI users or SQL endpoints with low result value</li></ul>","<ul id=""""><li id="""">Refactor BI queries to limit scan scope and reduce complexity</li><li id="""">Materialize frequently used intermediate results into temp or Delta tables</li><li id="""">Reduce auto-refresh frequency of dashboards unless real-time data is essential</li><li id="""">Educate analyst teams on query efficiency and impact on backend costs</li><li id="""">Enable query result caching or switch to serverless SQL warehouses if feasible</li></ul>","<p id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/databricks/"" id="""">Azure Databricks Pricing</a><br><a href=""https://learn.microsoft.com/en-us/azure/databricks/integrations/bi/power-bi"" id="""">Best practices for BI tools with Databricks</a><br><a href=""https://learn.microsoft.com/en-us/azure/databricks/sql/user/monitor/sql-query-history"" id="""">Monitoring SQL Queries</a></p>",FALSE,FALSE,,,,<p>Databricks-Compute-6762</p>
Inefficient Execution of Repeated Queries,inefficient-execution-of-repeated-queries,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4c110e271eeba54cf1,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-query-processing,inefficient-query-pattern,compute,"<p id="""">Inefficient execution of repeated queries occurs when common query patterns are frequently executed without optimization. Even if individual executions are successful, repeated inefficiencies compound overall compute consumption and credit costs.</p><p id="""">By analyzing Snowflake's parameterized query metrics, organizations can identify top repeated queries and optimize them for better performance, resource usage, and cost-efficiency.</p>",,"<ul id="""">  <li id="""">Review parameterized query groupings to identify the most frequently executed query patterns</li>  <li id="""">Analyze whether repeated queries are incurring high cumulative warehouse costs or extended runtimes</li>  <li id="""">Evaluate if frequent errors are occurring in repeated query executions, indicating underlying inefficiencies</li>  <li id="""">Consult with data engineering teams to assess whether query logic, data access patterns, or indexing strategies can be improved</li></ul>","<ul id="""">  <li id="""">Prioritize optimization efforts on the highest-cost or highest-frequency repeated queries</li>  <li id="""">Refactor query structures to minimize unnecessary complexity, joins, or large data scans</li>  <li id="""">Tune data models, clustering keys, or materialized views to support more efficient repeated query execution</li>  <li id="""">Apply query scheduling or caching techniques where possible to reduce redundant executions</li>  <li id="""">Monitor parameterized query patterns regularly to identify and address emerging inefficiencies</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history"" id="""">Query Cost Attribution</a></li><li id="""">‍<a href=""https://docs.snowflake.com/en/sql-reference/functions/query_history"" id="""">Query History</a></li></ul><p id=""""> </p>",FALSE,FALSE,,,,<p>Snowflake-Compute-4453</p>
Inefficient File Format and Layout for Athena Queries,inefficient-file-format-and-layout-for-athena-queries,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589415f6420d9777b718c4,FALSE,FALSE,Sun Jun 22 2025 23:39:01 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Erik Marke,aws,aws-athena,suboptimal-data-layout-or-format,compute,"<p id="""">Storing raw JSON or CSV files in S3—especially when written frequently in small batches—leads to excessive scan costs in Athena. These formats are row-based and verbose, requiring Athena to scan and parse the full content even when only a few fields are queried. Without columnar formats, partitioning, or metadata-aware table formats, queries become inefficient and expensive, especially in high-volume environments.</p>","<p id="""">Pay-per-scan — Athena charges based on the amount of data scanned per query, not the size of the result set. This makes storage format, partitioning, and file layout critical cost drivers.</p>","<ul id=""""><li id="""">Review whether data stored in S3 is in raw formats like JSON or CSV</li><li id="""">Check if files are written at high frequency, resulting in large numbers of small files</li><li id="""">Evaluate whether data is partitioned by commonly filtered fields (e.g., date, region)</li><li id="""">Determine if users repeatedly query overlapping data ranges without partition pruning</li><li id="""">Assess whether queries experience slow performance or high latency for recent data</li></ul>","<ul id=""""><li id="""">Convert raw data to columnar formats such as Parquet or ORC to reduce scan size</li><li id="""">Partition data based on common query dimensions (e.g., date, tenant ID)</li><li id="""">Consolidate small files into larger batches to improve scan efficiency</li><li id="""">Adopt table formats like Apache Iceberg, Delta Lake, or Hudi for metadata support and schema evolution</li><li id="""">Stream data into these formats using structured pipelines to reduce latency and support efficient querying</li><li id="""">Refactor query patterns or create materialized views to reduce redundant scans</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/athena/latest/ug/best-practices.html"" id="""">Best Practices for Amazon Athena</a><br><a href=""https://docs.aws.amazon.com/athena/latest/ug/parquet.html"" id="""">Using Parquet with Amazon Athena</a><br><a href=""https://aws.amazon.com/athena/pricing/"" id="""">Amazon Athena Pricing</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-9503</p>
Inefficient Pipeline Refresh Scheduling,inefficient-pipeline-refresh-scheduling,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4ca96194f6d3c736a2,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-tasks-and-pipelines,inefficient-scheduling,other,"<p id="""">Inefficient pipeline refresh scheduling occurs when data refresh operations are executed more frequently, or with more compute resources, than the actual downstream business usage requires.</p><p id="""">Without aligning refresh frequency and resource allocation to true data consumption patterns (e.g., report access rates in Tableau or Sigma), organizations can waste substantial Snowflake credits maintaining underutilized or rarely accessed data assets.</p>",,"<ul id="""">  <li id="""">Review the execution frequency and resource consumption (warehouse size, task duration) of scheduled pipelines and tasks</li></ul><p id="""">Map data lineage to understand which downstream assets (e.g., dashboards, reports) depend on each refreshed dataset</p><ul id="""">  <li id="""">Analyze BI tool usage metrics (e.g., Tableau, Sigma) to assess the frequency of access to downstream data consumers</li>  <li id="""">Identify pipelines where the refresh cost is high relative to the actual business consumption of the refreshed data</li></ul>","<ul id="""">  <li id="""">Adjust pipeline refresh frequencies to better align with actual data access patterns (e.g., move from hourly to daily refresh if applicable)</li>  <li id="""">Right-size the warehouse resources used for pipeline executions to minimize overprovisioning</li>  <li id="""">Implement usage monitoring frameworks that continuously correlate refresh costs with downstream consumption</li>  <li id="""">Periodically review pipeline operational costs and business value to optimize refresh schedules proactively</li></ul>",,FALSE,FALSE,,,,<p>Snowflake-Other-4439</p>
Inefficient Private Link Routing to Azure Databricks,inefficient-private-link-routing-to-azure-databricks,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4836f7348b9b82f7c4,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Benjamin van der Maas,azure,azure-databricks,misconfiguration,other,"<p id="""">In Azure Databricks environments that rely on Private Link for secure networking, it’s common to route traffic through multi-tiered network architectures. This often includes multiple VNets, Private Link endpoints, or peered subscriptions between data sources (e.g., ADLS) and the Databricks compute plane. While these architectures may be designed for isolation or compliance, they frequently introduce redundant routing paths that add cost without improving performance. Each additional hop may result in duplicated Private Link ingress and egress charges. Without regular review, this can create persistent and unrecognized network inefficiencies tied to Databricks usage.</p>","<p id="""">Azure networking charges are incurred based on:</p><ul id=""""><li id="""">Private Link data processing (per GB)</li><li id="""">VNet peering ingress/egress (per GB)</li><li id="""">Regional vs. cross-region data transfer</li></ul><p id="""">Unnecessary network hops can compound these charges, even when they serve no functional or security purpose.</p>","<ul id=""""><li id="""">Review network costs in the Azure subscription hosting your Databricks environment</li><li id="""">Identify high Private Link ingress charges or VNet peering transfer fees</li><li id="""">Trace end-to-end data paths between storage and Databricks compute</li><li id="""">Count how many Private Link or peering hops exist along the path</li><li id="""">Assess whether all network segments are functionally required</li></ul>","<ul id=""""><li id="""">Simplify routing by colocating Databricks and storage in the same region and VNet when possible</li><li id="""">Eliminate redundant Private Link endpoints that add no security or compliance value</li><li id="""">Use direct peering or shared services models to reduce network traversal</li><li id="""">Continuously audit Databricks data paths to align architecture with minimum-cost, minimum-hop configurations</li></ul>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/private-link/"" id="""">Azure Private Link Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/secure"" id="""">Deploy Azure Databricks in Your Own VNet</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Other-2429</p>
Inefficient Processor Selection in EC2 Instances,inefficient-processor-selection-in-ec2-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4c866414fe44b7325f,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Jason DiDomenico,aws,aws-ec2,suboptimal-instance-family-selection,compute,"<p id="""">  Many organizations default to Intel-based EC2 instances due to familiarity or assumptions about workload compatibility. However, AWS offers AMD and Graviton-based alternatives that often deliver significantly better price-performance for general-purpose and compute-optimized workloads.</p><p id="""">By not testing workloads across available architectures, teams may continue paying a premium for Intel instances even when no specific performance or compatibility benefit exists. Over time, this results in unnecessary compute spend across development, staging, and even production environments.</p>","<ul id=""""><li id="""">EC2 instances are billed per second (Linux) or per hour (Windows/older usage) based on:</li><li id=""""><strong id="""">Instance type and size</strong></li><li id=""""><strong id="""">Architecture family</strong> (e.g., Intel, AMD, Graviton)</li><li id="""">Graviton instances (`m6g`, `c6g`, etc.) typically offer <strong id="""">20–40% better price-performance</strong></li><li id="""">AMD-based instances (`m6a`, `r6a`, etc.) are often priced lower than equivalent Intel types</li></ul>","<ul id=""""><li id="""">Identify EC2 instances running on Intel-based families</li><li id="""">Identify applications with general-purpose compute requirements that could be migrated to Graviton or AMD</li></ul>","<ul id=""""><li id="""">Check for architecture-specific performance issues or compatibility blockers before switching</li><li id="""">Benchmark representative workloads on Intel, AMD, and Graviton instance types</li><li id="""">Migrate to the instance family that offers the best price-performance for the workload</li><li id="""">Update provisioning defaults and infrastructure templates to prefer AMD or Graviton when appropriate</li></ul>","<ul id=""""><li id="""">  <a href=""https://aws.amazon.com/ec2/graviton/"" id="""">AWS Graviton Processor Overview</a></li><li id="""">  <a href=""https://aws.amazon.com/ec2/instance-types/"" id="""">Amazon EC2 Instance Types</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-6667</p>
Inefficient Query Design in Databricks SQL and Spark Jobs,inefficient-query-design-in-databricks-sql-and-spark-jobs,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84babfa9b081371eadb,FALSE,FALSE,Sun Aug 10 2025 19:51:39 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Nicole Boyd,databricks,databricks-sql,inefficient-configuration,compute,"<p id="""">Many Spark and SQL workloads in Databricks suffer from micro-optimization issues — such as unfiltered joins, unnecessary shuffles, missing broadcast joins, and repeated scans of uncached data. These problems increase compute time and resource utilization, especially in exploratory or development environments. Disabling Adaptive Query Execution (AQE) can further exacerbate inefficiencies. Optimizing queries reduces DBU costs, improves cluster performance, and enhances user experience.</p>","<p id="""">Databricks charges by Databricks Units (DBUs), which are billed per-second based on the compute resources used. Inefficient query design leads to longer execution times, increased memory and shuffle usage, and higher DBU consumption without proportional business value.</p>","<ul id=""""><li id="""">Identify queries with long execution times and large shuffle stages</li><li id="""">Review query plans for missing filters or non-optimal joins (e.g., lack of broadcast joins)</li><li id="""">Check for repeated reads from large datasets without use of caching</li><li id="""">Check memory and storage metrics for signs of inefficient caching (e.g., low cache hit rate despite frequent reads)</li><li id="""">Evaluate whether Adaptive Query Execution (AQE) is enabled — it is on by default in recent Spark versions, but may be disabled in older clusters or overridden in session or job-level configurations.</li></ul>","<ul id=""""><li id="""">Enable Adaptive Query Execution to improve join strategies and reduce shuffle</li><li id="""">Use broadcast joins for small lookup tables where applicable</li><li id="""">Apply filtering and predicate pushdown early in the query</li><li id="""">Cache frequently accessed tables or intermediate query results</li><li id="""">Refactor inefficient joins, subqueries, or nested aggregations</li><li id="""">Promote a culture of reviewing physical query plans during development</li><li id="""">Implement partition pruning or Z-ordering to improve scan efficiency for large tables</li></ul>","<ul id=""""><li id="""">Optimizing Apache Spark SQL Joins in Databricks</li><li id="""">Caching and Persistence</li><li id=""""><a href=""https://spark.apache.org/docs/latest/sql-performance-tuning.html#adapting-query-execution"" id="""">Adaptive Query Execution in Spark</a></li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-4415</p>
Inefficient SnapStart Configuration in Lambda,inefficient-snapstart-configuration-in-lambda,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58fc9fee5ea45e74f82,FALSE,FALSE,Mon Nov 03 2025 16:17:19 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Jake McCracken,aws,aws-lambda,misconfigured-performance-optimization,compute,"<p id="""">SnapStart reduces cold-start latency, but when configured inefficiently, it can increase costs. High-traffic workloads can trigger frequent snapshot restorations, multiplying costs. Slow initialization code inflates the Init phase, which is now billed at the full rate. Suppressed-init conditions, where functions initialize without enhanced resources, can add further inefficiency if memory or timeout settings are misaligned. Together, these factors can cause SnapStart to deliver higher spend without proportional benefit.</p>","<p id="""">Lambda charges are based on requests and execution duration, multiplied by allocated memory. SnapStart introduces additional billing factors:</p><p id="""">* Snapshot caching — a fee is charged for each function version published with SnapStart enabled.</p><p id="""">* Snapshot restorations — each time a function instance is restored from a snapshot, cost is incurred based on memory size.</p><p id="""">* Init phase billing — the initialization (INIT) phase is charged at the standard rate, not the discounted warm start rate.</p>","<ul id=""""><li id="""">Review whether frequent traffic bursts are driving high restoration activity and associated costs</li><li id="""">Evaluate whether function initialization logic is lengthy, leading to increased charges in the Init phase</li><li id="""">Check for suppressed-init behavior in functions and determine if timeouts or memory allocations are contributing</li><li id="""">Assess whether SnapStart is delivering latency improvements that justify its incremental costs</li></ul>","<ul id=""""><li id="""">Implement concurrency controls to reduce excess restorations during high traffic bursts</li><li id="""">Optimize function initialization to minimize Init phase duration by loading only essential dependencies</li><li id="""">Use pre-snapshot hooks (for Java) to prepare code execution and reduce overhead before the snapshot is taken</li><li id="""">Adjust memory allocation and timeouts to reduce suppressed-init inefficiencies and ensure stable initialization</li></ul>",,FALSE,FALSE,,N/A,,<p>AWS-Compute-5876</p>
Inefficient Snowpipe Usage Due to Small File Ingestion,inefficient-snowpipe-usage-due-to-small-file-ingestion,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4b4d8602da4f4fa9bb,FALSE,FALSE,Thu May 22 2025 14:57:15 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-snowpipe,inefficient-data-ingestion,other,"<p id="""">Ingesting a large number of small files (e.g., files smaller than 10 MB) using Snowpipe can lead to disproportionately high costs due to the per-file overhead charges. Each file, regardless of its size, incurs the same overhead fee, making the ingestion of numerous small files less cost-effective. Additionally, small files can increase the load on Snowflake's metadata and ingestion infrastructure, potentially impacting performance.</p>",,"<ul id="""">  <li id="""">Analyze the average file size being ingested via Snowpipe; identify if many files are below the recommended size threshold (e.g., under 10 MB).</li>  <li id="""">Review the total number of files ingested over a period to assess the impact of per-file overhead charges.</li>  <li id="""">Evaluate the frequency of file arrivals; high-frequency ingestion of small files may indicate an opportunity for batching.</li>  <li id="""">Consult with data engineering teams to understand the source systems and whether file batching is feasible without impacting data freshness requirements.</li></ul>","<ul id="""">  <li id="""">Implement batching mechanisms to aggregate small files into larger ones before ingestion, aiming for file sizes between 10 MB and 250 MB for optimal cost-performance balance.</li></ul><p id="""">Adjust data pipeline configurations to stage data at regular intervals (e.g., every few minutes) to allow for file aggregation.</p><ul id="""">  <li id="""">Explore using Snowpipe Streaming for real-time ingestion scenarios, as it may offer more cost-effective options for high-frequency, small data loads.</li>  <li id="""">Monitor Snowpipe usage and costs regularly to identify and address inefficiencies promptly.</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.snowflake.com/en/user-guide/data-load-snowpipe-billing"" id="""">Snowpipe Best Practices</a><a href=""https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-recommendation"" id="""">‍</a></li><li id=""""><a href=""https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-recommendation"" id="""">Snowpipe Streaming Recommendations</a></li></ul><p id="""">‍</p>",FALSE,FALSE,,,,<p>Snowflake-Other-8418</p>
Inefficient Use of Azure Pipelines,inefficient-use-of-azure-pipelines,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fd7a32336abda3c83a,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Orphéric Allagbe,azure,azure-pipelines,,other,"<p id="""">Teams often overuse Microsoft-hosted agents by running redundant or low-value jobs, failing to configure pipelines efficiently, or neglecting to use self-hosted agents for steady workloads. These inefficiencies result in unnecessary cost and delivery friction, especially when pipelines create queues due to limited agent availability.</p>","<p id="""">Azure Pipelines charges for Microsoft-hosted agents based on pipeline execution time. Public projects get 1,800 free minutes/month; additional usage is billed per parallel job. Self-hosted agents avoid Microsoft billing but require infrastructure and maintenance.</p>","<ul id=""""><li id="""">Review pipeline job execution history for unnecessary or duplicated stages</li><li id="""">Identify pipelines with high agent usage but low business value</li><li id="""">Check for frequent pipeline runs that could be conditionally suppressed</li><li id="""">Evaluate whether self-hosted agents exist but are underutilized</li><li id="""">Look for consistent agent queue delays, signaling insufficient capacity</li></ul>","<ul id=""""><li id="""">Audit and streamline pipelines to remove redundant or unnecessary stages</li><li id="""">Use conditional logic to limit execution of non-critical pipelines</li><li id="""">Prioritize agent capacity for pipelines supporting core or production workloads</li><li id="""">Consider introducing or expanding self-hosted agents for predictable builds</li><li id="""">Monitor and adjust agent pool size to eliminate queue backlogs</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/pools?view=azure-devops"" id="""">https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/pools?view=azure-devops</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/devops/organizations/billing/overview?view=azure-devops"" id="""">https://learn.microsoft.com/en-us/azure/devops/organizations/billing/overview?view=azure-devops</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Other-4682</p>
Inefficient Use of Interactive Clusters,inefficient-use-of-interactive-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b476009684d2ff0815e,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Matt Weingarten,databricks,databricks-clusters,misconfiguration,compute,"<p id="""">Interactive clusters are intended for development and ad-hoc analysis, remaining active until manually terminated. When used to run scheduled jobs or production workflows, they often stay idle between executions—leading to unnecessary infrastructure and DBU costs. Job clusters are designed for ephemeral, single-job execution and automatically terminate upon completion, reducing runtime and isolating workloads. Using interactive clusters for production jobs leads to cost inefficiencies and weaker workload boundaries.</p>","<p id="""">Databricks compute costs consist of:</p><ul id=""""><li id="""">Databricks Units (DBUs): Billed per hour based on cluster type and instance class</li></ul><ul id=""""><li id="""">Cloud Infrastructure Charges: Passed through from the underlying cloud provider, billed per second or minute</li></ul><p id="""">Interactive clusters accrue charges continuously while running, regardless of workload activity. Job clusters are provisioned on demand and shut down automatically, incurring charges only for the job’s runtime.</p>","<ul id=""""><li id="""">Identify scheduled jobs assigned to interactive clusters</li><li id="""">Query system tables or logs to verify cluster assignment per job</li><li id="""">Review cluster uptime relative to job duration to assess idle overhead</li><li id="""">Evaluate whether workspace policies restrict interactive cluster use for jobs</li><li id="""">Confirm with engineering teams if job clusters can be safely adopted</li></ul>","<ul id=""""><li id="""">Reassign scheduled jobs to ephemeral job clusters</li><li id="""">Apply workspace policies to enforce job cluster usage for scheduled workflows</li><li id="""">Educate users on the differences between cluster modes and their appropriate use cases</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.databricks.com/aws/en/compute#cluster-modes"" id="""">Databricks Cluster Modes</a></li><li id=""""><a href=""https://docs.databricks.com/aws/en/jobs#cluster-options"" id="""">Job Cluster Configuration</a></li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-3587</p>
Inefficient Use of Job Clusters in Databricks Workflows,inefficient-use-of-job-clusters-in-databricks-workflows,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58e38af7c3b6e5fa12c,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Matt Weingarten,databricks,databricks-workflows,suboptimal-cluster-configuration,other,"<p id="""">When multiple tasks within a workflow are executed on separate job clusters — despite having similar compute requirements — organizations incur unnecessary overhead. Each cluster must initialize independently, adding latency and cost. This results in inefficient resource usage, especially for workflows that could reuse the same cluster across tasks. Consolidating tasks onto a single job cluster where feasible reduces start-up time and avoids duplicative compute charges.</p>","<p id="""">Databricks is billed based on Databricks Units (DBUs) consumed by the compute resources provisioned. Each job cluster accrues DBU charges while running, and additional cost arises from repeated cluster start-up times and redundant resource allocation.</p>","<ul id=""""><li id="""">Review whether multiple clusters are being provisioned within a single workflow for tasks with similar compute requirements</li><li id="""">Assess cluster start-up overhead across tasks to determine if repeated initialization is adding latency and cost</li><li id="""">Examine job execution logs to confirm whether resource utilization across clusters is consistent and could be consolidated</li><li id="""">Evaluate whether the workflow is creating clusters unnecessarily instead of leveraging a single reusable job cluster</li></ul>","<ul id=""""><li id="""">Configure a shared job cluster to run multiple tasks within the same workflow when compute requirements are similar</li><li id="""">Leverage cluster reuse settings to reduce start-up overhead and improve efficiency</li><li id="""">Validate that consolidation does not impact workload performance or isolation requirements before implementing</li></ul>","<ul id=""""><li id="""">Databricks Workflows Documentation</li></ul>",FALSE,FALSE,,N/A,,<p>Databricks-Other-7085</p>
Inefficient Use of On-Demand Capacity in DynamoDB,inefficient-use-of-on-demand-capacity-in-dynamodb,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b3f6506674ea88e7a18,FALSE,FALSE,Thu May 22 2025 14:57:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-dynamodb,inefficient-configuration,databases,"<p id="""">While On-Demand mode is well-suited for unpredictable or bursty workloads, it is often cost-inefficient for applications with consistent throughput. In these cases, shifting to Provisioned mode with Auto Scaling allows teams to set a baseline level of capacity and scale incrementally as needed—often yielding substantial cost savings without compromising performance.</p>","<p id="""">DynamoDB charges for reading, writing, and storing data, along with optional features like backups and global tables. It offers two pricing options:</p><ul id=""""><li id="""">On-Demand Capacity Mode:</li><li id="""">Serverless option with pay-per-request pricing and automatic scaling.</li><li id="""">Billed per read or write request consumed.</li><li id="""">Provisioned Capacity Mode:</li><li id="""">Charges based on hourly read and write capacity provisioned, regardless of actual consumption.</li><li id="""">Supports Auto Scaling to dynamically adjust capacity based on demand.</li></ul>","<ul id=""""><li id="""">Identify DynamoDB tables configured to use On-Demand capacity mode</li><li id="""">Review historical read/write activity for patterns of consistent or gradually increasing traffic</li><li id="""">Evaluate whether the workload exhibits steady throughput, such as regular API usage, background jobs, or scheduled data processing</li><li id="""">Assess whether a baseline level of provisioned capacity would meet typical demand, with Auto Scaling used to accommodate occasional fluctuations</li><li id="""">Check for any operational, architectural, or application-level dependencies that could be affected by switching capacity modes (e.g., throttling behavior, performance expectations)</li><li id="""">Ensure that cost savings would be meaningful and achievable based on current usage trends and capacity planning assumptions</li></ul>","<p id="""">For workloads with predictable usage patterns, transition the table to Provisioned capacity mode and enable Auto Scaling to dynamically adjust throughput within configured limits. This approach preserves performance while reducing cost relative to On-Demand pricing.</p>","<p id=""""><a href=""https://aws.amazon.com/dynamodb/pricing/\&quot;"" id="""">DynamoDB Pricing</a></p>",FALSE,FALSE,,116,,<p>AWS-Databases-3770</p>
Inefficient Use of Photon Engine in Azure Databricks,inefficient-use-of-photon-engine-in-azure-databricks,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4866886d07aedcb8bd,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Mathijs Hendriks,azure,databricks,suboptimal-configuration,compute,"<p id="""">Photon is optimized for SQL workloads, delivering significant speedups through vectorized execution and native C++ performance. However, Photon only accelerates workloads that use compatible operations and data patterns. If a workload includes unsupported functions, unoptimized joins, or falls back to interpreted execution, Photon may be silently bypassed — even on a Photon-enabled cluster. In this case, users are billed at a premium DBU rate while receiving no meaningful speed or efficiency gain. This inefficiency typically arises when teams enable Photon globally without validating workload compatibility or updating their pipelines to follow Photon best practices. The result is higher costs with no corresponding benefit — a classic case of configuration drift outpacing optimization discipline.</p>","<p id="""">Azure Databricks compute charges are based on Databricks Units (DBUs).</p><ul id=""""><li id="""">Photon-enabled clusters may have a higher DBU rate than standard clusters</li><li id="""">If workloads are not optimized for Photon, users may incur higher costs without realizing any performance benefits</li></ul>","<ul id=""""><li id="""">Analyze job execution plans to determine whether Photon is being used end-to-end</li><li id="""">Review workloads that run on Photon-enabled clusters but show no runtime improvement over standard execution</li><li id="""">Identify SQL operations or UDFs that are unsupported by Photon</li></ul><ul id=""""><li id="""">Check for repeated use of legacy query constructs, wide joins, or nested data structures that inhibit vectorization</li></ul>","<ul id=""""><li id="""">Ensure that Photon is only enabled for workloads structured to benefit from vectorized execution</li><li id="""">Refactor SQL logic and data models to align with Photon-optimized patterns (e.g., filter pushdowns, supported UDFs)</li><li id="""">Use built-in tools such as query plans and job profiles to verify Photon execution</li><li id="""">Monitor DBU consumption alongside job duration to track whether Photon is delivering net cost/performance gains</li><li id="""">Collaborate with data engineering teams to continuously tune high-volume pipelines for Photon compatibility</li></ul>","<p id=""""><a href=""https://docs.databricks.com/aws/en/compute/photon\&quot;"" id="""">Photon Runtime on Azure Databricks</a></p>",FALSE,FALSE,,,,<p>Azure-Compute-7871</p>
Inefficient Use of Photon Engine in Databricks Compute,inefficient-use-of-photon-engine-in-databricks-compute,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589415d6d2e5f2160aa85e,FALSE,FALSE,Sun Jun 22 2025 23:39:01 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Josh Collier,databricks,databricks-clusters,inefficient-configuration,compute,"<p id="""">Photon is enabled by default on many Databricks compute configurations. While it can accelerate certain SQL and DataFrame operations, its performance benefits are workload-specific and may not justify the increased DBU cost. Many pipelines, particularly ETL jobs or simpler Spark workloads, do not benefit materially from Photon but still incur the higher DBU multiplier. Disabling Photon by default and allowing it only where proven beneficial can reduce cost without degrading performance.</p>","<p id="""">Databricks charges based on Databricks Units (DBUs), which are consumed per node per hour. Enabling Photon increases the DBU rate by up to 2.9x compared to standard compute. While Photon can significantly reduce runtime for certain workloads, in many environments the performance benefit is marginal, resulting in higher cost without proportional gain.</p>","<ul id=""""><li id="""">Identify compute clusters or jobs with Photon enabled</li><li id="""">Review job configurations for SKUs or node types with Photon in the name</li><li id="""">Check cluster configuration settings for the Photon toggle</li><li id="""">Evaluate job-level performance data to determine if Photon meaningfully reduces runtime for the associated workload</li></ul>","<ul id=""""><li id="""">Update default compute configurations to disable Photon for general-purpose or low-complexity workloads</li><li id="""">Restrict users from enabling Photon unless justified by benchmarked performance gains</li><li id="""">Establish cluster policies or templates that exclude Photon by default and allow opt-in only under specific conditions</li></ul>","<p id=""""><a href=""https://docs.databricks.com/aws/en/compute/photon"" id="""">Photon Runtime on Databricks</a></p>",FALSE,FALSE,,,,<p>Databricks-Compute-3622</p>
Inefficient Use of RDS Reader Nodes,inefficient-use-of-rds-reader-nodes,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941727147d2d66173f55,FALSE,FALSE,Sun Jun 22 2025 23:39:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Jim Bendoraitis,aws,aws-rds,suboptimal-workload-distribution,databases,"<p id="""">RDS reader nodes are intended to handle read-only workloads, allowing for traffic offloading from the primary (writer) node. However, in many environments, services are misconfigured or hardcoded to send all traffic—including reads—to the writer node. This results in underutilized reader nodes that still incur full hourly charges, while the writer node becomes a performance bottleneck and may require upsizing to handle unnecessary load. This inefficiency reduces cost-effectiveness and resilience, especially in high-throughput or scalable architectures.</p>","<p id="""">Per-instance hourly cost for each provisioned writer and reader node</p>","<ul id=""""><li id="""">Review whether the resource has been actively used over a representative time period</li><li id="""">Compare the volume of read traffic across reader nodes vs. the writer node</li><li id="""">Identify services that are hardcoded to target the writer node for both read and write operations</li><li id="""">Evaluate whether the presence of reader nodes is necessary if they are consistently underutilized</li><li id="""">Assess application architectures for abstraction layers or configuration patterns that bypass proper routing to readers</li></ul>","<ul id=""""><li id="""">Refactor application logic or database client configurations to route read traffic to reader endpoints</li><li id="""">Introduce or enhance query routing layers (e.g., using database drivers with read/write splitting support)</li><li id="""">Remove reader nodes if there is no realistic path to utilizing them efficiently</li><li id="""">Monitor reader vs. writer utilization continuously to ensure traffic is appropriately distributed</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"" id="""">Amazon RDS Read Replicas</a><br><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html"" id="""">Best Practices for Amazon RDS</a></p>",FALSE,FALSE,,,,<p>AWS-Databases-7941</p>
Inefficient Use of Reservations in BigQuery,inefficient-use-of-reservations-in-bigquery,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941973f5677639a2a877,FALSE,FALSE,Sun Jun 22 2025 23:39:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,gcp,gcp-bigquery,underutilized-commitment,databases,"<p id="""">Teams often adopt flat-rate pricing (slot reservations) to stabilize costs or optimize for heavy, recurring workloads. However, if query volumes drop — due to seasonal cycles, architectural shifts (e.g., workload migration), or inaccurate forecasting — those reserved slots may sit underused. This inefficiency is easy to miss, as the cost remains fixed and detached from usage volume. Unlike autoscaling models, reservations require active monitoring and manual adjustment. In some organizations, multiple projects reserve separate slot pools, exacerbating waste through fragmentation.</p>","<p id="""">BigQuery offers two primary billing models:  * <strong id="""">On-Demand:</strong> Billed per TB of data scanned.   * <strong id="""">Flat-Rate Reservations:</strong> Billed based on dedicated slots reserved, regardless of actual usage.  Flat-rate pricing is ideal for consistent, high-volume workloads. However, when workload patterns are unpredictable or seasonal, reserved slots may remain idle, generating cost without performance benefit. Slot commitments are billed per second with a minimum duration, and unused capacity is not refunded or reallocated automatically.</p>","<ul id=""""><li id="""">Review reservation utilization trends over a representative period</li><li id="""">Compare committed slots vs. actual slots used per project or workload</li><li id="""">Identify whether query concurrency or runtime metrics justify reservation size</li><li id="""">Assess whether workloads could be served equally well with on-demand or flex slots</li><li id="""">Review historical query demand to identify periods of persistent underutilization</li></ul>","<ul id=""""><li id="""">Reduce reservation size if sustained usage is consistently lower than commitment</li><li id="""">Consolidate slot reservations across projects to improve pool utilization</li><li id="""">Switch low-concurrency or unpredictable workloads back to on-demand or flex slots</li><li id="""">Implement governance to review reservation adjustments regularly</li><li id="""">Leverage the Reservation Assignment API to dynamically reassign slot pools based on usage</li></ul>",,FALSE,FALSE,,,,<p>GCP-Databases-7511</p>
Inefficient Workflow Design in AWS Step Functions,inefficient-workflow-design-in-aws-step-functions,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941b69c49903b8ce5a87,FALSE,FALSE,Sun Jun 22 2025 23:39:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Jarred Clore,aws,aws-step-functions,misconfiguration,compute,"<p id="""">Improper design choices in AWS Step Functions can lead to unnecessary charges. For example:  * Using <strong id="""">Standard Workflows</strong> for short-lived, high-frequency executions leads to excessive per-transition charges.   * Using <strong id="""">Express Workflows</strong> for long-running processes (close to or exceeding the 5-minute limit) may cause timeouts or retries.   * Inefficient use of states—such as chaining many simple states instead of combining logic into a Lambda function—can increase cost in both workflow types.   * Overuse of payload-passing between states (especially in Express workflows) increases GB-second and data transfer charges.</p>","<p id="""">* *Standard Workflows:* Billed per state transition   * *Express Workflows:* Billed per invocation, execution duration (in GB-seconds), and data transfer      Choosing the wrong workflow type or poorly structuring a workflow (e.g., excessive state transitions) can significantly inflate costs, especially under high execution volumes or when parallel states and payload sizes are not optimized.</p>","<ul id=""""><li id="""">Express workflows frequently hit duration or payload limits</li><li id="""">Short-lived workflows are using Standard instead of Express</li><li id="""">The number of state transitions is unusually high relative to the business logic</li><li id="""">Simple sequential logic could be collapsed into a Lambda or intrinsic function</li><li id="""">Map and Parallel states are overused or not optimized</li><li id="""">Input/output payload sizes between states are larger than necessary</li></ul>","<ul id=""""><li id="""">Choose Express workflows for short-lived, high-volume executions</li><li id="""">Use Standard workflows for long-running, infrequent executions</li><li id="""">Combine simple logic steps into a single Lambda or use intrinsic functions</li><li id="""">Use *ResultPath*, *OutputPath*, and *Parameters* fields efficiently to trim payloads</li><li id="""">Minimize use of unnecessary Map or Parallel states where linear processing suffices</li><li id="""">Benchmark costs and execution patterns using AWS Step Functions metrics</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/step-functions/latest/dg/concepts-standard-vs-express.html"" id="""">https://docs.aws.amazon.com/step-functions/latest/dg/concepts-standard-vs-express.html</a><br><a href=""https://aws.amazon.com/step-functions/pricing/"" id="""">https://aws.amazon.com/step-functions/pricing/</a><br><a href=""https://docs.aws.amazon.com/step-functions/latest/dg/best-practices.html"" id="""">https://docs.aws.amazon.com/step-functions/latest/dg/best-practices.html</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-4309</p>
Inefficient Workload Distribution Across Warehouses,inefficient-workload-distribution-across-warehouses,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4ab6006ad5b82dd4cf,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-virtual-warehouse,underutilized-resource,compute,"<p id="""">Many organizations assign separate Snowflake warehouses to individual business units or teams to simplify chargebacks and operational ownership. This often results in redundant and underutilized warehouses, as workloads frequently do not require the full capacity of even the smallest warehouse size.</p><p id="""">By consolidating compatible workloads onto shared warehouses, organizations can maximize utilization, reduce idle runtime across the fleet, and significantly lower total credit consumption. Cost allocation can still be achieved using Query Billing Attribution.</p>",,"<ul id="""">  <li id="""">Review warehouse usage patterns to identify warehouses operating consistently below concurrency capacity (e.g., low simultaneous query counts).</li>  <li id="""">Evaluate whether workloads running on separate warehouses have compatible scheduling, priority, or SLA needs that allow consolidation.</li>  <li id="""">Assess existing query queuing or resource contention risks before consolidation to avoid performance degradation.</li>  <li id="""">Validate with workload owners that consolidation would not introduce unacceptable performance risks.</li></ul>","<ul id="""">  <li id="""">Consolidate compatible workloads onto shared warehouses to improve overall utilization without sacrificing performance.</li>  <li id="""">Adjust warehouse sizing or enable multi-cluster scaling if necessary to accommodate increased concurrency after consolidation.</li>  <li id="""">Validate SLA and performance expectations with all impacted business units or workload owners prior to consolidation.</li>  <li id="""">Use Query Billing Attribution to maintain accurate cost allocation across teams sharing a consolidated warehouse.</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history"" id="""">Warehouse Load History</a></li><li id=""""><a href=""https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history"" id="""">Query Cost Attribution</a></li></ul>",FALSE,FALSE,,,,<p>Snowflake-Compute-6034</p>
Infrequently Accessed Data Stored in Azure Cosmos DB,infrequently-accessed-data-stored-in-azure-cosmos-db,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4ae4cd169550c6cf82,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Chibueze Eke,azure,azure-cosmos-db,inefficient-storage-tiering,databases,"<p id="""">Azure Cosmos DB is optimized for low-latency, globally distributed workloads—not long-term storage of infrequently accessed data. Yet in many environments, cold data such as logs, telemetry, or historical records is retained in Cosmos DB due to a lack of lifecycle management.</p>","<p id="""">Cosmos DB charges include:</p><ul id=""""><li id="""">Data storage billed per GB per month, across all regions where data is replicated</li><li id="""">Provisioned throughput (RU/s) or autoscale, billed regardless of actual usage</li><li id="""">Backup storage billed separately (if configured)</li><li id="""">Serverless mode available, but subject to limitations on scaling and total RU consumption</li></ul><p id="""">Storing cold data incurs persistent storage charges and may inflate RU requirements unnecessarily.</p>","<ul id=""""><li id="""">Identify Cosmos DB resources (e.g., containers, collections, or tables) with high storage usage but low request volume</li><li id="""">Use Azure Monitor, diagnostic logs, or Workload Insights to assess read/write activity over time</li><li id="""">Evaluate whether the data is actively queried or needed for operational workloads</li><li id="""">Check whether data retention or lifecycle policies are defined for the resource</li></ul>","<ul id=""""><li id="""">Export infrequently accessed data to lower-cost storage services:</li><li id="""">Use Blob Storage Cool for rarely accessed but readily retrievable data</li><li id="""">Use Blob Storage Archive for long-term retention with delayed retrieval</li><li id="""">Use Azure Table Storage for simple key/value access when global distribution is unnecessary</li><li id="""">Delete cold data from Cosmos DB after successful archival</li><li id="""">Implement data lifecycle automation to routinely transition stale data out of Cosmos DB</li></ul>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/cosmos-db/\&quot;"" id="""">Azure Cosmos DB Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/plan-manage-throughput\"" id="""">Azure Cosmos DB Capacity Planning</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview\"" id="""">Azure Blob Storage Access Tiers</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-overview\"" id="""">Azure Table Storage Overview</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Databases-4766</p>
Infrequently Accessed Objects Stored in S3 Standard Tier,infrequently-accessed-objects-stored-in-s3-standard-tier,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b3f02e839f6de1b5ce2,FALSE,FALSE,Thu May 22 2025 14:57:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-s3,inefficient-configuration,storage,"<p id="""">S3 Standard is the default storage class and is often used by default even for data that is rarely accessed. Keeping large volumes of infrequently accessed data in S3 Standard leads to unnecessary costs. Data such as backups, logs, archives, or historical snapshots are often strong candidates for migration to colder tiers like S3 Glacier or Deep Archive. If access patterns are unknown or variable, S3 Intelligent-Tiering can reduce costs without requiring manual transitions.</p>","<p id="""">S3 is billed based on an hourly per-GB storage rate, plus additional charges for access, updates, security features, monitoring, etc. There are multiple “classes” of storage, each with a different cost structure. Generally, there is a tradeoff between monthly per-GB storage rates, and the cost/speed of access and retrieval of data.</p><ul id=""""><li id="""">S3 Standard has the highest storage cost per GB but low access costs, making it best for frequently accessed data.</li><li id="""">S3 Glacier and S3 Glacier Deep Archive offer significantly lower storage costs, with higher retrieval times and fees.</li><li id="""">S3 Intelligent-Tiering automatically moves objects between storage classes based on access patterns, with a small monitoring fee.</li></ul>","<ul id=""""><li id="""">Identify buckets or prefixes where large volumes of data are stored in the S3 Standard tier</li><li id="""">Assess whether the data is actively accessed or retained for archival, compliance, or backup purposes</li><li id="""">Review historical trends to determine whether data access is infrequent, irregular, or absent</li><li id="""">Evaluate the intended lifecycle of the data (e.g., short-term vs. long-term retention requirements)</li><li id="""">Confirm that access latency and retrieval costs associated with lower-cost tiers (e.g., Glacier) align with business needs</li><li id="""">Engage relevant teams (e.g., data owners, compliance leads) to validate retention requirements and assess tolerance for delayed retrieval</li></ul>","<p id="""">Move eligible objects to colder storage tiers (e.g., Glacier Flexible Retrieval or Deep Archive) using lifecycle policies. For datasets with uncertain access patterns, enable S3 Intelligent-Tiering to automatically transition objects across storage classes based on real-world usage. Always validate business, compliance, and restore-time requirements before implementing transitions.</p>","<p id=""""><a href=""https://aws.amazon.com/s3/pricing/\&quot;"" id="""">S3 Pricing</a></p>",FALSE,FALSE,,49,"<p id="""">100 TB of backups stored in S3 Standard that can be migrated to Glacier – Flexible Retrieval:</p><ul id=""""><li id=""""><strong id="""">S3 Standard:</strong> $0.022/GB-month = $2,200/mo</li><li id=""""><strong id="""">S3 Glacier Flexible Retrieval:</strong> $0.0036/GB-month = $360/mo</li><li id=""""><strong id="""">Savings:</strong> $1,840/mo (84% Savings)</li></ul>",<p>AWS-Storage-7015</p>
Lack of Deduplication and Change Block Tracking in AWS Backup,lack-of-deduplication-and-change-block-tracking-in-aws-backup,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941a3c5b583be2ee24da,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Bhanu Kiran,aws,aws-backup,underutilization,storage,"<p id="""">AWS Backup does not natively support global deduplication or change block tracking across backups. As a result, even traditional incremental or differential backup strategies (e.g., daily incremental, weekly full) can accumulate redundant data. Over time, this leads to higher-than-necessary storage usage and cost — especially in environments with frequent backup schedules or large data volumes that only change minimally between snapshots.  While some third-party agents can implement CBT and deduplication at the client level, AWS Backup alone offers no built-in mechanism to avoid storing unchanged data across backup generations.</p>","<p id="""">Backups are billed per GB-month of backup storage. This includes both warm storage (primary backup copies) and cold storage (if lifecycle policies transition backups to cold tiers). Charges are accrued for each full or incremental backup, regardless of data change volume. Lack of deduplication or CBT at the platform level can inflate backup sizes and recurring costs.</p>","<ul id=""""><li id="""">Review high-frequency backup schedules (e.g., daily or hourly) across services like EBS, FSx, and RDS</li><li id="""">Analyze backup size growth relative to underlying data changes</li><li id="""">Identify environments using only AWS-native backup with no CBT or deduplication in place</li><li id="""">Check for repetitive full backups that duplicate mostly unchanged data</li><li id="""">Assess the use of cold storage lifecycle policies, which may mask inefficiencies but not resolve them</li></ul>","<ul id=""""><li id="""">Where supported, leverage third-party backup tools with CBT and deduplication (e.g., Commvault, Veeam, Druva)</li><li id="""">Reevaluate backup frequency and retention periods based on RPO/RTO requirements</li><li id="""">Consolidate redundant backup plans across environments and services</li><li id="""">Implement tighter backup scoping to exclude unchanged or non-critical volumes</li><li id="""">Monitor backup growth trends to detect inefficient accumulation of redundant data</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html"" id="""">https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html</a><br><a href=""https://aws.amazon.com/backup/pricing/"" id="""">https://aws.amazon.com/backup/pricing/</a><br><a href=""https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html#whatisbackup-how"" id="""">https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html\#whatisbackup-how</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-1682</p>
Lack of Functional Cost Attribution in Databricks Workloads,lack-of-functional-cost-attribution-in-databricks-workloads,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b483956b57320da5b9d,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Benjamin van der Maas,databricks,databricks,visibility-gap,other,"<p id="""">Databricks cost optimization begins with visibility. Unlike traditional IaaS services, Databricks operates as an orchestration layer spanning compute, storage, and execution — but its billing data often lacks granularity by workload, job, or team. This creates a visibility gap: costs fluctuate without clear root causes, ownership is unclear, and optimization efforts stall due to lack of actionable insight. When costs are not attributed functionally — for example, to orchestration (query/job DBUs), compute (cloud VMs), storage, or data transfer — it becomes difficult to pinpoint what’s driving spend or where improvements can be made. As a result, inefficiencies persist not due to a single misconfiguration, but because the system lacks the structure to surface them.</p>","<p id="""">Databricks costs are composed of:</p><ul id=""""><li id="""">Databricks Units (DBUs): Charged per second of execution based on workload type and cluster configuration</li><li id="""">Underlying compute costs: Passed through from AWS, Azure, or GCP (e.g., EC2, VMs)</li><li id="""">Storage and data transfer: Separate charges based on object store access and inter-service communication</li></ul><p id="""">Without clear attribution, these components blend together, obscuring usage patterns and hiding optimization opportunities.</p>","<p id="""">Review billing exports and internal dashboards for Databricks to determine whether spend is broken down by:</p><ul id=""""><li id="""">Workload type (interactive, job, SQL)</li><li id="""">Component (DBU vs. cloud infrastructure vs. transfer)</li><li id="""">Team or business unit</li></ul><p id="""">Check whether tags, cluster names, or workspace structures allow attribution Look for teams reporting fluctuating or opaque Databricks costs without clear levers to act on them</p>","<p id="""">Break Databricks costs into functional layers to establish traceability and accountability:</p><ul id=""""><li id="""">Orchestration (DBUs): Analyze query/job-level execution and optimize workload design</li><li id="""">Compute: Review underlying VM types and cost models (e.g., Spot, RI, Savings Plans)</li><li id="""">Storage: Align S3/ADLS/GCS usage with lifecycle policies and avoid excessive churn</li><li id="""">Data Transfer: Identify cross-region or cloud egress patterns driving hidden charges</li></ul><p id="""">Implement job naming conventions, tagging standards, or workspace isolation to support attribution Build dashboards or reports that expose per-team or per-function Databricks spend Use structured cost data as the foundation for deeper optimization of queries, clusters, and data movement</p>","<ul id=""""><li id="""">Databricks Pricing Overview</li><li id="""">Best Practices for Managing Databricks Costs</li></ul>",FALSE,FALSE,,,,<p>Databricks-Other-6847</p>
Lack of Graviton Usage in Databricks Clusters,lack-of-graviton-usage-in-databricks-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b472d677a5f43b910be,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Matt Weingarten,databricks,databricks-clusters,suboptimal-instance-selection,compute,"<p id="""">Databricks supports AWS Graviton-based instances for most workloads, including Spark jobs, data engineering pipelines, and interactive notebooks. These instances offer significant cost advantages over traditional x86-based VMs, with comparable or better performance in many cases. When teams default to legacy instance types, they miss an easy opportunity to reduce compute spend. Unless workloads have known compatibility issues or specialized requirements, Graviton should be the default instance family used in Databricks Clusters.</p>","<p id="""">Databricks compute is billed based on:</p><ul id=""""><li id="""">Databricks Units (DBUs): Determined by cluster configuration, including instance type</li></ul><ul id=""""><li id="""">AWS Infrastructure Charges: Graviton-based instances (e.g., m6g, r6g, c6g) are generally priced lower than their x86 equivalents, billed per second or minute</li></ul><p id="""">Choosing x86-based instances when Graviton would suffice leads to higher infrastructure costs for equivalent performance.</p>","<ul id=""""><li id="""">Query cluster configurations to identify use of non-Graviton (x86-based) instance types</li><li id="""">Check for absence of Graviton enforcement in workspace-level compute policies</li><li id="""">Review workload compatibility for clusters running frequently on x86</li><li id="""">Engage with teams to confirm whether Graviton instances have been tested or benchmarked</li></ul>","<ul id=""""><li id="""">Monitor utilized instance types and recommend Graviton-based families</li><li id="""">Reconfigure default cluster templates to use Graviton by default</li><li id="""">Allow exceptions only for workloads with documented compatibility or performance issues</li></ul>","<ul id=""""><li id=""""><a href=""https://www.databricks.com/blog/2022/04/18/announcing-databricks-support-for-aws-graviton2-with-up-to-3x-better-price-performance.html"" id="""">Databricks Support for Graviton Instances</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/graviton/"" id="""">AWS Graviton Overview</a></li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-5609</p>
Lack of Workload-Specific Cluster Segmentation,lack-of-workload-specific-cluster-segmentation,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84ca702846f9c3d27ac,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Nicole Boyd,databricks,databricks-compute,inefficient-configuration,compute,"<p id="""">Running varied workload types (e.g., ETL pipelines, ML training, SQL dashboards) on the same cluster introduces inefficiencies. Each workload has different runtime characteristics, scaling needs, and performance sensitivities. When mixed together, resource contention can degrade job performance, increase cost, and obscure cost attribution.</p><p id="""">ETL jobs may overprovision memory, while lightweight SQL queries may trigger unnecessary cluster scale-ups. Job failures or retries may increase due to contention, and queued jobs can further inflate runtime costs. Without clear segmentation, teams lose the ability to tune environments for specific use cases or monitor workload-specific efficiency.</p>","<p id="""">Databricks charges per-node-hour using Databricks Units (DBUs), with different rates based on cluster type and configuration. When disparate workloads share a single cluster — especially in all-purpose clusters — compute is inefficiently allocated, jobs contend for resources, and DBU consumption can spike due to overprovisioning, retry inflation, or queuing delays.</p>","<ul id=""""><li id="""">Identify all-purpose or shared clusters that execute a wide range of job types</li><li id="""">Look for clusters with inconsistent scaling or job runtimes</li><li id="""">Review job metadata (e.g., task type, frequency, team owner) and compare to cluster setup</li><li id="""">Examine cluster tags and naming conventions for clarity on intended usage</li><li id="""">Check whether long-lived clusters are being used by multiple teams or pipelines</li></ul>","<ul id=""""><li id="""">Define and enforce separate cluster types for distinct workload categories (e.g., SQL, ML, ETL)</li><li id="""">Encourage the use of job clusters for short-lived, batch-oriented workloads to ensure clean isolation and efficient resource use</li><li id="""">Use job clusters for single-purpose, short-lived jobs to ensure isolation and efficient spin-up</li><li id="""">Apply strict cluster tagging and naming standards to reflect usage intent</li><li id="""">Implement cluster policies that restrict configuration options based on workload class</li><li id="""">Educate platform users on workload characteristics and recommend cluster segmentation best practices</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.databricks.com/aws/en/admin/clusters/policies"" id="""">Cluster Policies</a></li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-7521</p>
Lifecycle Visibility Gaps Inflating Renewal Costs in Azure Marketplace,lifecycle-visibility-gaps-inflating-renewal-costs-in-azure-marketplace,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58e4e63e1fcc902f55e,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Alexa Abbruscato,azure,azure-marketplace,contract-lifecycle-mismanagement,other,"<p id="""">When Marketplace contracts or subscriptions expire or change without visibility, Azure may automatically continue billing at higher on-demand or list prices. These lapses often go unnoticed due to lack of proactive tracking, ownership, or renewal alerts, resulting in substantial cost increases. The issue is amplified when contract records are siloed across procurement, finance, and engineering teams, with no centralized mechanism to monitor entitlement status or reconcile expected versus actual billing.</p>","<p id="""">Subscription- or contract-based pricing through the Azure Marketplace; reverts to on-demand or list rates upon entitlement expiration.</p>","<ul id=""""><li id="""">Check for sudden cost spikes associated with Marketplace subscriptions or vendors</li><li id="""">Review historical entitlement terms and compare current rates with prior contracted pricing</li><li id="""">Identify subscriptions with undefined or lapsed renewal terms</li><li id="""">Audit billing periods immediately following contract end dates for unexpected pricing changes</li><li id="""">Determine whether contract ownership and renewal tracking responsibilities are formally assigned</li></ul>","<ul id=""""><li id="""">Assign clear ownership of Marketplace contracts across business, finance, or procurement teams</li><li id="""">Set calendar-based and system-based reminders for contract renewals and entitlement expiration</li><li id="""">Regularly reconcile Azure billing data with vendor-provided SLA or entitlement terms</li><li id="""">Use Azure Cost Management and programmatic access to subscription metadata to track expiration timelines</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/marketplace/marketplace-faq-purchasing"" id="""">https://learn.microsoft.com/en-us/azure/marketplace/marketplace-faq-purchasing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/marketplace/marketplace-faq-publisher#how-long-does-an-offer-remain-available"" id="""">https://learn.microsoft.com/en-us/azure/marketplace/marketplace-faq-publisher#how-long-does-an-offer-remain-available</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/understand-azure-marketplace-charges"" id="""">https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/understand-azure-marketplace-charges</a></li></ul>",FALSE,FALSE,,N/A,,<p>Azure-Other-4314</p>
Logging Buckets in Non-Production Environments Storing Info Logs,logging-buckets-in-non-production-environments-storing-info-logs-1edcf,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80dffb9168f186b8ce9a,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:20 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Yuval Goldstein,gcp,gcp-cloud-logging,excessive-ingestion-of-low-value-logs,other,"<p id="""">Non-production environments frequently generate INFO-level logs that capture expected system behavior or routine API calls. While useful for troubleshooting in development, they rarely need to be retained. Allowing all INFO logs to be ingested and stored in Logging buckets across dev or staging environments can lead to disproportionate ingestion and storage costs. This inefficiency often persists because log routing and severity filters are not differentiated between production and non-production projects.</p>","<p id="""">Cloud Logging charges for data ingestion and retained storage beyond the free tier. INFO-level logs can represent the majority of log volume, and in non-production environments — where they rarely drive operational decisions — these costs provide little return on value.</p>","<ul id=""""><li id="""">Identify non-production projects with Logging buckets configured to capture all log severities, including INFO</li><li id="""">Review ingestion volume by log severity to quantify the share of INFO-level logs</li><li id="""">Assess whether INFO logs are ever queried or analyzed in non-production environments</li><li id="""">Determine if sink filters or routing policies apply uniform rules across production and non-production projects</li></ul>","<ul id=""""><li id="""">Adjust sink filters to exclude INFO-level logs in non-production environments</li><li id="""">Establish environment-specific logging policies that retain only WARNING and ERROR logs in dev and staging projects</li><li id="""">Route necessary INFO logs temporarily to low-cost storage (e.g., Cloud Storage with limited retention) for debugging purposes</li><li id="""">Periodically review ingestion patterns to ensure filters remain aligned with operational needs and cost targets</li></ul>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/logging/docs/exclusions"" id="""">Cloud Logging Exclusions and Filters</a></li><li id=""""><a href=""https://cloud.google.com/stackdriver/pricing"" id="""">Cloud Logging Pricing</a></li></ul>",FALSE,FALSE,,,,GCP-Other-2637
Long-Retained Azure Snapshot,long-retained-azure-snapshot,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4aabb5f410c703a6e1,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Anderson Oliveira,azure,azure-snapshots,retained-unused-resource,storage,"<p id="""">Snapshots are often created for short-term protection before changes to a VM or disk, but many remain in the environment far beyond their intended lifespan. Over time, this leads to an accumulation of snapshots that are no longer associated with any active resource or retained for operational need.Since Azure does not enforce automatic expiration or lifecycle policies for snapshots, they can persist indefinitely and continue to incur monthly storage charges. This inefficiency is especially common in development environments, migration efforts, or manual backup workflows that lack centralized cleanup.Snapshots older than 30–90 days, especially those not tied to a documented backup strategy or workload, are strong candidates for review and removal.</p>","<ul id=""""><li id="""">Snapshots are billed based on the amount of data stored (per GB/month)</li><li id="""">Charges accrue regardless of whether the snapshot is used or linked to an active disk</li><li id="""">Incremental snapshots are more space-efficient but still incur storage cost</li></ul>","<ul id=""""><li id="""">List all snapshots in the subscription</li><li id="""">Filter for snapshots not associated with any currently active managed disk</li><li id="""">Review creation timestamps to identify snapshots older than a defined threshold (e.g., 30 or 90 days)</li><li id="""">Cross-reference with project timelines, VM decommissioning dates, or change records to validate relevance</li><li id="""">Use Azure Resource Graph or scripting to automate identification at scale</li></ul>","<ul id=""""><li id="""">Manually review long-retained snapshots with application or infrastructure owners</li><li id="""">Delete snapshots no longer needed for recovery, rollback, or compliance retention</li><li id="""">Adopt tagging standards to track purpose, owner, and expected retention period at time of snapshot creation</li><li id="""">Incorporate snapshot lifecycle reviews into infrastructure decommissioning or backup policy audits</li></ul>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/managed-disks/\&quot;"" id="""">Azure Snapshot Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/disks-snapshots\"" id="""">Azure Snapshot Overview</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Storage-8855</p>
Long-Retained RDS Manual Snapshot,long-retained-rds-manual-snapshot,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b41f64ac241a1ad8b4d,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-rds,unused-resource,databases,"<p id="""">Manual snapshots are often created for operational tasks like upgrades, migrations, or point-in-time backups. Unlike automated backups, which are automatically deleted after a set retention period, manual snapshots remain in place until explicitly deleted. Over time, this can lead to an accumulation of snapshots that are no longer needed but still incur monthly storage charges. This is particularly common in environments where snapshots are taken frequently but not consistently reviewed. If left unmanaged, manual snapshots can become a source of ongoing cost, especially for large databases or when snapshots are copied across regions.</p>","<p id="""">Amazon RDS snapshot storage is billed based on the total volume of data retained across all snapshots, measured in GB per month. AWS uses incremental storage under the hood, meaning only the changes between snapshots are stored. However, you're charged for the full set of unique data blocks needed to restore each snapshot, and these charges persist until the snapshot is deleted. There are two types of snapshots:</p><ul id=""""><li id="""">Automated backups are managed by AWS and retained for a limited time (up to 35 days). A portion of automated backup storage is included at no additional charge, up to the size of your provisioned database storage.</li><li id="""">Manual snapshots are created by the user and retained indefinitely unless explicitly deleted. They are not covered by the free backup quota and are billed fully for the storage they consume. If multiple manual snapshots exist, the total storage billed includes all unique data needed across them.</li></ul>","<ul id=""""><li id="""">List all manual RDS snapshots across regions and accounts</li><li id="""">Identify snapshots that exceed a predefined age threshold (e.g., 30, 60, or 90 days)</li><li id="""">Check whether snapshots are tied to deleted or decommissioned database instances</li><li id="""">Evaluate whether any snapshots are shared or copied across regions, increasing costs</li><li id="""">Coordinate with database owners or teams to confirm which snapshots are still needed for recovery, audit, or compliance purposes</li></ul>","<p id="""">Delete manual snapshots that are no longer required. Before removal, confirm that the data is not needed for restore workflows or long-term retention. Establish internal policies or automation to routinely review and clean up aged snapshots. If long-term storage is necessary, evaluate exporting data to object storage formats that may offer lower-cost archival options.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/rds/pricing/"" id="""">RDS Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithBackups.Managing"" id="""">Managing RDS Snapshots</a></li></ul>",FALSE,TRUE,,138,,<p>AWS-Databases-3032</p>
Managed Disk Attached to a Deallocated VM,managed-disk-attached-to-a-deallocated-vm,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b43ae00e2b0e2a09ead,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,azure,azure-managed-disks,unused-resource,storage,"<p id="""">This inefficiency occurs when a VM is deallocated but its attached managed disks are still active and incurring storage charges. While compute billing stops for deallocated VMs, the disks remain provisioned and billable. These disks often persist unintentionally after a VM is paused, retired, or left unused in dev/test environments, resulting in waste if not explicitly cleaned up.</p>","<p id="""">Managed Disks are billed per GB of provisioned size, regardless of usage or attachment state. Disks continue to incur costs even when their associated VM is deallocated. Additional charges may apply for features such as disk snapshots, premium performance tiers, or zone-redundant storage.</p>","<ul id=""""><li id="""">Identify managed disks attached to deallocated VMs during the defined lookback period</li><li id="""">Review disk activity to confirm no read/write operations occurred while the VM was deallocated</li><li id="""">Evaluate whether the disk is still needed for backup, migration, or future reactivation</li><li id="""">Check whether the VM and disk are associated with an active project or retired environment</li><li id="""">Confirm with owners or stakeholders whether the data must be retained or can be removed</li></ul>","<p id="""">Delete managed disks that are no longer needed. If data may be needed in the future, take a snapshot or export the disk before deletion. Establish regular cleanup processes to review disks attached to deallocated VMs. Automate tagging and audit routines to flag idle resources for follow-up.</p>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/managed-disks/\&quot;"" id="""">Azure Managed Disks Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/windows/disks-types#clean-up\"" id="""">Azure Disk Cleanup</a></li></ul>",FALSE,FALSE,,122,,<p>Azure-Storage-8052</p>
Managed Disk Attached to a Stopped VM,managed-disk-attached-to-a-stopped-vm,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b46f77ba902a30b3475,FALSE,FALSE,Thu May 22 2025 14:57:10 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,azure,azure-managed-disks,unused-resource,storage,"<p id="""">Disks attached to VMs that have been stopped for an extended period, particularly when showing no read or write activity, may indicate abandoned infrastructure or obsolete resources. Retaining these disks without validation leads to unnecessary monthly storage costs. Reviewing and cleaning up inactive disks helps optimize spend and maintain storage hygiene.</p>","<p id="""">Managed Disks are billed per provisioned GB per month, regardless of whether the associated virtual machine is running. Stopped VMs do not release or deallocate attached disks, resulting in ongoing storage charges for unused capacity.</p>","<ul id=""""><li id="""">Identify Managed Disks attached to virtual machines that have remained in a stopped state over a representative time window</li><li id="""">Analyze disk activity metrics to detect absence of read/write operations during the lookback period</li><li id="""">Review VM metadata, ownership tags, and decommissioning records to assess whether the disk is still required</li><li id="""">Check compliance and backup retention requirements before initiating deletion activities</li><li id="""">Validate findings with infrastructure owners or application teams to avoid accidental loss of necessary data</li></ul>","<p id="""">Delete Managed Disks confirmed to be redundant or obsolete to eliminate unnecessary storage charges. Where long-term retention is required, create a snapshot of the disk or export its contents to a lower-cost storage option before deletion. Incorporate disk ownership tracking and regular audits into operational practices to minimize future accumulation of unused resources.</p>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types"" id="""">Azure Managed Disks Overview</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/managed-disks/"" id="""">Azure Managed Disks Pricing</a></li></ul>",FALSE,FALSE,,124,,<p>Azure-Storage-3227</p>
Managed NAT Gateway with Excessive Data Transfer,managed-nat-gateway-with-excessive-data-transfer,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b41a96194f6d3c73138,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-nat-gateway,inefficient-architecture,networking,"<p id="""">NAT Gateways are convenient for enabling outbound access from private subnets, but in data-intensive environments, they can quietly become a major cost driver. When large volumes of traffic flow through the gateway—particularly during batch processing, frequent software updates, or hybrid cloud integrations—the per-GB charges accumulate rapidly. In some cases, replacing a managed NAT Gateway with a self-managed NAT instance can substantially reduce costs, provided that the organization is prepared to operate and maintain the alternative solution.</p>","<p id="""">NAT Gateway pricing includes:</p><ul id=""""><li id="""">Hourly cost per deployed gateway per Availability Zone</li><li id="""">Per-GB data processing fees for all traffic routed through the gateway</li></ul><p id="""">These charges apply regardless of usage pattern and can scale significantly in high-throughput environments.</p>","<ul id=""""><li id="""">Identify NAT Gateways with consistently high data processing volumes over the lookback period</li><li id="""">Review per-GB transfer charges to assess whether NAT Gateway usage represents a significant portion of total networking costs</li><li id="""">Determine whether traffic patterns are driven by expected workload behavior or architectural inefficiencies</li><li id="""">Evaluate whether alternative designs—such as VPC endpoints or NAT instances—could reduce data processing costs</li><li id="""">Consider that replacing a managed NAT Gateway with a self-managed NAT instance introduces operational overhead (e.g., scaling, patching, monitoring).</li><li id="""">Confirm whether the potential savings outweigh the additional management effort</li></ul>","<p id="""">In environments with large volumes of outbound traffic, consider replacing the managed NAT Gateway with a self-managed NAT instance to reduce per-GB data processing costs. Alternatively, evaluate architectural changes to reduce or avoid NAT usage altogether—such as using VPC endpoints for AWS service access. Weigh the trade-offs between cost savings and the operational complexity of managing your own NAT infrastructure.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/vpc/pricing/#nat_gateway"" id="""">AWS NAT Gateway Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html"" id="""">NAT Gateway vs. NAT Instance Comparison</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Networking-8089</p>
Misaligned S3 Storage Tier Selection Based on Access Patterns,misaligned-s3-storage-tier-selection-based-on-access-patterns,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589418db0f8dcba0ecf2af,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),John Webb,aws,aws-s3,misconfigured-storage-tier,storage,"<p id="""">While moving objects to colder storage classes like Glacier or Infrequent Access (IA) can reduce storage costs, premature transitions without analyzing historical access patterns can lead to unintended expenses. Retrieval charges, restore time delays, and early delete penalties often go unaccounted for in simplistic tiering decisions. This inefficiency arises when teams default to colder tiers based solely on perceived “age” of data or storage savings—without confirming access frequency, restore time SLAs, or application requirements.  Unlike inefficiencies focused on *underuse* of cold storage, this inefficiency reflects *overuse* or misalignment, resulting in higher total costs or operational friction.</p>","<p id="""">S3 is billed by storage class, with additional charges for data retrieval, API requests, and minimum storage durations in cold tiers. While cold storage (e.g., Glacier, Infrequent Access) offers lower per-GB storage pricing, retrieval costs and minimum duration charges can outweigh savings if access patterns are not well understood or if data is accessed frequently or unpredictably.</p>","<ul id=""""><li id="""">Review historical access patterns for S3 buckets before applying lifecycle transitions to colder storage classes</li><li id="""">Evaluate whether buckets or prefixes in IA or Glacier tiers are being accessed more frequently than expected</li><li id="""">Check for retrieval operations, restore requests, or early delete penalties that may offset storage savings</li><li id="""">Assess whether application SLAs or workloads are sensitive to Glacier restore latency</li><li id="""">Identify transitions that occur uniformly across all objects without regard to access variability</li></ul>","<ul id=""""><li id="""">Use S3 Storage Lens or CUR data to analyze per-object or per-prefix access frequency before applying lifecycle transitions</li><li id="""">Apply intelligent-tiering selectively where access patterns are unpredictable</li><li id="""">Avoid bulk transitions to IA or Glacier for data with unclear or variable access characteristics</li><li id="""">Regularly audit lifecycle policies and revise based on actual usage patterns</li><li id="""">Educate teams on the full cost model of cold storage (e.g., retrieval fees, early deletion penalties, restore time delays)</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"" id="""">Amazon S3 Storage Classes</a><br><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html"" id="""">S3 Lifecycle Configuration</a><br><a href=""https://aws.amazon.com/s3/pricing/"" id="""">S3 Pricing</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-2410</p>
Missing Auto-Termination Policy for Databricks Clusters,missing-auto-termination-policy-for-databricks-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b495394d0355a79a5db,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Jason Eckle,databricks,databricks-clusters,missing-safeguard,compute,"<p id="""">In many environments, users launch Databricks clusters for development or analysis and forget to shut them down after use. When no auto-termination policy is configured, these clusters remain active indefinitely, incurring unnecessary charges for both Databricks and cloud infrastructure usage. This inefficiency is especially common in interactive clusters that are user-managed, ephemeral, or exploratory in nature. While Databricks provides built-in support for cluster auto-termination, teams often overlook it unless it's enforced through workspace policies. Without this safeguard in place, idle clusters can persist unnoticed for hours or days, leading to avoidable cost.</p>","<p id="""">Databricks clusters accrue cost per second through:</p><ul id=""""><li id="""">Databricks Unit (DBU) charges — vary by workload type (interactive, job, SQL)</li></ul><p id="""">Underlying cloud compute — billed through the host cloud provider (e.g., EC2, Azure VMs) Clusters without auto-termination continue to run — and generate cost — even if idle or abandoned.</p>","<ul id=""""><li id="""">Identify clusters that do not have auto-termination enabled</li><li id="""">Check for clusters with long idle times and no active workloads</li><li id="""">Analyze cost reports to detect charges from underutilized or inactive clusters</li><li id="""">Review workspace-level cluster policies and defaults to ensure consistent enforcement</li></ul>","<ul id=""""><li id="""">Enable auto-termination for all clusters that do not require persistent runtime</li><li id="""">Set cluster policies to require auto-termination configuration for new clusters</li><li id="""">Establish reasonable inactivity thresholds based on workload type (e.g., 30–60 minutes for interactive)</li><li id="""">Educate users on the financial impact of idle clusters and the role of auto-termination as a cost control mechanism</li></ul>",,FALSE,FALSE,,,,<p>Databricks-Compute-2210</p>
Missing Autoclass on GCS Bucket,missing-autoclass-on-gcs-bucket,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b46d1ac1e662f8052c4,FALSE,FALSE,Thu May 22 2025 14:57:10 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),,gcp,gcp-gcs,inefficient-configuration,storage,"<p id="""">Buckets without Autoclass enabled can accumulate infrequently accessed data in more expensive storage classes, inflating monthly costs. Enabling Autoclass allows GCS to automatically move objects to lower-cost tiers based on observed access behavior, optimizing storage costs without manual lifecycle policy management. Activating Autoclass reduces operational overhead while maintaining seamless access to objects across storage classes.</p>","<p id="""">GCS charges for data stored based on the selected storage class. Standard, Nearline, and Coldline classes offer different pricing tiers optimized for access frequency. Without Autoclass, objects may remain in higher-cost tiers even when usage patterns change, leading to unnecessary storage expenses. Autoclass dynamically transitions objects between storage classes based on access frequency without impacting performance or requiring operational effort.</p>","<ul id=""""><li id="""">Identify GCS buckets where Autoclass is not enabled</li><li id="""">Review object access patterns to confirm a mix of frequently and infrequently accessed data</li><li id="""">Assess current storage class distribution to identify potential inefficiencies</li><li id="""">Evaluate whether manual lifecycle management policies exist or are absent</li><li id="""">Validate that enabling Autoclass aligns with data retention and compliance requirements</li></ul>","<p id="""">Enable Autoclass on eligible GCS buckets to automate storage class transitions based on real-time access patterns. Review and adjust any existing lifecycle rules to prevent conflicts. Note that once Autoclass is enabled, it cannot be disabled without recreating the bucket. Monitor storage cost trends after activation to verify expected savings and operational improvements.</p>","<ul id=""""><li id="""">Google Cloud Storage Autoclass Documentation</li><li id="""">Google Cloud Storage Pricing</li></ul>",FALSE,FALSE,,108,,<p>GCP-Storage-8607</p>
Missing Delta Optimization Features for High-Volume Tables,missing-delta-optimization-features-for-high-volume-tables,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941a432e579dce524a39,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Scott Shulman,databricks,delta-lake,suboptimal-data-layout,storage,"<p id="""">In many Databricks environments, large Delta tables are created without enabling standard optimization features like partitioning and Z-Ordering. Without these, queries scanning large datasets may read far more data than necessary, increasing execution time and compute usage.  * <strong id="""">Partitioning</strong> organizes data by a specified column to reduce scan scope.   * <strong id="""">Z-Ordering</strong> optimizes file sorting to minimize I/O during range queries or filters.   * <strong id="""">Delta Format</strong> enables additional optimizations like data skipping and compaction.  Failing to use these features in high-volume tables often results in avoidable performance overhead and elevated spend, especially in environments with frequent exploratory queries or BI workloads.</p>","<p id="""">Databricks charges are based on DBUs (Databricks Units) per hour, which correlate directly with compute resource use. Query performance heavily impacts DBU consumption. Inefficient data layout leads to longer scan times, increased cluster runtime, and higher costs.</p>","<ul id=""""><li id="""">Tables lacking partitioning on commonly filtered columns</li><li id="""">Absence of Z-Ordering on high-selectivity columns (e.g., timestamps, IDs)</li><li id="""">Slow query performance tied to full-table scans</li><li id="""">High DBU usage by queries reading large volumes of data unnecessarily</li><li id="""">ETL pipelines writing to Delta tables without compaction or OPTIMIZE steps</li></ul>","<ul id=""""><li id="""">Apply partitioning when writing Delta tables, using columns commonly filtered in queries</li><li id="""">Enable Z-Ordering on appropriate columns to improve data skipping efficiency</li><li id="""">Use `OPTIMIZE` and `VACUUM` to reduce file fragmentation and improve query performance</li><li id="""">Standardize use of Delta Lake format in ETL pipelines</li><li id="""">Automate periodic optimizations for long-lived tables based on size or access patterns</li></ul>","<p id=""""><a href=""https://learn.microsoft.com/en-us/azure/databricks/delta/optimizations/file-management"" id="""">Optimize performance with file management</a><br><a href=""https://learn.microsoft.com/en-us/azure/databricks/delta/optimizations/optimization-examples"" id="""">Partitioning and Z-Ordering</a><br><a href=""https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-optimize"" id="""">Databricks Delta Table OPTIMIZE Command</a></p>",FALSE,FALSE,,,,<p>Databricks-Storage-7131</p>
Missing Intelligent-Tiering on EFS Lifecycle Policy,missing-intelligent-tiering-on-efs-lifecycle-policy,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894155cb321668d618411,FALSE,FALSE,Sun Jun 22 2025 23:39:01 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Charles Haile,aws,aws-efs,suboptimal-lifecycle-configuration,storage,"<p id="""">EFS offers lifecycle policies that transition files from the Standard tier to Infrequent Access (IA) based on inactivity, significantly reducing storage costs for cold data. When this feature is not enabled, infrequently accessed files remain in the more expensive Standard tier indefinitely. This often occurs when the file system is initially provisioned for performance but long-term access patterns are not reevaluated.</p>","<p id="""">Billed per GB-month based on storage class (e.g., Standard, Infrequent Access); access and metadata requests may incur additional charges</p>","<ul id=""""><li id="""">Review if lifecycle management is disabled or not set to transition files after an appropriate period of inactivity</li><li id="""">Evaluate the proportion of files that are infrequently accessed yet stored in the Standard tier</li><li id="""">Check whether the file system is used by workloads with predictable cold data patterns (e.g., logs, archive, ML input files)</li><li id="""">Determine if storage costs are dominated by data that has not been accessed in weeks or months</li><li id="""">Confirm whether business requirements permit longer access latencies for cold data</li></ul>","<ul id=""""><li id="""">Enable EFS lifecycle management with a transition period (e.g., 7, 14, 30 days) aligned to actual data access patterns</li><li id="""">Monitor and periodically review access logs or file activity metrics to refine lifecycle timing</li><li id="""">If access latency is not a concern, consider more aggressive transitions to IA for archival-type data</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management.html"" id="""">AWS EFS Lifecycle Management</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-8760</p>
Missing Lifecycle Policy on Object Storage,missing-lifecycle-policy-on-object-storage,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589417023ce2946d297c23,FALSE,FALSE,Sun Jun 22 2025 23:39:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),,oci,oci-object-storage,missing-cost-control-configuration,storage,"<p id="""">Without lifecycle policies, data in OCI Object Storage remains in the default storage tier indefinitely—even if it is rarely accessed. This can lead to growing costs from unneeded or rarely accessed data that could be expired or transitioned to lower-cost tiers like Archive Storage.</p>","<p id="""">Charged based on object storage tier and total GB stored per month.</p>","<ul id=""""><li id="""">Review buckets lacking lifecycle policies or where all objects remain in standard tier</li><li id="""">Analyze object creation timestamps and last access data (if available)</li><li id="""">Identify buckets used for logs, backups, or system exports where retention can be limited</li><li id="""">Check with teams to validate retention requirements for each bucket</li></ul>","<ul id=""""><li id="""">Create lifecycle rules to transition older objects to Archive Storage</li><li id="""">Set expiration policies for data older than required retention thresholds</li><li id="""">Standardize lifecycle policies across log or backup buckets</li></ul>","<p id=""""><a href=""https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/usinglifecyclepolicies.htm"" id="""">Object Lifecycle Management</a></p>",FALSE,FALSE,,,,<p>OCI-Storage-1444</p>
Missing Lifecycle Policy on Replicated EFS File System,missing-lifecycle-policy-on-replicated-efs-file-system,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589415bc6b6874a484015d,FALSE,FALSE,Sun Jun 22 2025 23:39:01 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Charles Haile,aws,aws-efs,misconfiguration,storage,"<p id="""">When replicating an EFS file system across AWS regions (e.g., for disaster recovery), the destination file system does not automatically inherit the source’s lifecycle policy. As a result, files replicated to the destination will remain in the Standard storage class unless a new lifecycle policy is explicitly configured. Over time, this can lead to significantly higher storage costs, particularly in DR environments where data is rarely accessed but still replicated in full.</p>","<p id="""">EFS charges are based on the amount of data stored in each storage class (Standard and Infrequent Access). Transitioning infrequently accessed files to the IA tier reduces storage costs significantly. However, if a lifecycle policy is not applied, all files remain in the more expensive Standard tier, including on replicated file systems.</p>","<ul id=""""><li id="""">Review whether the destination EFS file system has a lifecycle policy applied following replication</li><li id="""">Determine if the destination file system contains large volumes of infrequently accessed or untouched data</li><li id="""">Evaluate whether the data in the DR region is being accessed regularly or retained only for redundancy purposes</li><li id="""">Confirm whether the storage class distribution (Standard vs. IA) is optimized for access patterns in the replicated environment</li><li id="""">Assess if lifecycle policies were applied manually post-replication or were omitted entirely</li></ul>","<ul id=""""><li id="""">Manually apply a lifecycle policy to the destination EFS file system after replication is configured</li><li id="""">Align the policy with the source file system or tune it based on expected access frequency in the DR region</li><li id="""">Periodically audit replicated EFS environments to ensure lifecycle policies remain in place as environments evolve</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/efs/latest/ug/efs-replication.html"" id="""">Amazon EFS Replication Overview</a><br><a href=""https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html"" id="""">Amazon EFS Lifecycle Management</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-7148</p>
Missing Performance Plus on Eligible Managed Disks,missing-performance-plus-on-eligible-managed-disks,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b48eea36dec99f431b4,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-managed-disks,misconfiguration,storage,"<p id="""">For Premium SSD and Standard SSD disks 513 GiB or larger, Azure now offers the option to enable Performance Plus — unlocking higher IOPS and MBps at no extra cost. Many environments that previously required custom performance settings continue to pay for additional throughput unnecessarily. By not enabling Performance Plus on eligible disks, organizations miss a straightforward opportunity to reduce disk spend while maintaining or improving performance. The feature is opt-in and must be explicitly enabled on each qualifying disk.</p>","<p id="""">Azure Managed Disks are billed based on:</p><ul id=""""><li id="""">Provisioned disk size (GB/month)</li><li id="""">Disk type (e.g., Premium SSD, Standard SSD, Ultra)</li><li id="""">Additional performance settings, if configured manually</li></ul><p id="""">Performance Plus allows eligible disks to receive enhanced performance without incurring additional costs, potentially replacing paid performance upgrades.</p>","<ul id=""""><li id="""">Identify disks ≥ 513 GiB that have manually configured additional IOPS or throughput</li><li id="""">Check for disks operating near IOPS or MBps limits that are eligible for Performance Plus</li><li id="""">Review whether Performance Plus has been enabled on qualifying Premium SSD or Standard SSD disks</li><li id="""">Cross-reference against workloads with high sustained disk usage to prioritize impact</li></ul>","<ul id=""""><li id="""">Enable Performance Plus on all eligible disks using Azure CLI, API, or portal</li><li id="""">Decommission paid performance tiers or custom throughput settings where Performance Plus provides equivalent capability</li><li id="""">Incorporate Performance Plus enablement into provisioning templates for large disks going forward</li></ul>","<p id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-performance?tabs=azure-cli"" id="""">Enable Performance Plus on Azure Disks</a></p>",FALSE,FALSE,,,,<p>Azure-Storage-9649</p>
Missing Reserved PTUs for Steady-State Azure OpenAI Workloads,missing-reserved-ptus-for-steady-state-azure-openai-workloads-f78d4,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e0de027536abae03d1,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:16 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Ariel Lichterman,azure,azure-cognitive-services,unoptimized-pricing-model,ai,"<p id="""">Many production Azure OpenAI workloads—such as chatbots, inference services, and retrieval-augmented generation (RAG) pipelines—use PTUs consistently throughout the day. When usage stabilizes after initial experimentation, continuing to rely on on-demand PTUs results in ongoing unnecessary spend. These workloads are strong candidates for reserved PTUs, which provide identical performance guarantees at a substantially reduced hourly rate. Migrating to reservations usually requires no architectural changes and delivers immediate cost savings.</p>","<p id="""">PTUs are billed hourly based on provisioned throughput. On-demand PTUs use standard hourly rates, whereas reserved PTUs offer significant discounts—often up to \~80%—when capacity is committed for a month or year. Workloads running continuously on on-demand PTUs incur avoidable premium pricing.</p>","<ul id=""""><li id="""">Review PTU deployments supporting production workloads that operate continuously throughout the day</li><li id="""">Assess whether throughput demand remains stable enough to justify reserved capacity</li><li id="""">Identify deployments that have moved beyond experimentation but still use on-demand PTUs</li><li id="""">Evaluate the cost difference between on-demand PTUs and reserved PTUs for these workloads</li></ul>","<ul id=""""><li id="""">Purchase monthly or annual reserved PTUs for workloads with sustained, predictable throughput needs</li><li id="""">Establish governance criteria defining when production PTU deployments should transition to reservations</li><li id="""">Periodically reassess workload stability to ensure PTU reservation commitments remain aligned with demand</li><li id="""">Use cost modeling to evaluate reservation options as part of production readiness reviews</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput"" id="""">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput</a></li></ul>",FALSE,FALSE,,,,Azure-AI-2318
Missing S3 Gateway Endpoint for Intra-Region EC2 Access,missing-s3-gateway-endpoint-for-intra-region-ec2-access,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4c3e6867b416ecb52a,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Mike Graff,aws,aws-s3,inefficient-configuration,storage,"<p id="""">  When EC2 instances within a VPC access Amazon S3 in the same region without a Gateway VPC Endpoint, traffic is routed through the public S3 endpoint and incurs standard internet egress charges — even though it remains within the AWS network. This results in unnecessary egress charges, as AWS treats this traffic as data transfer out to the internet, billed under the S3 service.</p><p id="""">By contrast, provisioning a Gateway Endpoint for S3 allows traffic between EC2 and S3 to flow over the AWS private backbone at no additional cost. This configuration is especially important for data-intensive applications, such as analytics jobs, backups, or frequent uploads/downloads, where the cumulative data transfer can be substantial.</p><p id="""">Because the egress cost is billed under S3, it is often misattributed or overlooked during EC2 or networking reviews, leading to silent overspend.</p>","<ul id=""""><li id="""">S3 charges for <strong id="""">data transfer out to the internet</strong>, even for intra-region transfers, <strong id="""">unless</strong> a Gateway VPC Endpoint is used</li><li id=""""><strong id="""">Data transfers from S3 to EC2 in the same region</strong>, without a Gateway Endpoint, are billed as <strong id="""">internet egress</strong></li><li id="""">The cost appears on the bill under <strong id="""">Amazon S3</strong> as <strong id="""">Data Transfer Out</strong>, despite EC2 initiating the request</li><li id="""">Gateway VPC Endpoints for S3 are <strong id="""">free to use</strong> and avoid these egress charges</li></ul>","<ul id=""""><li id="""">Identify environments with significant S3 data transfer out costs in the billing report</li><li id="""">Review whether EC2 instances in the same region are reading from or writing to S3 buckets</li><li id="""">Evaluate whether a Gateway VPC Endpoint for S3 is provisioned in the relevant VPCs</li><li id="""">Determine whether the absence of the endpoint is contributing to avoidable egress charges</li><li id="""">Check if the traffic pattern is internal (e.g., S3-to-EC2 backup jobs or intra-app data movement)</li></ul>","<ul id=""""><li id="""">Deploy a Gateway VPC Endpoint for S3 in VPCs that generate large intra-region S3 traffic</li><li id="""">Update route tables and access policies to route S3 traffic through the endpoint</li><li id="""">Validate that EC2-to-S3 traffic is using the private path and no longer incurring egress charges</li><li id="""">  Incorporate Gateway Endpoint provisioning into default network infrastructure templates</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html"" id="""">Gateway Endpoints for Amazon S3</a></li><li id=""""><a href=""https://aws.amazon.com/s3/pricing/"" id="""">Amazon S3 Pricing</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Storage-8077</p>
Missing S3 Lifecycle Policy for Incomplete Multipart Uploads,missing-s3-lifecycle-policy-for-incomplete-multipart-uploads,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b43a7a888da77b07b4a,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-s3,inefficient-configuration,storage,"<p id="""">Multipart upload allows large files to be uploaded in segments. Each part is stored individually until the upload is finalized by a “CompleteMultipartUpload” request. If this final request is never issued—due to a timeout, crash, failed job, or misconfiguration—the parts remain stored but are effectively useless: they do not form a valid object and cannot be retrieved. Without a lifecycle policy in place to clean up these incomplete uploads, the orphaned parts persist and continue to incur storage charges indefinitely.</p>","<p id="""">S3 charges per GB-month for all stored data, including partial object data from incomplete multipart uploads. These parts are billed as standard storage and remain in the bucket unless explicitly removed—either manually or via a lifecycle policy.</p>","<ul id=""""><li id="""">Identify S3 buckets that are used for large file uploads or automation-driven data ingestion</li><li id="""">Review whether an S3 Lifecycle rule exists to abort incomplete multipart uploads</li><li id="""">Consult with application owners or platform teams to confirm upload workflows and fault tolerance</li></ul>","<p id="""">Define an S3 Lifecycle rule to automatically abort incomplete multipart uploads after a specified number of days (e.g., 7 days). This ensures that leftover parts are cleaned up and do not persist indefinitely. Include this rule as a default in all buckets handling large object uploads or automation-driven data pipelines.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html\&quot;"" id="""">S3 Multipart Upload Overview</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpu-abort-incomplete-mpu-lifecycle-config.html\"" id="""">Managing Incomplete Multipart Uploads Using Lifecycle Configuration</a></li></ul>",FALSE,FALSE,,16,,<p>AWS-Storage-3133</p>
Missing Scheduled Shutdown for Non-Production Azure Virtual Machines,missing-scheduled-shutdown-for-non-production-azure-virtual-machines,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b49d757428dc24f12f8,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Yulia Perlis,azure,azure-virtual-machines,inefficient-configuration,compute,"<p id="""">Non-production Azure VMs are frequently left running during off-hours despite being used only during business hours. When these instances remain active overnight or on weekends, they generate unnecessary compute spend. Azure offers built-in auto-shutdown features that allow teams to define daily stop times, retaining disk data and configurations without paying for VM runtime. Implementing scheduled shutdowns in dev/test environments is a simple, low-risk optimization that can reduce compute costs by 30–60%.</p>","<p id="""">Azure VMs are billed per second, with a minimum of one minute, based on:</p><ul id=""""><li id="""">VM size and type</li><li id="""">Operating system and licensing</li><li id="""">Runtime duration</li></ul><p id="""">Note: Storage (managed disks) continues to incur charges even when the VM is stopped.</p>","<ul id=""""><li id="""">Identify VMs labeled or tagged as dev/test/staging</li><li id="""">Use Azure Monitor or Cost Management to assess usage and uptime patterns</li><li id="""">Detect VMs with consistent business-hours usage but 24/7 runtime</li><li id="""">Review VM configuration to check whether auto-shutdown is enabled</li></ul>","<ul id=""""><li id="""">Enable Azure’s built-in auto-shutdown setting for applicable non-prod VMs</li><li id="""">Alternatively, configure shutdown/start schedules using Azure Automation or Logic Apps</li><li id="""">Preserve state using managed disks, snapshots, or generalized images</li><li id="""">Review schedules periodically to align with changing team usage</li></ul>",,FALSE,FALSE,,,,<p>Azure-Compute-5903</p>
Missing Scheduled Shutdown for Non-Production Compute Engine Instances,missing-scheduled-shutdown-for-non-production-compute-engine-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b49b6006ad5b82dd3fe,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Yulia Perlis,gcp,gcp-compute-engine,inefficient-configuration,compute,"<p id="""">Development and test environments on Compute Engine are commonly provisioned and left running around the clock, even if only used during business hours. This results in wasteful spend on compute time that could be eliminated by scheduling shutdowns during idle periods. GCP enables scheduling via native tools such as Cloud Scheduler, Cloud Functions, or Terraform automation. Stopping VMs during off-hours preserves boot disks and instance metadata while halting compute billing.</p>","<p id="""">Compute Engine instances are billed per second (minimum of one minute) based on:</p><ul id=""""><li id="""">Instance type and configuration</li><li id="""">Runtime duration</li><li id="""">Additional resources (e.g., persistent disks, external IPs) are billed separately</li></ul>","<ul id=""""><li id="""">Identify VMs in dev/test environments using labels, folders, or project naming conventions</li><li id="""">Analyze instance uptime using Cloud Monitoring or billing export data</li><li id="""">Detect instances with consistent 9-to-5 usage but continuous runtime</li><li id="""">Check whether startup/shutdown schedules are already in place</li></ul>","<ul id=""""><li id="""">Use Cloud Scheduler and Cloud Functions to automate VM stop/start workflows</li><li id="""">Preserve instance configuration and state using persistent disks or custom images</li><li id="""">Align schedules to working hours and review regularly with workload owners</li><li id="""">Consider integrating automation with CI/CD tools or environment provisioning scripts</li></ul>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/compute/docs/instances/schedule-instance-start-stop\&quot;"" id="""">Schedule Compute Engine VM Start and Stop</a></li><li id=""""><a href=""https://cloud.google.com/compute/docs/instances\"" id="""">Managing VM Instances</a></li></ul>",FALSE,FALSE,,,,<p>GCP-Compute-5966</p>
Missing Scheduled Shutdown for Non-Production EC2 Instances,missing-scheduled-shutdown-for-non-production-ec2-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b49c8bf9cc747eb47c3,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Yulia Perlis,aws,aws-ec2,inefficient-configuration,compute,"<p id="""">Non-production EC2 instances are often provisioned for daytime-only usage but remain running 24/7 out of convenience or oversight. This results in unnecessary compute charges, even if the workload is inactive for 16+ hours per day. AWS supports automated schedules to stop and start instances at predefined times, allowing organizations to retain data and instance configuration without paying for unused runtime. Implementing a shutdown schedule for inactive periods (e.g., nights, weekends) can reduce compute costs by up to 60% in typical non-prod environments.</p>","<p id="""">EC2 is billed per second (Linux) or per hour (Windows, older instances) based on:</p><ul id=""""><li id="""">Instance type and size</li><li id="""">Uptime duration</li><li id="""">Additional charges for associated resources (e.g., EBS volumes) continue even when the instance is stopped</li></ul>","<ul id=""""><li id="""">Identify EC2 instances in dev/test/staging environments using tags or naming conventions</li><li id="""">Analyze instance runtime patterns via CloudWatch metrics or billing data</li><li id="""">Highlight instances with predictable daily usage but continuous uptime</li><li id="""">Determine whether workloads require persistent availability outside working hours</li></ul>","<ul id=""""><li id="""">Implement scheduled shutdowns using AWS Instance Scheduler or EventBridge rules</li><li id="""">Ensure stateful data is retained via attached EBS volumes or AMIs</li><li id="""">Set start/stop windows aligned to working hours (e.g., 8 a.m.–6 p.m. weekdays)</li><li id="""">Review and optimize schedules quarterly to match evolving team usage</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/solutions/instance-scheduler/\&quot;"" id="""">AWS Instance Scheduler</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html\"" id="""">Stopping and Starting EC2 Instances</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-9831</p>
Missing Shared Scope Configuration for Azure Reservations,missing-shared-scope-configuration-for-azure-reservations,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589414cabd263c42dd39fa,FALSE,FALSE,Sun Jun 22 2025 23:39:00 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Jeffrey Wang,azure,azure-reservations,suboptimal-configuration,compute,"<p id="""">When reservations are scoped only to a single subscription, any unused capacity cannot be applied to matching resources in other subscriptions within the same tenant. This leads to underutilization of the committed reservation and continued on-demand charges in other parts of the organization.   Enabling <strong id="""">Shared scope</strong> allows all eligible subscriptions to consume the reservation benefit, improving utilization and reducing overall spend. This is particularly impactful in environments with decentralized provisioning, such as across dev/test/prod subscriptions or multiple business units.</p>","<p id="""">Prepaid commitment for reserved capacity (hourly usage covered by reservation, regardless of whether resource is provisioned)</p>","<ul id=""""><li id="""">Check for reservations that consistently show low utilization over a representative time window</li><li id="""">Evaluate whether matching on-demand resources exist in other subscriptions within the same tenant</li><li id="""">Assess whether the reservation scope is currently set to 'Single' rather than 'Shared'</li><li id="""">Determine if usage patterns across subscriptions are dynamic or uneven, which would benefit from shared scope flexibility</li></ul>","<ul id=""""><li id="""">Change reservation scope from *Single* to *Shared* in the Azure Portal or via API</li><li id="""">Reevaluate periodically to ensure the scope aligns with current organizational structure and usage distribution</li></ul>","<p id=""""><a href=""https://learn.microsoft.com/en-us/azure/cost-management-billing/reservations/scope-reservations"" id="""">Scope a reservation in Azure</a><br><a href=""https://learn.microsoft.com/en-us/azure/cost-management-billing/reservations/understand-reserved-instance-utilization"" id="""">Reservation utilization insights</a></p>",FALSE,FALSE,,,,<p>Azure-Compute-2313</p>
Missing VPC Endpoints for High-Volume AWS Service Access,missing-vpc-endpoints-for-high-volume-aws-service-access,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4a8040f49223e290f3,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Trig Ghosh,aws,aws-vpc,inefficient-network-configuration,networking,"<p id="""">When EC2 instances, Lambda functions, or containerized workloads access AWS-managed services without VPC Endpoints, that traffic exits the VPC through a NAT Gateway or Internet Gateway. This introduces unnecessary egress charges and NAT processing costs, especially for data-intensive or high-frequency workloads.</p>",,"<ul id=""""><li id="""">Review VPC architecture for services that communicate with S3, DynamoDB, Secrets Manager, or other AWS-managed APIs</li><li id="""">Check whether Gateway Endpoints for S3 and DynamoDB exist and are attached to relevant route tables</li><li id="""">Identify missing Interface Endpoints for high-traffic services like Secrets Manager, SSM, or KMS</li><li id="""">Analyze NAT Gateway metrics (bytes processed per destination service) to quantify potential endpoint-eligible traffic</li><li id="""">Correlate NAT Gateway charges with known service access patterns to surface reroutable costs</li></ul>","<ul id=""""><li id="""">Provision Gateway Endpoints for S3 and DynamoDB in each VPC that accesses those services</li><li id="""">Create Interface Endpoints (via AWS PrivateLink) for services with frequent or latency-sensitive access (e.g., Secrets Manager, CloudWatch Logs)</li><li id="""">Ensure routing tables and DNS settings support private resolution to AWS services</li><li id="""">Embed VPC endpoint provisioning into infrastructure-as-code templates to ensure consistency across accounts and regions</li><li id="""">Monitor NAT Gateway data transfer volume over time to verify cost reduction after endpoint rollout</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html#gateway-endpoints\&quot;"" id="""">AWS Gateway Endpoints Overview</a></li><li id=""""><a href=""https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html#interface-endpoints\"" id="""">Interface Endpoints and AWS PrivateLink</a></li><li id=""""><a href=""https://aws.amazon.com/vpc/pricing/\"" id="""">AWS NAT Gateway Pricing</a></li><li id=""""><a href=""https://aws.amazon.com/privatelink/pricing/\"" id="""">AWS PrivateLink Pricing</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Networking-8908</p>
Missing or Inefficient Use of Materialized Views,missing-or-inefficient-use-of-materialized-views,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4be4cd169550c6d087,FALSE,FALSE,Thu May 22 2025 14:57:15 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-materialized-views,inefficient-resource-usage,other,"<p id="""">Inefficiency arises when MVs are either underused or misused.</p><ul id="""">  <li id="""">When high-cost, repetitive queries are not backed by MVs, workloads consume unnecessary compute resources.</li>  <li id="""">When MVs exist but are rarely queried, their background refresh and storage costs accumulate without offsetting savings.</li></ul><p id="""">Proper evaluation of workload patterns and strategic use of MVs is critical to achieve a net cost benefit.</p>",,"<ul id="""">  <li id="""">Analyze query history to identify expensive, frequently executed queries that could benefit from materialization.</li>  <li id="""">Review existing MVs for read activity to detect underutilized or idle materialized views.</li>  <li id="""">Compare MV refresh costs and storage footprint against active query savings to assess net cost effectiveness.</li>  <li id="""">Validate whether candidate workloads involve relatively stable datasets appropriate for materialization.</li></ul>","<ul id="""">  <li id="""">Create materialized views for high-cost, repetitive queries where refresh costs are low relative to compute savings.</li>  <li id="""">Decommission materialized views that incur maintenance and storage costs without sufficient query usage.</li>  <li id="""">Implement periodic reviews of MV usage and refresh behavior as data volumes and access patterns evolve.</li>  <li id="""">Engage data engineering teams to tune MV designs for optimal cost-benefit balance (e.g., selective columns, filtered subsets).</li></ul>",,FALSE,FALSE,,,,<p>Snowflake-Other-5250</p>
Misuse of Aurora Serverless for Steady-State Workloads,misuse-of-aurora-serverless-for-steady-state-workloads-6c2d9,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80dfaf6d38cd8694c3f5,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:09 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Cristian Măgherușan-Stanciu,aws,aws-aurora,suboptimal-deployment-model,databases,"<p id="""">Aurora Serverless is designed for workloads with unpredictable or intermittent usage patterns that benefit from automatic scaling. However, when used for databases with constant load, the service’s elasticity offers little advantage and adds cost overhead. Serverless instances run continuously in steady workloads, resulting in persistent ACU billing at a higher effective rate than a provisioned cluster of similar size. In addition, Serverless configurations cannot use Reserved Instances or Savings Plans, missing out on predictable cost reductions available to provisioned Aurora.</p>","<p id="""">Aurora Provisioned clusters are billed by instance type and size, with options for Reserved Instance or Savings Plan discounts. Aurora Serverless clusters are billed per Aurora Capacity Unit (ACU) per second, with pricing optimized for variable workloads. When workloads are steady, Serverless’ dynamic pricing leads to consistently higher costs compared to equivalent provisioned instances.</p>","<ul id=""""><li id="""">Review Aurora Serverless clusters where capacity usage remains constant over time</li><li id="""">Assess whether average and peak ACU consumption show minimal fluctuation across representative time windows</li><li id="""">Confirm whether workload patterns reflect consistent utilization rather than bursty or intermittent usage</li><li id="""">Evaluate whether provisioned Aurora instances could deliver similar performance at lower cost, especially with Reserved Instance coverage</li></ul>","<ul id=""""><li id="""">Convert steady-state Aurora Serverless clusters to provisioned Aurora instances with appropriate instance sizing</li><li id="""">Apply Reserved Instances or Savings Plans to reduce ongoing compute costs for predictable workloads</li><li id="""">Retain Aurora Serverless only for workloads with irregular, unpredictable, or low-utilization patterns</li><li id="""">Validate that performance requirements are met after migration; provisioned instances often provide lower latency and jitter</li><li id="""">validate performance (latency, jitter) and reorganized into 3 subthemes (Migration, Optimization, Governance)</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html"" id="""">Amazon Aurora Serverless Documentation</a></li><li id=""""><a href=""https://aws.amazon.com/rds/aurora/pricing/"" id="""">Aurora Pricing</a></li></ul>",FALSE,FALSE,,,,AWS-Databases-8775
No Lifecycle Management for Temporarily Stopped RDS Instances,no-lifecycle-management-for-temporarily-stopped-rds-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b48c21e86af3272ae09,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Shireen Maini,aws,aws-rds,unused-resource,databases,"<p id="""">While stopping an RDS instance reduces runtime cost, AWS enforces a 7-day limit on stopped state. After this period, the instance is automatically restarted and resumes incurring compute charges — even if the database is still not needed. This creates waste in cases where teams intended to pause the environment but failed to manage its lifecycle beyond the 7-day window. Without proper automation or teardown workflows, stopped RDS instances silently become active and billable again. The best practice for long-term inactivity is to snapshot the database and delete the instance entirely. If the instance must remain available for fast recovery, automation should be in place to re-stop it upon restart.</p>","<p id="""">RDS instances are billed based on:</p><ul id=""""><li id="""">Instance runtime (per hour) — charged only while running</li><li id="""">Provisioned storage and snapshots — billed independently of instance state</li></ul><p id="""">When an instance is restarted after 7 days without being actively used, runtime charges resume.</p>","<ul id=""""><li id="""">Identify RDS instances that were restarted automatically after reaching the 7-day stop limit</li><li id="""">Check for repeated stop/restart patterns without sustained usage</li><li id="""">Review tags or naming patterns indicating temporary or paused projects</li><li id="""">Engage with workload owners to determine whether reactivation was intentional or overlooked</li></ul>","<p id="""">For projects on hold or long-term inactive environments:</p><ul id=""""><li id="""">Take a snapshot and delete the RDS instance to avoid all runtime charges</li><li id="""">Restore from snapshot only when the environment is needed again</li></ul><p id="""">If the instance must remain stopped for quick recovery:</p><ul id=""""><li id="""">Tag eligible instances and implement automation (e.g., AWS Step Functions or Lambda) to monitor for automatic restarts and re-stop them as needed</li><li id="""">Ensure teams are aware of the 7-day stop limit and its cost implications</li><li id="""">Consider integrating this logic into existing environment lifecycle tooling</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html"" id="""">Stopping and Starting an Amazon RDS Instance</a></p>",FALSE,FALSE,,,,<p>AWS-Databases-4346</p>
Non-Graviton ElastiCache Node on Eligible Workload,non-graviton-elasticache-node-on-eligible-workload,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4a3664b2e5e107c4a4,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Loïc Fournier,aws,aws-elasticache,suboptimal-instance-family-selection,databases,"<p id="""">Many Redis and Memcached clusters still use legacy x86-based node types (e.g., cache.r5, cache.m5) even though Graviton-based alternatives are available. In-memory workloads tend to be highly compatible with Graviton due to their simplicity and reliance on standard CPU and memory usage patterns.Unless constrained by architecture-specific extensions or strict compliance requirements, most ElastiCache clusters can be transitioned with no application-level changes. Failing to migrate to Graviton results in unnecessary compute spend and missed opportunities to improve cache efficiency.</p>","<p id="""">ElastiCache is billed hourly per node, based on:</p><ul id=""""><li id="""">Node type and size</li><li id="""">Engine and region</li></ul><p id="""">Graviton node types typically deliver up to 40% lower cost for equivalent performance compared to x86.</p>","<ul id=""""><li id="""">Enumerate all ElastiCache clusters and identify those using x86-based families</li><li id="""">Cross-check with cache.*g instance types available in the region and engine</li><li id="""">Confirm that clusters have no known architecture-specific dependencies</li><li id="""">Analyze memory and CPU usage patterns to validate fit with Graviton</li></ul>","<ul id=""""><li id="""">Switch node types to cache.r6g, cache.m6g, or cache.t4g equivalents</li><li id="""">Update provisioning logic to default to Graviton families for new clusters</li><li id="""">Pilot in non-prod or lower-tier environments to validate behavior and latency</li><li id="""">Incorporate architecture reviews into ElastiCache lifecycle and scaling workflows</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheNodes.SupportedTypes.html\&quot;"" id="""">ElastiCache Graviton Node Types</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/graviton/\"" id="""">AWS Graviton Performance Claims</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Databases-2544</p>
Non-Graviton RDS Instance on Eligible Workload,non-graviton-rds-instance-on-eligible-workload,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4afe1476b7455d3534,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Loïc Fournier,aws,aws-rds,suboptimal-instance-family-selection,databases,"<p id="""">Many RDS workloads continue to run on older x86 instance types (e.g., db.m5, db.r5) even though compatible Graviton-based options (e.g., db.m6g, db.r6g) are widely available. These newer families deliver improved performance per vCPU and lower hourly costs, yet are often not adopted due to legacy defaults, inertia, or lack of awareness.When workloads are not tightly bound to architecture-specific extensions (e.g., x86-specific binaries or drivers), switching to Graviton typically requires no application changes and results in immediate savings. Persisting with x86 in eligible use cases results in avoidable overpayment for compute.</p>","<p id="""">RDS is billed per-hour or per-second (depending on engine) based on:</p><ul id=""""><li id="""">Instance family and size</li><li id="""">Storage and backup usage</li></ul><p id="""">Graviton-based instances offer 20–40% better price-performance and can lower compute costs significantly for compatible workloads.</p>","<ul id=""""><li id="""">List all active RDS instances and identify those using x86-based families (db.m5, db.r5, etc.)</li><li id="""">Cross-reference instance types with available Graviton equivalents for the same engine</li><li id="""">Use Compute Optimizer (where available) or performance data to validate compatibility</li><li id="""">Identify workloads with steady, general-purpose usage patterns that would benefit from Graviton</li></ul>","<ul id=""""><li id="""">Evaluate performance requirements and test Graviton-backed RDS instances in staging</li><li id="""">Modify instance class to a db.*g Graviton-based equivalent (e.g., db.r6g.large)</li><li id="""">Update infrastructure templates (e.g., Terraform, CloudFormation) to default to Graviton where applicable</li><li id="""">Revisit instance family selections during each modernization, upgrade, or cost review cycle</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/graviton.html\&quot;"" id="""">Amazon RDS Graviton Instances</a></li><li id=""""><a href=""https://docs.aws.amazon.com/compute-optimizer/latest/ug/what-is.html\"" id="""">AWS Compute Optimizer</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Databases-6409</p>
Non-Production Azure OpenAI Deployments Using PTUs Instead of PAYG,non-production-azure-openai-deployments-using-ptus-instead-of-payg-9fe81,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e06aed9f5d2a6dd9c6,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:16 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Ariel Lichterman,azure,azure-cognitive-services,misaligned-pricing-model,ai,"<p id="""">Development, testing, QA, and sandbox environments rarely have the steady, predictable traffic patterns needed to justify PTU deployments. These workloads often run intermittently, with lower throughput and shorter usage windows. When PTUs are assigned to such environments, the fixed hourly billing generates continuous cost with little utilization. Switching non-production workloads to PAYG aligns cost with actual usage and eliminates the overhead of managing PTU quota in low-stakes environments.</p>","<p id="""">PAYG charges per input and output token, making it cost-efficient for sporadic or low-volume workloads. PTUs incur fixed hourly charges regardless of utilization. Using PTUs in non-production environments typically results in paying for unused capacity.</p>","<ul id=""""><li id="""">Identify OpenAI deployments in dev, test, QA, or sandbox environments running on PTUs</li><li id="""">Review utilization patterns showing intermittent or low-throughput activity</li><li id="""">Evaluate whether non-production environments require dedicated throughput or low-latency guarantees</li><li id="""">Check for PTU deployments where performance requirements could be met by PAYG consumption</li></ul>","<ul id=""""><li id="""">Migrate non-production OpenAI deployments from PTUs to PAYG to align cost with usage</li><li id="""">Create environment-based deployment standards specifying PAYG as the default for non-prod workloads</li><li id="""">Implement workload reviews to validate whether PTU performance guarantees are truly required</li><li id="""">Periodically assess non-production usage patterns to prevent unnecessary PTU provisioning</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput"" id="""">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput</a></li></ul>",FALSE,FALSE,,,,Azure-AI-5981
On-Demand-Only Configuration for Non-Production Databricks Clusters,on-demand-only-configuration-for-non-production-databricks-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84cb0515e89510ee315,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Benjamin van der Maas,databricks,databricks-clusters,suboptimal-pricing-model,compute,"<p id="""">In non-production environments—such as development, testing, and experimentation—many teams default to on-demand nodes out of habit or caution. However, Databricks offers built-in support for using spot instances safely. Its job scheduler and cluster management system are designed to detect spot instance evictions and automatically replace them with on-demand nodes when necessary, making the use of spot compute relatively low-risk.</p><p id="""">Failing to enable spot for non-critical or short-lived workloads leads to unnecessary overspend. The inefficiency often arises because spot usage is not enabled by default and must be explicitly selected in cluster settings. In teams that don’t revisit infrastructure defaults or where FinOps guardrails are missing, this results in a persistent cost gap between actual usage and what could be safely optimized.</p>","<p id="""">Databricks clusters are billed based on the underlying virtual machines used for driver and worker nodes. When on-demand instances are selected, charges are based on standard cloud provider rates. If spot instances are enabled (where available), compute costs can be significantly lower—often 60–90% cheaper. Databricks includes native failover capabilities that automatically replace preempted spot nodes with on-demand nodes to maintain job continuity, minimizing the impact of eviction risk.</p>","<ul id=""""><li id="""">Identify Databricks clusters that are not configured to use spot instances</li><li id="""">Filter for non-production environments (e.g., dev, test, staging) where eviction risk is acceptable</li><li id="""">Review the duration and criticality of jobs; short-lived or interruptible workloads are ideal candidates</li><li id="""">Check whether spot replacement policies are enabled in workspace settings</li><li id="""">Evaluate whether cost differences between current on-demand usage and spot alternatives are material</li></ul>","<ul id=""""><li id="""">Enable spot instance usage for non-production clusters where workloads are resilient to interruption</li><li id="""">Leverage Databricks’ native fallback-to-on-demand capabilities to preserve job continuity</li><li id="""">Establish workspace-level defaults or templates that promote spot usage in dev/test clusters</li><li id="""">Periodically audit compute configurations to detect persistent on-demand usage in non-critical environments</li></ul>","<ul id=""""><li id="""">https://docs.databricks.com/clusters/configure.html</li><li id="""">https://docs.databricks.com/clusters/instance-pools.html</li><li id="""">https://docs.databricks.com/clusters/clusters-manage.html</li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-9359</p>
Orphaned Kubernetes Resources,orphaned-kubernetes-resources,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84cdd1bc004aac4c3d0,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Yonah Dissen,gcp,gcp-gke,orphaned-resource,compute,"<p id="""">In GKE environments, it is common for unused Kubernetes resources to accumulate over time. Examples include Persistent Volume Claims (PVCs) that retain provisioned Persistent Disks, or Services of type LoadBalancer that continue to front GCP external load balancers even after the backing pods are gone. ConfigMaps and Secrets may also linger, creating operational overhead.</p><p id="""">These orphaned objects often persist due to gaps in CI/CD teardown logic, manual testing workflows, or drift over time. While some carry negligible cost on their own, others can result in significant charges, especially storage and networking artifacts. This inefficiency applies broadly across Kubernetes platforms and is scoped here to GKE.</p>","<p id="""">GKE charges for the control plane in certain tiers and always bills for underlying node resources (e.g., Compute Engine VMs), storage (e.g., Persistent Disks provisioned by PVCs), and network resources like external load balancers created by Services. Orphaned Kubernetes objects—such as Services, PVCs, ConfigMaps, or Secrets—can lead to idle infrastructure costs, especially when they trigger or retain provisioned GCP resources.</p>","<ul id=""""><li id=""""><strong id="""">PVCs &amp; PVs</strong></li><li id="""">`kubectl get pvc -A --field-selector=status.phase!=Bound`; map to GCP PDs via `gcloud compute disks list --filter=""-users:*""`</li><li id=""""><strong id="""">Services of type LoadBalancer</strong> with <strong id="""">0 endpoints</strong> (`kubectl get ep -A`). Cross‑check for dangling forwarding rules/IPs.</li><li id=""""><strong id="""">ConfigMaps / Secrets</strong> not referenced in any `Deployment`, `StatefulSet`, `CronJob`, or `Job` in the last *n* days (audit via kube‑audit‑export or OPA‑Gatekeeper).</li><li id=""""><strong id="""">Unused Namespaces</strong> (dev/test TTL exceeded).</li><li id=""""><strong id="""">GCP Recommender APIs</strong> for *idle Persistent Disks* and *unattached external IPs*.</li></ul>","<ul id=""""><li id="""">Remove PVCs to deprovision underlying Persistent Disks</li><li id="""">Delete unused Services to avoid charges for external Load Balancers and reserved IPs</li><li id="""">Clean up ConfigMaps and Secrets not in use</li><li id="""">Regularly audit clusters for orphaned resources using policy checks or automation</li><li id="""">Tag PVCs with <strong id="""">owner/team</strong> labels for accountability</li></ul>",,FALSE,FALSE,,,,
Orphaned Kubernetes Resources,orphaned-kubernetes-resources-e7e68,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fc6dc4c41e57f9e70c,FALSE,FALSE,Thu Aug 28 2025 22:32:28 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Yonah Dissen,aws,aws-eks,orphaned-resource,compute,"<p id="""">In Kubernetes environments, resources such as ConfigMaps, Secrets, Services, and Persistent Volume Claims (PVCs) are often created dynamically by applications or deployment pipelines. When applications are removed or reconfigured, these resources may be left behind if not explicitly cleaned up. Over time, they accumulate as orphaned resources — not referenced by any live workload.</p><p id="""">Some of these objects, like PVCs or Services of type LoadBalancer, result in active infrastructure that continues to incur cloud charges (e.g., retained EBS volumes or unused Elastic Load Balancers). Even lightweight objects like ConfigMaps and Secrets bloat the API server’s object store, causing latency and impacting deployments/scaling,, clutter the control plane, and complicate configuration management. This issue is especially common during cluster upgrades, namespace decommissioning, and workload migrations.</p>","<p id="""">EKS clusters incur cost through the underlying compute (e.g., EC2 or Fargate), attached storage (e.g., EBS volumes via Persistent Volume Claims), and any provisioned networking resources (e.g., Load Balancers from Services).</p><p id="""">While Kubernetes control plane costs are fixed, orphaned objects such as ConfigMaps, Secrets, Services, or PVCs can indirectly consume billable cloud infrastructure — especially when Services create load balancers or PVCs retain EBS volumes. These costs persist even if the associated workloads have been deleted.</p>","<ul id=""""><li id="""">Review the cluster for Kubernetes objects not referenced by any current Deployment, StatefulSet, or Pod</li><li id="""">Identify Persistent Volume Claims that are not mounted by any active workload</li><li id="""">Check for Services that no longer correspond to live endpoints or backing workloads</li><li id="""">Evaluate whether orphaned Services are still fronting AWS Load Balancers</li><li id="""">Identify ConfigMaps or Secrets with old creation timestamps and no recent pod mounts</li><li id="""">Confirm that none of the orphaned resources are linked to active CronJobs, Jobs, or custom controllers</li></ul>","<ul id=""""><li id="""">Delete orphaned Persistent Volume Claims to release underlying storage (e.g., EBS volumes)</li><li id="""">Remove unused Services, especially those of type LoadBalancer, to eliminate unnecessary networking charges</li><li id="""">Clean up ConfigMaps and Secrets that are no longer referenced by any active workloadImplement tagging/labeling standards for all workloads to simplify orphan detection.</li><li id="""">Incorporate automated cleanup routines into CI/CD pipelines or periodic governance processesSchedule periodic audits after cluster upgrades or namespace deletions to prevent accumulation</li><li id="""">Relevant Documentation</li><li id="""">https://aws.amazon.com/eks/pricing/</li><li id="""">https://aws.amazon.com/ebs/pricing/</li><li id="""">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</li><li id="""">https://aws.amazon.com/elasticloadbalancing/pricing/</li><li id="""">https://kubernetes.io/docs/concepts/services-networking/service/</li><li id="""">https://kubernetes.io/docs/concepts/overview/kubernetes-api/</li><li id="""">https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/</li><li id="""">https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/</li></ul>",,FALSE,FALSE,,,,<p>AWS-Compute-9085</p>
Orphaned Kubernetes Resources,orphaned-kubernetes-resources-230dd,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fc846fc77ec30f0161,FALSE,FALSE,Thu Aug 28 2025 22:32:28 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Yonah Dissen,azure,azure-aks,orphaned-resource,compute,"<p id="""">Kubernetes environments often accumulate unused resources over time as applications evolve. Common examples include Persistent Volume Claims (PVCs) backed by Azure Disks, Services that trigger load balancer provisioning, or stale ConfigMaps and Secrets. When the associated deployments or pods are removed, these resources may remain unless explicitly deleted.</p><p id="""">In AKS, this can lead to unmanaged costs, such as idle managed disks from orphaned PVCs or public load balancers from Services of type LoadBalancer. Even lightweight resources like unused Secrets or ConfigMaps degrade cluster hygiene and can introduce security or operational risk. This inefficiency is common across Kubernetes environments and is scoped here to AKS.</p>","<p id="""">While AKS clusters do not charge for the control plane, costs arise from the underlying compute (VMs or node pools), storage (e.g., managed disks from PVCs), and networking (e.g., public IPs and load balancers from Services). Orphaned Kubernetes objects—such as PVCs, Services, ConfigMaps, and Secrets—may persist beyond the lifecycle of the workloads that created them, causing unnecessary consumption of Azure resources and cost.</p>","<ul id=""""><li id="""">Review Kubernetes objects not linked to active Deployments, StatefulSets, or running Pods</li><li id="""">Check across all namespaces, not just default (default should never be deleted</li><li id="""">Check for PVCs not mounted by any current workload</li><li id="""">kubectl get pvc --all-namespaces to check PVC usage</li><li id="""">Identify Services of type LoadBalancer that are not backed by any endpoints</li><li id="""">kubectl get svc -o wide to identify LoadBalancer services</li><li id="""">Confirm whether any retained Services have public IPs or Azure Load Balancers associated</li><li id="""">Locate Secrets and ConfigMaps with old creation dates and no recent usage</li><li id="""">Validate that no Jobs or CronJobs are expected to recreate workloads using these resources</li><li id="""">kubectl get secrets,configmaps --show-labels for unused resources</li></ul>","<ul id=""""><li id="""">Before deletion, verify resources are truly orphaned</li><li id="""">Delete orphaned PVCs to release Azure Managed Disks</li><li id="""">Remove Services that no longer front active workloads to deallocate Load Balancers and public IPs</li><li id="""">Clean up unreferenced ConfigMaps and Secrets</li><li id="""">Use scheduled audits or automation tools to identify and clean up orphaned resources regularly</li></ul>",,FALSE,FALSE,,,,
Orphaned and Overprovisioned Resources in AKS Clusters,orphaned-and-overprovisioned-resources-in-aks-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84c3762f2f61160c475,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Yisrael Gross,azure,azure-aks,inefficient-configuration,compute,"<p id="""">Clusters often accumulate unused components when applications are terminated or environments are cloned. These include PVCs backed by Managed Disks, Services that still front Azure Load Balancers, and test namespaces that are no longer maintained. Node pools are frequently overprovisioned, especially in multi-tenant or CI environments.</p><p id="""">The cost impact of these idle resources is magnified in organizations with many environments or without standardized cleanup routines. Since billing is resource-specific, even low-cost items like Managed Disks, load balancer rules, and frontend configurations can accumulate meaningful waste over time.</p>","<p id="""">While the AKS control plane is free, costs accrue from the underlying compute (VMs), storage (Managed Disks provisioned via PVCs), and networking (load balancers and public IPs from Services). Orphaned and overprovisioned resources continue to incur charges even if the corresponding workloads are no longer active.</p>","<ul id=""""><li id="""">Scan for unused PVCs that are not attached to any pod</li><li id="""">Check for abandoned StatefulSets with orphaned PVCs</li><li id="""">Review Services that have Azure Load Balancers but no active endpoints</li><li id="""">Check node pools for sustained underutilization</li><li id="""">Identify stale namespaces or test environments with no recent changes</li><li id="""">Look for config maps or secrets that are unused but still consuming control plane capacity</li><li id="""">Audit for Helm or CI/CD jobs that left behind residual resources</li><li id="""">Scan for unused Ingress controllers that may still have associated Azure Application Gateways or Load Balancers</li></ul>","<ul id=""""><li id="""">Delete unused PVCs to release backing Managed Disks</li><li id="""">Clean up Services that are no longer in use to avoid unnecessary load balancer charges</li><li id="""">Scale down underutilized node pools</li><li id="""">Remove outdated or inactive namespaces</li><li id="""">Implement periodic cleanup routines for ephemeral environments</li><li id="""">Implement resource tagging strategies to track the environment lifecycle</li><li id="""">Use Azure Policy to enforce cleanup requirements</li></ul>","<ul id=""""><li id="""">https://learn.microsoft.com/en-us/azure/aks/</li><li id="""">https://learn.microsoft.com/en-us/azure/aks/concepts-storage</li><li id="""">https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-overview</li></ul>",FALSE,FALSE,,,,<p>Azure-Compute-2976</p>
Orphaned and Overprovisioned Resources in EKS Clusters,orphaned-and-overprovisioned-resources-in-eks-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fdd768c1eead675250,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Yisrael Gross,aws,aws-eks,inefficient-configuration,compute,"<p id="""">In EKS environments, cluster sprawl can occur when workloads are removed but underlying resources remain. Common issues include persistent volumes no longer mounted by pods, services still backed by ELBs despite being unused, and overprovisioned nodes for workloads that no longer exist. Node overprovisioning can result from high CPU/memory requests or limits, DaemonSets running on every node, restrictive Pod Disruption Budgets, anti-affinity rules, uneven AZ distribution, or slow scale-down timers. Preventative measures include improving bin packing efficiency, enabling Karpenter consolidation, and right-sizing node instance types and counts. Dev/test namespaces and short-lived environments often accumulate without clear teardown processes, leading to ongoing idle costs.</p><p id="""">These remnants contribute to excess infrastructure cost and control plane noise. Since AWS bills independently for each resource (e.g., EBS, ELB, EC2), inefficiencies can add up quickly. Without structured governance or cleanup tooling, clusters gradually fill with orphaned objects and unused capacity.</p>","<p id="""">EKS clusters incur compute charges through EC2 nodes or Fargate profiles, storage charges from attached EBS volumes, and networking charges from load balancers created via Kubernetes Services. Additional network-level costs can include NAT Gateway hourly and per-GB data processing fees, cross-Availability Zone (AZ) data transfer, and other inter-VPC or internet-bound traffic. Orphaned resources such as unused PVCs (backed by EBS), idle Services (backed by Elastic Load Balancers), and underutilized nodes can generate persistent charges even when no active workloads are running.</p>","<ul id=""""><li id="""">Identify namespaces that no longer contain active Deployments or Pods</li><li id="""">Review Services still provisioned with ELBs but lacking backend endpoints</li><li id="""">Check for PVCs that are not mounted by any current workloads</li><li id="""">Analyze node utilization to detect consistently underused EC2 nodes, referencing median CPU and memory utilization per node group (targeting 60–70%+ in production)</li><li id="""">Audit test or sandbox namespaces that have not been updated recently</li><li id="""">Verify whether terminated Helm releases or jobs left residual resources</li></ul>","<ul id=""""><li id="""">Remove unused PVCs to deprovision backing EBS volumes</li><li id="""">Delete idle Services to release associated ELBs and IP addresses</li><li id="""">Clean up inactive namespaces and workloads</li><li id="""">Resize or scale down overprovisioned EC2 nodes, incorporating proactive prevention strategies such as Karpenter consolidation, bin packing optimization, and right-sizing of node instance types and counts</li><li id="""">Implement environment lifecycle policies for dev/test clusters</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html"" id="""">https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html</a></li><li id=""""><a href=""https://kubernetes.io/docs/concepts/storage/persistent-volumes/"" id="""">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a></li><li id=""""><a href=""https://kubernetes.io/docs/concepts/services-networking/service/"" id="""">https://kubernetes.io/docs/concepts/services-networking/service/</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html"" id="""">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-1348</p>
Orphaned and Overprovisioned Resources in GKE Clusters,orphaned-and-overprovisioned-resources-in-gke-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84cb9b3ce38c28465d8,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Yisrael Gross,gcp,gcp-gke,inefficient-configuration,compute,"<p id="""">As environments scale, GKE clusters tend to accumulate artifacts from ephemeral workloads, dev environments, or incomplete job execution. PVCs can continue to retain Persistent Disks, Services may continue to expose public IPs and provision load balancers, and node pools are often oversized for steady-state demand. This results in cloud spend that is not aligned with application activity.</p><p id="""">Organizations that lack visibility into Kubernetes-level resource usage often miss these inefficiencies because GCP billing tools surface usage at the infrastructure level, not the Kubernetes object level.</p>","<p id="""">GKE clusters incur costs for node VMs (in Standard mode) or pod resources and control plane (in Autopilot mode), Persistent Disks via PVCs, and external IP addresses and load balancers from exposed Services. Orphaned objects like unused volumes, idle services, and overprovisioned node pools continue to generate charges even when not tied to running workloads.</p>","<ul id=""""><li id="""">Identify Persistent Disks not mounted by any pod via PVCs</li><li id="""">Find Services that expose public IPs or external load balancers with no active backend</li><li id="""">Review node pools for consistently low utilization</li><li id="""">Audit for stale namespaces or environments created by CI/CD pipelines</li><li id="""">Check for lingering objects from terminated jobs or Helm charts</li><li id="""">Look for secrets or config maps unused by any workload</li><li id="""">Check for unused Network Endpoint Groups (NEGs) from deleted services</li><li id="""">Audit for abandoned Ingress resources that may retain Global Load Balancers</li></ul>","<ul id=""""><li id="""">Delete PVCs with unmounted Persistent Disks</li><li id="""">Clean up Services with no backend to release IPs and load balancers</li><li id="""">Scale down overprovisioned node pools</li><li id="""">Prune unused namespaces and workloads</li><li id="""">Implement governance for ephemeral environments and pipelines</li></ul>","<ul id=""""><li id="""">https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview</li><li id="""">https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes</li><li id=""""><a href=""https://cloud.google.com/kubernetes-engine/docs/how-to/load-balance-l7"" id="""">https://cloud.google.com/kubernetes-engine/docs/how-to/load-balance-l7</a></li><li id=""""><a href=""https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview"" id="""">https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview</a></li></ul>",FALSE,FALSE,,,,<p>GCP-Compute-9463</p>
Outdated AWS Glue Version for Python Jobs,outdated-aws-glue-version-for-python-jobs-d2d59,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80dffcf2a6db2621c33f,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:26 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Nick Sudarikov,aws,aws-glue,outdated-runtime-version,compute,"<p id="""">Newer AWS Glue versions—such as Glue 5.0—include significant performance optimizations for **Python-based** ETL jobs, often reducing runtime by 10–60%. These improvements do not require any code changes, making version upgrades a simple and impactful optimization. When jobs remain on older runtimes such as Glue 3.0 or 4.0, they execute more slowly, consume more DPUs, and incur unnecessary cost. Additionally, Glue 5.0 offers more worker types (larger standard workers and memory-optimized workers), that can provide additional performance gain for some jobs. This inefficiency does not apply to Scala-based jobs, which do not benefit from the same performance uplift.</p>","<p id="""">AWS Glue is billed per second based on the number of Data Processing Units (DPUs) used. Faster runtime versions reduce job execution time, lowering total DPU consumption and overall cost. Older versions can extend runtime duration without providing any functional benefit.</p>","<ul id=""""><li id="""">Review Glue job configurations to identify jobs still running on older Glue versions such as 3.0 or 4.0</li><li id="""">Determine whether the job is Python-based, as only Python workloads benefit from upgrading</li><li id="""">Assess job execution times that remain consistently longer than expected for similar workloads</li><li id="""">Evaluate whether runtime upgrades have been incorporated into standard Glue job governance or deployment templates</li></ul>","<ul id=""""><li id="""">Upgrade Python-based Glue jobs to the latest supported Glue runtime version to benefit from performance improvements</li><li id="""">Incorporate runtime version management into IaC templates and deployment pipelines to ensure consistent modernization</li><li id="""">Periodically review job runtimes to confirm alignment with current Glue versions and available optimizations</li><li id="""">Validate job output after upgrading to confirm compatibility with updated library and Spark versions</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/glue/latest/dg/release-notes.html"" id="""">https://docs.aws.amazon.com/glue/latest/dg/release-notes.html</a></li></ul>",FALSE,FALSE,,,,AWS-Compute-1601
Outdated Aurora Versions Triggering Extended Support Charges,outdated-aurora-versions-triggering-extended-support-charges,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58f7288fed1c67359bd,FALSE,FALSE,Mon Nov 03 2025 16:17:19 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:46:00 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:46:00 GMT+0000 (Coordinated Universal Time),Dhara Kansagara,aws,aws-aurora,outdated-engine-version,databases,"<p id="""">Customers often delay upgrading Aurora clusters due to compatibility concerns or operational overhead. However, when older versions such as MySQL 5.7 or PostgreSQL 11 move into Extended Support, AWS applies automatic surcharges to ensure continued patching. These charges affect all clusters regardless of usage, creating unnecessary cost exposure across both production and non-production environments. For large Aurora fleets, the incremental expense can become significant if upgrades are not proactively managed.</p>","<p id="""">Aurora Extended Support adds a per-vCPU, per-hour charge for clusters running outdated MySQL or PostgreSQL engine versions beyond their community lifecycle. The surcharge applies in addition to normal Aurora instance costs and is incurred even when clusters are idle.</p>","<ul id=""""><li id="""">Inventory all Aurora clusters and identify the engine type and major version in use</li><li id="""">Compare active versions with AWS lifecycle policies to determine if they are under Extended Support</li><li id="""">Analyze billing data for “RDS Extended Support” charges tied to Aurora clusters</li><li id="""">Review development, staging, or idle Aurora environments for avoidable Extended Support spend</li></ul>","<ul id=""""><li id="""">Upgrade Aurora clusters to currently supported major versions to avoid Extended Support charges</li><li id="""">Test upgrades in lower environments to validate application compatibility before production cutovers</li><li id="""">Decommission unused or non-critical Aurora clusters rather than paying ongoing surcharges</li><li id="""">Adopt governance practices that monitor version lifecycle timelines and plan upgrades before end of life</li></ul>","<p id="""">Amazon Aurora Engine Versions | Amazon RDS Extended Support</p>",FALSE,FALSE,,N/A,,<p>AWS-Databases-8221</p>
Outdated Azure App Service Plan,outdated-azure-app-service-plan,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4d02e839f6de1b61ea,FALSE,FALSE,Thu May 22 2025 14:57:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Prasanna Ravichandran,azure,azure-app-service,outdated-resource,compute,"<p id="""">  Applications running on App Service V2 plans may incur higher operational costs and degraded performance compared to V3 plans. V2 uses older hardware generations that lack access to platform-level enhancements introduced in V3, including improved cold start times, faster scaling, and enhanced networking options.</p><p id="""">This inefficiency often arises from legacy deployments or default provisioning choices that haven't been revisited. Without proactive review, teams may continue running production workloads on suboptimal infrastructure—paying more for less performance.</p>","<ul id=""""><li id="""">App Service plans are billed based on:</li><li id=""""><strong id="""">Instance size and tier</strong> (e.g., Premium v2 vs. Premium v3)</li><li id=""""><strong id="""">Provisioned capacity</strong> (number of instances)</li><li id="""">V2 plans are often <strong id="""">less efficient per vCPU/memory unit</strong> and may lack access to new platform features</li><li id="""">V3 plans offer <strong id="""">better cost-performance</strong>, especially for workloads sensitive to cold starts or scaling latency</li></ul>","<ul id=""""><li id="""">Identify applications hosted on Premium V2 or Isolated V1 App Service plans</li><li id="""">Check whether a review has been conducted to evaluate compatibility with V3 plans</li><li id="""">Look for signs of degraded performance, such as slow cold starts or scaling lag</li><li id="""">Determine if newer App Service features are required but unavailable on V2 plans</li></ul>","<ul id=""""><li id="""">Evaluate workload compatibility with V3-based plans (e.g., Premium v3 or Isolated v2)</li><li id="""">Plan a phased migration of applications from V2 to V3 to improve performance and reduce cost per resource unit</li><li id="""">Update infrastructure-as-code templates and provisioning defaults to prefer V3-based plans</li><li id="""">Periodically review App Service plan usage to align with current Azure best practices</li></ul>","<ul id=""""><li id=""""> <a href=""https://learn.microsoft.com/en-us/azure/app-service/overview-hosting-plans#premium-v3-and-isolated-v2"" id="""">Premium v3 and Isolated v2 plan options for Azure App Service</a></li><li id=""""> <a href=""https://azure.microsoft.com/en-us/pricing/details/app-service/"" id="""">App Service Pricing</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Compute-8122</p>
Outdated EKS Cluster Incurring Extended Support Charges,outdated-eks-cluster-incurring-extended-support-charges,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b43c0bd0856e2327652,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),"Jason Eckle, Yulia Perlis",aws,aws-eks,inefficient-configuration,compute,"<p id="""">When an EKS cluster remains on a Kubernetes version that has reached the end of standard support, AWS begins charging an additional Extended Support fee. These charges often arise from delays in upgrade cycles, uncertainty about workload compatibility, or overlooked legacy clusters. If the workload does not require the older version, continuing to run the cluster in this state results in unnecessary cost and technical risk.</p>","<p id="""">EKS charges a fixed hourly rate per cluster for the control plane. In addition, AWS imposes an additional hourly Extended Support charge per cluster for any control plane running a Kubernetes version beyond the standard support period. This fee is charged in addition to the standard cluster fee, and continues until the cluster is upgraded to a supported version.</p>","<ul id=""""><li id="""">Identify clusters running Kubernetes versions that have reached end of standard support</li><li id="""">Confirm whether the cluster is incurring Extended Support charges</li><li id="""">Review upgrade history and backlog to understand why the version was not updated</li><li id="""">Assess any upgrade blockers, such as incompatible workloads or dependency constraints</li><li id="""">Engage with platform teams to determine if the version can be upgraded safely</li></ul>","<p id="""">Upgrade the EKS control plane and associated node groups to a supported Kubernetes version. Follow AWS best practices for staged upgrades, workload compatibility testing, and version planning. Incorporate version lifecycle checks into your infrastructure review process to avoid future support charges.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html\&quot;"" id="""">AWS EKS Kubernetes Version Lifecycle</a></li><li id=""""><a href=""https://aws.amazon.com/eks/pricing/\"" id="""">AWS EKS Pricing</a></li></ul>",FALSE,FALSE,,98,,<p>AWS-Compute-5729</p>
Outdated ElastiCache Node Type,outdated-elasticache-node-type,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b426e9d967771be5d6a,FALSE,FALSE,Thu May 22 2025 14:57:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-elasticache,overprovisioned-resource,databases,"<p id="""">Some ElastiCache clusters continue to run on older-generation node types that have since been replaced by newer, more cost-effective options. This can happen due to legacy templates, lack of version validation, or infrastructure that has not been reviewed in years. Newer instance families often deliver better performance at a lower hourly rate. Modernizing to newer node types can reduce compute spend without sacrificing performance, and in many cases, improve it.</p>","<p id="""">ElastiCache nodes are billed hourly based on instance type, engine (Redis or Memcached), and AWS region. Older-generation node types (e.g., M3, T2, R3) often have higher hourly rates and lower performance efficiency compared to newer generations (e.g., M6g, R6g, T4g), especially those powered by AWS Graviton processors. Costs accrue continuously while nodes are running.</p>","<ul id=""""><li id="""">Identify ElastiCache nodes running on older-generation instance types (e.g., T2, M3, R3)</li><li id="""">Compare current node types to newer generation equivalents that offer the same size and better price/performance (e.g., M6g, R6g, T4g)</li><li id="""">Evaluate whether there are operational or application constraints that prevent an upgrade</li><li id="""">Confirm that your current Redis or Memcached version is supported on the target node type, and upgrade the engine version if required before migrating.</li><li id="""">Engage with infrastructure or platform teams to validate upgrade feasibility and timing</li></ul>","<p id="""">Upgrade ElastiCache clusters from older-generation node types to newer instance families that offer improved performance and reduced cost. Use AWS documentation to identify compatible replacements and ensure support for your Redis or Memcached version. Plan upgrades during a maintenance window to minimize disruption. Consider Graviton-based nodes for additional savings where supported.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/elasticache/pricing/\&quot;"" id="""">Amazon ElastiCache Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheNodeType.html\"" id="""">Supported Node Types</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/graviton/\"" id="""">Choosing Graviton-Based Nodes</a></li></ul>",FALSE,FALSE,,75,,<p>AWS-Databases-2772</p>
Outdated Elasticsearch Version Triggering Extended Support Charges,outdated-elasticsearch-version-triggering-extended-support-charges,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b49097a641a499777d7,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Yulia Perlis,aws,aws-elasticsearch,inefficient-configuration,databases,"<p id="""">Many legacy workloads still run on older Elasticsearch versions — particularly 5.x, 6.x, or 7.x — due to inertia, compatibility constraints, or lack of ownership. Once these versions exceed their standard support window, AWS begins charging an hourly Extended Support fee for each domain. These fees are often missed in cost reviews, especially in environments that are inactive but still provisioned. In aggregate, outdated Elasticsearch clusters contribute to significant silent spend unless proactively addressed.</p>","<p id="""">Elasticsearch domains are billed per hour based on:</p><ul id=""""><li id="""">Instance size and count</li><li id="""">Storage (EBS or UltraWarm)</li><li id="""">Extended Support Charges — applied per domain per hour for versions outside standard support</li></ul>","<ul id=""""><li id="""">Inventory all Elasticsearch domains running on Amazon OpenSearch Service</li><li id="""">Identify engine versions using the AWS Console or API</li><li id="""">Review cost reports for Extended Support line items</li><li id="""">Flag domains that are inactive or non-critical but still incurring support costs</li></ul>","<ul id=""""><li id="""">Upgrade to a supported version of OpenSearch</li><li id="""">Be aware that upgrading from Elasticsearch 7.x may require reindexing or application compatibility changes</li><li id="""">Where possible, consolidate or delete unused domains to eliminate unnecessary charges</li><li id="""">Establish a lifecycle policy review process to ensure domains stay within standard support timelines</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/blogs/big-data/amazon-opensearch-service-announces-standard-and-extended-support-dates-for-elasticsearch-and-opensearch-versions/\&quot;"" id="""">AWS Blog: OpenSearch Version Support Model</a></li><li id=""""><a href=""https://aws.amazon.com/opensearch-service/pricing/\"" id="""">Amazon OpenSearch Pricing</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Databases-8053</p>
Outdated OpenSearch Version Triggering Extended Support Charges,outdated-opensearch-version-triggering-extended-support-charges,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b49227d926580d4c46e,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Yulia Perlis,aws,aws-opensearch,inefficient-configuration,databases,"<p id="""">Domains running outdated OpenSearch versions — particularly OpenSearch 1.x — begin to incur AWS Extended Support charges once they fall outside of the standard support period. These charges are persistent and apply even if the domain is inactive or lightly used. Many teams overlook this cost when delaying upgrades or maintaining non-critical environments like dev, test, or staging. In large organizations, outdated versions can silently drive meaningful spend over time, especially across many small or idle domains.</p>","<p id="""">OpenSearch domains are billed per hour based on:</p><ul id=""""><li id="""">Instance size and count</li><li id="""">Storage (including UltraWarm if enabled)</li><li id="""">Extended Support Charges — applied per hour per domain once a version exceeds its standard support window</li></ul>","<ul id=""""><li id="""">List all OpenSearch domains and record their engine version</li><li id="""">Compare against the AWS OpenSearch Support Calendar</li><li id="""">Identify any domains running versions outside standard support</li><li id="""">Use the AWS Console or API to retrieve version metadata</li><li id="""">Review billing data to detect Extended Support fees under OpenSearch spend</li></ul>","<ul id=""""><li id="""">Upgrade OpenSearch domains to a supported version</li><li id="""">Test upgrade compatibility in lower environments before applying in production</li><li id="""">Decommission inactive domains to eliminate unnecessary support and compute charges</li><li id="""">Incorporate version tracking into platform reviews to proactively avoid future Extended Support exposure</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/blogs/big-data/amazon-opensearch-service-announces-standard-and-extended-support-dates-for-elasticsearch-and-opensearch-versions/\&quot;"" id="""">AWS Blog: OpenSearch Version Support</a></li><li id=""""><a href=""https://aws.amazon.com/opensearch-service/pricing/\"" id="""">Amazon OpenSearch Pricing</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Databases-7692</p>
Outdated Provisioned IOPS Volume Type for High-I/O Workloads,outdated-provisioned-iops-volume-type-for-high-i-o-workloads,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894182795fc3b6d3191eb,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Adam Richter,aws,aws-ebs,outdated-resource-selection,storage,"<p id="""">Many environments continue using io1 volumes for high-performance workloads due to legacy provisioning or lack of awareness of io2 benefits. io2 volumes provide equivalent or better performance and durability with reduced cost at scale. Failing to adopt io2 where appropriate results in unnecessary spend on IOPS-heavy volumes.</p>","<p id="""">Provisioned IOPS volumes are billed based on the amount of storage (per GB) and the number of provisioned IOPS. While io1 and io2 share the same baseline pricing, io2 introduces <strong id="""">tiered IOPS pricing</strong>—offering a lower per-IOPS rate once volumes exceed thresholds such as 32,000 IOPS. For high-throughput workloads, this tiering makes io2 significantly more cost-effective.</p>","<ul id=""""><li id="""">Review provisioned EBS volumes with the io1 volume type</li><li id="""">Identify volumes provisioned with more than 32,000 IOPS</li><li id="""">Confirm the workload requires sustained high IOPS</li><li id="""">Estimate cost differences between io1 and io2 for similar configurations</li><li id="""">Check for infrastructure-as-code templates still defaulting to io1</li></ul>","<ul id=""""><li id="""">Convert high-IOPS io1 volumes to io2 where supported</li><li id="""">Update provisioning templates to default to io2 for performance-critical workloads</li><li id="""">Validate application compatibility (typically no changes required)</li><li id="""">Monitor cost and performance post-migration to confirm impact</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"" id="""">Amazon EBS Volume Types</a><br><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#ebs-volume-types-io2"" id="""">Provisioned IOPS SSD io2 Volumes</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-5883</p>
Outdated RDS Cluster Incurring Extended Support Charges,outdated-rds-cluster-incurring-extended-support-charges,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b44f1ca61e1ef49c190,FALSE,FALSE,Thu May 22 2025 14:57:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-rds,modernization,databases,"<p id="""">When an RDS cluster is not upgraded in time, it can fall out of standard support and incur Extended Support charges. This often happens when upgrade cycles are delayed, blocked by compatibility issues, or deprioritized due to competing initiatives. Over time, these fees can add up significantly. Staying on an outdated version also increases operational risk and reduces access to engine improvements, performance enhancements, and security patches.</p>","<p id="""">RDS instances are billed per hour based on instance type, size, and region, with additional charges for storage, I/O, and backups. If a cluster is running a database engine version that has reached the end of standard support, AWS applies Extended Support charges, which are billed per vCPU-hour. This fee applies in addition to standard RDS charges until the cluster is upgraded to a supported version.</p>","<ul id=""""><li id="""">Identify RDS clusters running database engine versions past their standard support window</li><li id="""">Confirm whether the cluster is currently accruing Extended Support charges</li><li id="""">Check whether a newer, recommended version is available and compatible with the application</li><li id="""">Evaluate blockers such as ORM compatibility, library dependencies, or testing backlog.</li></ul>","<p id="""">Plan and execute a controlled upgrade to a supported engine version. Use AWS recommendations or internal version policies to determine the target version. Conduct application testing in staging environments before applying changes in production. Integrate engine version reviews into your standard infrastructure lifecycle to avoid future Extended Support charges.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/rds/pricing/\&quot;"" id="""">RDS Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Engine_Updates.html\"" id="""">RDS Engine Version Support Policy</a></li></ul>",FALSE,FALSE,,104,,<p>AWS-Databases-5213</p>
Outdated RDS Versions Triggering Extended Support Charges,outdated-rds-versions-triggering-extended-support-charges,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58fd15d0e844792a664,FALSE,FALSE,Mon Nov 03 2025 16:17:19 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Dhara Kansagara,aws,aws-rds,outdated-engine-version,databases,"<p id="""">Many organizations continue to run outdated database engines, such as MySQL 5.7 or PostgreSQL 11, beyond their support windows. Beginning in 2024, AWS automatically enrolls these into Extended Support to maintain security updates, adding incremental charges that scale with vCPU count. These costs often appear suddenly, impacting both production and non-production environments. For development and test databases in particular, the charges may outweigh their value, leading to hidden inefficiencies if not addressed promptly.</p>","<p id="""">RDS Extended Support incurs a per-vCPU, per-hour surcharge on top of standard RDS instance charges once a database engine version passes its community end-of-life date. Fees apply continuously, even if the instance is idle.</p>","<ul id=""""><li id="""">Inventory all RDS instances and record engine type and major version</li><li id="""">Compare deployed versions against AWS published support timelines to determine Extended Support status</li><li id="""">Check billing data for “RDS Extended Support” line items and correlate to specific RDS instances</li><li id="""">Identify non-production or idle databases that are accruing Extended Support charges without business justification</li></ul>","<ul id=""""><li id="""">Upgrade RDS instances to currently supported major versions to avoid Extended Support fees</li><li id="""">Plan upgrades in non-production first to validate compatibility before applying in production</li><li id="""">Decommission unused or development databases that no longer provide value</li><li id="""">Embed version monitoring in database governance processes to anticipate and prepare for end-of-life events</li></ul>",,FALSE,FALSE,,N/A,,<p>AWS-Databases-5308</p>
Outdated Virtual Machine Version in Azure,outdated-virtual-machine-version-in-azure,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84c35b4ca176b5ea081,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Jurian van Hoorn,azure,azure-virtual-machines,outdated-resource,compute,"<p id="""">Many organizations choose a VM SKU and version (e.g., `D4s_v3`) during the initial planning phase of a project, often based on availability, compatibility, or early cost estimates. Over time, Microsoft releases newer hardware generations (e.g., `D4s_v4`, `D4s_v5`) that offer equivalent or better performance at the same or reduced cost. However, existing VMs are not automatically migrated, and these newer versions are often overlooked unless intentionally evaluated.</p><p id="""">This inefficiency tends to persist because switching to a newer version typically requires VM deallocation and resizing, which introduces temporary downtime. As a result, outdated VM series versions continue to run indefinitely, even in environments where brief downtime is acceptable. The cost delta between series versions—especially across certain families like `D`, `E`, or `F`—can be significant when scaled across environments or multiple VMs. Note that VM series versions (v3, v4, v5) are distinct from VM generations (Gen 1 vs Gen 2), with series versions representing the primary opportunity for cost optimization.</p>","<p id="""">Azure VMs are billed per second based on instance size, operating system, and hardware generation (series version). Newer VM versions typically offer equivalent or better performance at the same of reduced cost due to improved infrastructure efficiency.. Legacy versions may persist in production environments despite suboptimal price/performance ratios, simply because they were originally selected during initial provisioning or project planning.</p>","<ul id=""""><li id="""">Identify running VMs using older version SKUs ((e.g., v3 when v4 or v5 are available)</li><li id="""">Compare current SKUs to newer available versions within the same family and size class</li><li id="""">Check whether newer versions are available in the same region and support the required OS or workload</li><li id="""">Assess cost differences using the Azure Pricing Calculator</li><li id="""">Confirm with application teams whether brief downtime for resizing is acceptable</li><li id="""">Review whether the original SKU selection is still aligned with performance and cost objectives</li><li id="""">Consider that some older series may have limited regional availability or eventual deprecation timelines</li></ul>","<ul id=""""><li id="""">Evaluate alternative VM versions (e.g., v4 or v5) within the same family to identify better cost/performance options</li><li id="""">Plan and schedule VM resizing during maintenance windows to avoid unplanned downtime</li><li id="""">Coordinate with application owners to validate compatibility and risk tolerance</li><li id="""">Periodically audit VM fleet for outdated versions as part of cost governance</li><li id="""">Prioritize migrations from significantly outdated series (e.g., v3 to v5) for maximum benefit</li></ul>","<ul id=""""><li id="""">https://learn.microsoft.com/en-us/azure/virtual-machines/sizes</li><li id="""">https://azure.microsoft.com/en-us/pricing/details/virtual-machines/</li><li id="""">https://learn.microsoft.com/en-us/azure/virtual-machines/change-vm-size</li></ul>",FALSE,FALSE,,,,<p>Azure-Compute-2837</p>
Outdated and Expensive EBS Volume Type,outdated-and-expensive-ebs-volume-type,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4446471869064cd1a0,FALSE,FALSE,Thu May 22 2025 14:57:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-ebs,modernization,storage,"<p id="""">This inefficiency occurs when legacy volume types such as gp2 or io1 remain in use, even though AWS has released newer types—like gp3 and io2—that offer better performance at lower cost. Gp3 allows users to configure IOPS and throughput independently of volume size, while io2 provides higher durability and more predictable performance than io1. These newer volumes are generally more cost-effective and can be adopted without re-architecting workloads. Many teams continue using outdated types due to default AMIs, automation templates, or simple oversight.</p>","<p id="""">EBS volumes are billed per GB-month based on the provisioned size and the selected volume type. Additional charges may apply for provisioned IOPS or throughput beyond the default baseline, depending on the volume type.</p>","<ul id=""""><li id="""">Identify volumes using gp2, io1, or other legacy types</li><li id="""">Compare current performance needs (IOPS, throughput) with the default capabilities of newer alternatives</li><li id="""">Evaluate whether the volume type was chosen intentionally or inherited from legacy infrastructure</li><li id="""">Check whether Elastic Volumes is supported for live migration on the attached instance</li><li id="""">Consult with infrastructure owners or automation templates to confirm ongoing default behavior</li></ul>","<p id="""">Modify the EBS volume type in place—such as changing from gp2 to gp3, or io1 to io2—using the AWS Console, CLI, or API. The change is typically non-disruptive and completes without downtime. In rare cases where in-place modification isn’t supported (e.g., older instance types), create a snapshot and restore it to a new volume with the desired configuration. Update AMIs, launch templates, and automation tools to ensure future volumes use cost-effective, modern types by default.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\&quot;"" id="""">EBS Volume Types</a></li><li id=""""><a href=""https://aws.amazon.com/ebs/pricing/\"" id="""">EBS Pricing</a></li></ul>",FALSE,FALSE,,8,,<p>AWS-Storage-9554</p>
Outdated and Expensive Premium SSD Disk,outdated-and-expensive-premium-ssd-disk,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b47e2c8699623007bd7,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),,azure,azure-managed-disks,modernization,storage,"<p id="""">Workloads using legacy Premium SSD managed disks may be eligible for migration to Premium SSD v2, which delivers equivalent or improved performance characteristics at a lower cost. Premium SSD v2 decouples disk size from performance metrics like IOPS and throughput, enabling more granular cost optimization. Additionally, Premium SSD disks are often overprovisioned in size—for example, a P40 disk with more IOPS and capacity than the workload requires—resulting in inflated storage costs. Rightsizing includes both transitioning to v2 and resizing to smaller SKUs (e.g., P40 → P20) based on observed utilization. Failure to address either form of overprovisioning leads to persistent waste.</p>","<p id="""">Azure Premium SSD disks are billed per provisioned GB per month, with costs based on the selected disk size and tier. Premium SSD v2 offers greater flexibility in performance configuration and lower pricing compared to the original Premium SSD offering, allowing users to optimize both cost and performance.</p>","<ul id=""""><li id="""">Identify Premium SSD managed disks provisioned using the original Premium SSD offering (not v2)</li><li id="""">Review disk IOPS, throughput, and sizing requirements to ensure compatibility with Premium SSD v2 capabilities</li><li id="""">Analyze whether the current SKU size (e.g., P30, P40) exceeds actual capacity and performance needs</li><li id="""">Confirm that the region supports Premium SSD v2, as availability may vary across Azure regions</li><li id="""">Evaluate whether application or OS dependencies require any disk-specific configurations during migration</li><li id="""">Validate migration plans with infrastructure owners to align with performance expectations and change management policies</li></ul>","<p id="""">Transition eligible Premium SSD disks to Premium SSD v2 to achieve improved cost efficiency and flexible performance configuration. Where appropriate, resize oversized disks to smaller SKUs that better match actual usage. Migration and resizing can typically be performed without downtime using snapshots or disk export/import processes. Monitor performance metrics after migration to ensure service quality is maintained. Update infrastructure provisioning templates to adopt Premium SSD v2 and rightsized configurations by default where applicable.</p>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types#premium-ssd-v2\&quot;"" id="""">Azure Premium SSD v2 Disks Overview</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/managed-disks/\"" id="""">Azure Managed Disks Pricing</a></li></ul>",FALSE,FALSE,,123,,<p>Azure-Storage-3384</p>
Outdated and Expensive Standard SSD Disk,outdated-and-expensive-standard-ssd-disk,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b47e2ec8a6fafc66ca3,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,azure,azure-managed-disks,modernization,storage,"<p id="""">Standard SSD disks can often be replaced with Premium SSD v2 disks, offering enhanced IOPS, throughput, and durability at competitive or lower pricing. For workloads that require moderate to high performance but are currently constrained by Standard SSD capabilities, migrating to Premium SSD v2 improves both performance and cost efficiency without significant operational overhead.</p>","<p id="""">Standard SSD disks are billed per provisioned GB per month. Premium SSD v2 offers a more flexible and cost-efficient model that can outperform Standard SSDs for certain workloads, providing better alignment between performance needs and spend.</p>","<ul id=""""><li id="""">Identify Managed Disks using the Standard SSD offering that are eligible for migration to Premium SSD v2</li><li id="""">Review workload performance requirements to confirm suitability for Premium SSD v2 characteristics</li><li id="""">Verify regional availability of Premium SSD v2 before planning migration</li><li id="""">Evaluate disk attachment patterns and application dependencies to ensure compatibility during transition</li><li id="""">Validate proposed changes with infrastructure teams to align with performance, availability, and cost goals</li></ul>","<p id="""">Migrate eligible Standard SSD disks to Premium SSD v2 to enhance performance while reducing costs. Migration can typically be achieved through disk snapshots, image exports, or disk-level cloning without significant downtime. Update infrastructure provisioning standards to default to Premium SSD v2 where appropriate to maintain ongoing optimization.</p>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types#premium-ssd-v2"" id="""">Azure Premium SSD v2 Disks Overview</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/managed-disks/"" id="""">Azure Managed Disks Pricing</a></li></ul>",FALSE,FALSE,,127,,<p>Azure-Storage-4741</p>
Over-Retained Exported Object Versions in GCS Versioning Buckets,over-retained-exported-object-versions-in-gcs-versioning-buckets,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589419df82bf1073931459,FALSE,FALSE,Sun Jun 22 2025 23:39:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,gcp,gcp-gcs,over-retention-of-data,storage,"<p id="""">When GCS object versioning is enabled, every overwrite or delete operation creates a new noncurrent version. Without a lifecycle rule to manage old versions, they persist indefinitely. Over time, this results in:  * Accumulation of outdated data   * Unnecessary storage costs, especially in Standard or Nearline classes   * Lack of visibility into what is still needed vs. legacy debris  This issue often goes unnoticed in environments with frequent data updates or automated processes (e.g., logs, models, config snapshots).</p>","<p id="""">Storage costs accrue based on:  * Total volume of data stored (including noncurrent object versions)   * Storage class (e.g., Standard, Nearline, Coldline, Archive)   * Retention of object versions, regardless of access patterns  Versioned buckets retain every object version until explicitly deleted or transitioned, leading to silent growth in storage usage if no lifecycle policy is enforced.</p>","<ul id=""""><li id="""">Identify GCS buckets with versioning enabled</li><li id="""">Review total storage split between current and noncurrent object versions</li><li id="""">Assess frequency of updates to versioned objects</li><li id="""">Determine if lifecycle rules exist to delete or transition noncurrent versions</li><li id="""">Evaluate whether object version retention aligns with compliance or business needs</li></ul>","<ul id=""""><li id="""">Implement lifecycle policies to delete noncurrent versions after a defined period</li><li id="""">Transition noncurrent versions to colder storage classes (e.g., Archive) if needed for compliance</li><li id="""">Audit versioned buckets periodically to ensure alignment with data governance and cost goals</li></ul>",,FALSE,FALSE,,,,<p>GCP-Storage-5855</p>
Overbilling Due to Tier Switches and Allocation Overlaps in DTU Model,overbilling-due-to-tier-switches-and-allocation-overlaps-in-dtu-model,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fd568cdfe584aa647d,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sid Ahmed Redjini,azure,azure-sql,suboptimal-pricing-model,databases,"<p id="""">Workloads that frequently scale up and down within the same day—whether manually, via automation, or platform-managed—can encounter hidden cost amplification under the DTU model. When a database changes tiers (e.g., S7 → S4), Azure treats each tiered segment as a separate allocation and applies full-hour rounding independently. In some cases, both tiers may be billed for the same time period due to failover, reallocation delays, or timing mismatches during transitions.</p><p id="""">This behavior is opaque to most users because billing granularity is daily, and Azure does not explicitly surface overlapping charges. The result is unexpected overbilling where a single database may appear to consume 28 or more “hours” of DTU in a single calendar day. While technically aligned with Azure’s billing design, this creates inefficiencies when tier switches are frequent and uncoordinated.</p>","<p id="""">In the DTU-based pricing model, customers select a predefined service tier (e.g., S3, S6, S7), and are billed per hour of provisioned capacity, regardless of actual usage. If the tier is changed during the day, Azure rounds each allocation to the next full hour and may bill for both tiers if there are overlaps. Even a brief overlap or mid-hour tier switch can result in multiple billable hours for a single wall-clock hour. Over time, this behavior can result in more than 24 billed hours in a single day for a single database.</p>","<ul id=""""><li id="""">Identify Azure SQL Database resources using the DTU-based pricing model</li><li id="""">Check for multiple billing entries for the same database and date</li><li id="""">Verify whether different service tiers (e.g., S3, S6) appear in the same day for the same resource</li><li id="""">Calculate total DTU-consumed hours per day; values exceeding 24 suggest overlapping allocations</li><li id="""">Review workload automation or manual scaling activity for frequency of same-day tier switches</li><li id="""">Assess whether tier transitions are occurring mid-hour or near peak usage windows</li></ul>","<ul id=""""><li id="""">Minimize same-day tier switches unless operationally justified</li><li id="""">Schedule up/down-scaling during off-peak windows to reduce risk of overlapping billing</li><li id="""">Move to the vCore or serverless pricing model for more transparent and granular cost control</li><li id="""">Establish internal policies to monitor and govern tier changes for DTU-based databases</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/purchase-models-dtu-vcore-comparison"" id="""">https://learn.microsoft.com/en-us/azure/azure-sql/database/purchase-models-dtu-vcore-comparison</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu"" id="""">https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-pricing"" id="""">https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-pricing</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Databases-7399</p>
Overly Frequent Querying in Azure Monitor Alerts,overly-frequent-querying-in-azure-monitor-alerts,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fcbb110698f1554004,FALSE,FALSE,Thu Aug 28 2025 22:32:28 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Jurian van Hoorn,azure,,inefficient-configuration,other,"<p id="""">While high-frequency alerting is sometimes justified for production SLAs, it's often overused across non-critical alerts or replicated blindly across environments. Projects with multiple environments (e.g., dev, QA, staging, prod) often duplicate alert rules without adjusting for business impact, which can lead to alert sprawl and inflated monitoring costs.</p><p id="""">In large-scale environments, reducing the frequency of non-critical alerts—especially in lower environments—can yield significant savings. Teams often overlook this lever because alert configuration is considered part of operational hygiene rather than cost control. Tuning alert frequencies based on SLA requirements and actual urgency is a low-friction optimization opportunity that does not compromise observability when implemented thoughtfully.</p>","<p id="""">Azure Monitor Alerts are billed based on the number of alert rules and the frequency of evaluation. Alert rules with higher evaluation frequencies generate more evaluation requests per month, directly impacting costs. Metric alerts are charged per rule and evaluation, while log search alerts are billed based on query frequency and data volume processed. For example, Alert rules with a 1-minute evaluation frequency are significantly more expensive than those set to 5, or 15-minute intervals.</p><p id="""">These costs scale linearly with the number of rules and environments. For example, a 1-minute frequency can cost over 5x more than a 15-minute interval, making frequency configuration a key cost lever, especially in environments with hundreds or thousands of alerts.</p>","<ul id=""""><li id="""">Identify Azure Monitor alerts with a 1-minute or 5-minute evaluation frequencyReview whether the alert is tied to a production SLA or high-severity operational impact</li><li id="""">Query Azure Resource Graph across various subscriptions</li><li id="""">Evaluate the business criticality of each alert and its current frequency</li><li id="""">Assess whether alerts were duplicated across environments with the same configuration</li><li id="""">Confirm with application owners whether high-frequency evaluation is still justified</li></ul>","<ul id=""""><li id="""">Test changes gradually. Start with non-production environments and non-critical alerts</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/alerts-metric-overview"" id="""">https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/alerts-metric-overview</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview"" id="""">https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/monitor/"" id="""">https://azure.microsoft.com/en-us/pricing/details/monitor/</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Other-7364</p>
Overly Permissive VPC Flow Log Filters Sent to CloudWatch Logs,overly-permissive-vpc-flow-log-filters-sent-to-cloudwatch-logs,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fdf4f474a95b3742bb,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Kevin Talbierz,aws,aws-cloudwatch,,other,"<p id="""">VPC Flow Logs configured with the ALL filter and delivered to CloudWatch Logs often result in unnecessarily high log ingestion volumes — especially in high-traffic environments. This setup is rarely required for day-to-day monitoring or security use cases but is commonly enabled by default or for temporary debugging and then left in place. As a result, teams incur excessive CloudWatch charges without realizing the logging configuration is misaligned with actual needs.</p>","<p id="""">CloudWatch Logs is billed based on:</p><p id="""">Ingestion volume (per GB) — charged for each log event ingested</p><p id="""">Storage (per GB per month) — ongoing charges for retained logs</p>","<ul id=""""><li id="""">Identify VPC Flow Logs with a destination of CloudWatch Logs</li><li id="""">Check whether the log filter is set to ALL</li><li id="""">Review total ingestion volumes and associated CloudWatch Logs charges over time</li><li id="""">Evaluate whether full traffic logging is necessary or if ACCEPT or REJECT would suffice</li><li id="""">Confirm whether logs are used for active monitoring or compliance, or are remnants of prior debugging</li></ul>","<ul id=""""><li id="""">Update the VPC Flow Log filter to ACCEPT or REJECT where appropriate</li><li id="""">Consider redirecting logs to S3 for lower-cost storage if detailed analysis is not required in CloudWatch</li><li id="""">Implement periodic audits of logging configurations to catch overly verbose setups</li><li id="""">Align logging retention and granularity with actual monitoring and compliance needs</li></ul>","<ul id=""""><li id="""">Logging IP traffic using VPC Flow Logs</li><li id="""">Amazon CloudWatch Logs pricing</li></ul>",FALSE,FALSE,,,,<p>AWS-Other-8388</p>
Overprovisioned Azure Database for PostgreSQL Flexible Server,overprovisioned-azure-database-for-postgresql-flexible-server,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4da96194f6d3c73848,FALSE,FALSE,Thu May 22 2025 14:57:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-database-for-postgresql---flexible-server,overprovisioned-resource,databases,"<p id="""">  Azure Database for PostgreSQL – Flexible Server often defaults to general-purpose D-series VMs, which may be oversized for many production or development workloads. PostgreSQL typically does not require a high sustained high CPU, making it well-suited to memory-optimized (E-series) or burstable (B-series) instances.</p><p id="""">When actual usage consistently falls below the provisioned capacity — particularly CPU — the deployment may be overprovisioned, resulting in unnecessary compute charges. Choosing the wrong VM family or size leads to silent overspend, especially in long-lived database environments.</p>","<ul id=""""><li id="""">Charged based on:</li><li id="""">Provisioned compute (vCores and instance family)</li><li id="""">Provisioned storage (per GB/month)</li><li id="""">Pricing varies significantly by VM series (e.g., D-series vs. B-series vs. E-series)</li><li id="""">Charges accrue continuously, regardless of actual CPU or memory usage</li></ul>","<ul id=""""><li id="""">Analyze average CPU and memory utilization of PostgreSQL Flexible Server instances</li><li id="""">Determine whether current performance requirements justify the provisioned instance size</li><li id="""">Identify servers running on D-series or other general-purpose SKUs that could be supported by B-series or E-series</li><li id="""">Validate compatibility and performance before considering a shift in instance family</li></ul>","<ul id=""""><li id="""">Resize the PostgreSQL Flexible Server to a smaller or more suitable VM family based on actual workload behavior</li><li id="""">For low-CPU workloads, consider B-series (burstable) or E-series (memory-optimized) configurations</li><li id="""">Review usage patterns quarterly to ensure the selected SKU remains aligned with performance needs</li><li id="""">Avoid unnecessary performance buffer unless justified by workload spikes or growth</li></ul>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/postgresql/flexible-server/"" id="""">Azure Database for PostgreSQL – Flexible Server Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/concepts-compute-storage"" id="""">Compute options for PostgreSQL Flexible Server</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Databases-4858</p>
Overprovisioned Compute Tier in Azure SQL Database,overprovisioned-compute-tier-in-azure-sql-database,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4d25616f62e3819208,FALSE,FALSE,Thu May 22 2025 14:57:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-sql,overprovisioned-resource,databases,"<p id="""">  Azure SQL Database resources are frequently overprovisioned due to default configurations, conservative sizing, or legacy requirements that no longer apply. This inefficiency appears across all deployment models:</p><p id="""">* Single Databases may be assigned more DTUs or vCores than the workload requires  * Elastic Pools may be oversized for the actual demand of pooled databases  * Managed Instances are often deployed with excess compute capacity that remains underutilized</p><p id="""">Because billing is based on provisioned capacity, not actual consumption, organizations incur unnecessary costs when sizing is not aligned with workload behavior. Without regular reviews, these resources become persistent sources of waste — especially across dev/test environments or variable workloads.</p>","<ul id=""""><li id="""">Billed based on provisioned compute resources (DTUs or vCores), regardless of usage</li><li id="""">Storage billed per GB/month</li><li id="""">Higher tiers (e.g., Business Critical, Premium) incur significantly greater cost</li><li id="""">Charges apply whether the database is actively used or idle</li></ul>","<ul id=""""><li id="""">Identify SQL Databases, Elastic Pools, or Managed Instances with consistently low CPU and memory utilization</li><li id="""">Review actual workload patterns and performance requirements relative to the provisioned tier</li><li id="""">Determine whether the workload could perform equally well on a lower SKU or with fewer vCores</li><li id="""">Check for stale, inactive, or low-traffic workloads that no longer require high performance tiers</li></ul>","<ul id=""""><li id="""">Downsize the compute tier (DTUs or vCores) to better match observed usage</li><li id="""">For Elastic Pools, reduce the total eDTUs/vCores and consider consolidating lightly used databases</li><li id="""">For Managed Instances, assess whether the vCore allocation can be reduced or workloads refactored</li><li id="""">Establish periodic reviews of resource utilization as part of database lifecycle management</li><li id="""">Use performance insights to guide rightsizing decisions, independent of reservation or licensing strategies</li></ul>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/azure-sql-database/"" id="""">Azure SQL Database Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/performance-guidance-overview"" id="""">Performance Guidance for Azure SQL Database</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/monitoring-sql-database"" id="""">Monitor resource utilization</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Databases-4864</p>
Overprovisioned EBS Volume,overprovisioned-ebs-volume,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894155cb321668d6183e7,FALSE,FALSE,Sun Jun 22 2025 23:39:01 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Mike Rosenberg,aws,aws-ebs,inefficient-configuration,storage,"<p id="""">EBS volumes often remain significantly overprovisioned compared to the actual data stored on them. Because billing is based on the total provisioned capacity—not actual usage—this creates ongoing waste when large volumes are only partially used. Overprovisioning may result from default sizing in templates, misestimated requirements, or conservative provisioning practices. Identifying and remediating these cases can lead to meaningful storage cost reductions without impacting workload performance.</p>","<p id="""">Amazon EBS volumes are billed per GB-month of provisioned storage capacity, not based on the amount of data actually stored on the volume. You incur charges for the total provisioned size—even if only a portion of that capacity is used. This pricing model means volumes with low utilization relative to their allocated size can result in unnecessary costs.</p>","<ul id=""""><li id="""">Identify EBS volumes with low storage utilization compared to their provisioned size</li><li id="""">Compare used GB vs. allocated GB for each volume, using a configurable threshold to flag inefficiencies</li><li id="""">Review usage trends to ensure the volume hasn’t recently grown or experienced temporary spikes</li><li id="""">Assess whether volume size was inherited from legacy templates or provisioned without sizing analysis</li></ul>","<ul id=""""><li id="""">If the volume is attached to a running EC2 instance and can tolerate replacement, create a smaller volume of the same type and migrate the data</li><li id="""">For volumes that cannot be replaced easily, plan to adjust provisioning defaults in AMIs, launch templates, or infrastructure-as-code going forward</li><li id="""">Document sizing assumptions and consider automated checks to catch overprovisioning at creation time</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"" id="""">Amazon EBS Volume Types and Sizing</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-5151</p>
Overprovisioned Load Balancer,overprovisioned-load-balancer,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894179d508827025112ab,FALSE,FALSE,Sun Jun 22 2025 23:39:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,oci,oci-load-balancer,overprovisioned-networking-resource,networking,"<p id="""">Load balancers incur charges based on provisioned bandwidth shape, even if backend traffic is minimal. If traffic is low, or if only one backend server is configured, the load balancer may be oversized or unnecessary, especially in test or staging environments.</p>","<p id="""">Charged based on bandwidth and minimum bandwidth shape (Mbps), not just request volume.</p>","<ul id=""""><li id="""">Review traffic metrics to assess actual vs. provisioned throughput</li><li id="""">Identify load balancers with only one or inactive backend servers</li><li id="""">Check for load balancers in non-production environments with little to no usage</li><li id="""">Evaluate whether an Application Gateway or direct instance access is more appropriate</li></ul>","<ul id=""""><li id="""">Downgrade to a smaller bandwidth shape if supported</li><li id="""">Decommission underutilized load balancers</li><li id="""">Consolidate redundant load balancers across applications or environments</li></ul>","<p id=""""><a href=""https://www.oracle.com/cloud/networking/load-balancing/pricing/"" id="""">Load Balancing Pricing and Shapes</a><br><a href=""https://docs.oracle.com/en-us/iaas/Content/Balance/Tasks/managingloadbalancer.htm"" id="""">Managing Load Balancers</a></p>",FALSE,FALSE,,,,<p>OCI-Networking-4589</p>
Overprovisioned Managed Disk for VM Limits,overprovisioned-managed-disk-for-vm-limits,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4825616f62e3818d5f,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-managed-disks,overprovisioned-resource,storage,"<p id="""">Each Azure VM size has a defined limit for total disk IOPS and throughput. When high-performance disks (e.g., Premium SSDs with high IOPS capacity) are attached to low-tier VMs, the disk’s performance capabilities may exceed what the VM can consume. This results in paying for performance that the VM cannot access. For example, attaching a large Premium SSD to a B-series VM will not provide the expected performance because the VM cannot deliver that level of throughput. Without aligning disk selection with VM limits, organizations incur unnecessary storage costs with no corresponding performance benefit.</p>","<p id="""">Azure Managed Disks are billed based on:</p><ul id=""""><li id="""">Provisioned capacity (GB/month)</li><li id="""">Disk type (Premium, Standard SSD, etc.)</li><li id="""">Provisioned performance (IOPS and MBps), where applicable</li></ul><p id="""">However, the maximum usable IOPS and throughput is also capped by the VM size — regardless of what the disk can deliver.</p>","<ul id=""""><li id="""">Compare attached disk performance (IOPS and MBps) against the maximum supported by the associated VM size</li><li id="""">Identify cases where disk capabilities exceed what the VM can deliver</li><li id="""">Review whether performance tier (e.g., Premium SSD) is justified for the workload</li><li id="""">Consult VM and disk documentation to validate the effective throughput bottleneck</li></ul>","<ul id=""""><li id="""">Resize disks to match the performance envelope of the associated VM</li><li id="""">Downgrade to lower disk tiers (e.g., Premium SSD → Standard SSD) when full performance is not needed</li><li id="""">Establish guardrails to ensure disk and VM configurations are aligned during provisioning and resizing events</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/sizes"" id="""">Azure VM Sizes and IOPS Limits</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types"" id="""">Azure Managed Disk Types and Performance</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Storage-5871</p>
Overprovisioned Memory Allocation for Lambda Functions,overprovisioned-memory-allocation-for-lambda-functions,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b48e774e67ee1f12135,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Shireen Maini,aws,aws-lambda,overprovisioned-resource,compute,"<p id="""">Each Lambda function must be configured with a memory setting, which indirectly controls the amount of CPU and networking performance allocated. In many environments, memory settings are defined arbitrarily or left unchanged as functions evolve. Over time, this leads to overprovisioning — with functions running well below their allocated memory and incurring unnecessary compute costs. Systematic right-sizing using performance benchmarks can significantly reduce spend without sacrificing performance or reliability. This is especially important for frequently invoked functions or those with long execution times.</p>","<p id="""">Lambda is billed based on:</p><ul id=""""><li id="""">Number of invocations</li><li id="""">Duration (ms) × memory size (MB)</li><li id="""">Larger memory settings increase vCPU and I/O, but also cost more per millisecond</li></ul><p id="""">Overprovisioning memory leads to unnecessary cost without meaningful performance improvement, especially if functions complete within modest CPU and memory bounds.</p>","<ul id=""""><li id="""">Analyze memory usage metrics for Lambda functions to identify consistently low utilization</li><li id="""">Use performance profiling (e.g., Lambda Power Tuning) to model cost vs. performance tradeoffs</li><li id="""">Check for functions using default or inherited memory values without workload-specific tuning</li><li id="""">Look for functions with high invocation volume and long durations to prioritize optimization impact</li></ul>","<ul id=""""><li id="""">Use tools like AWS Lambda Power Tuning to benchmark and optimize memory and vCPU settings</li><li id="""">Incorporate right-sizing into CI/CD workflows to evaluate configuration during deployment</li><li id="""">Apply consistent tagging or governance to track functions requiring periodic review</li><li id="""">Consider automated tuning pipelines for environments with frequent code changes or dynamic usage patterns</li></ul>","<ul id=""""><li id=""""><a href=""https://github.com/alexcasalboni/aws-lambda-power-tuning"" id="""">AWS Lambda Power Tuning</a></li><li id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html"" id="""">Lambda Resource Configuration</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-4631</p>
Overprovisioned Memory Allocation in Cloud Run Services,overprovisioned-memory-allocation-in-cloud-run-services,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941abc6b6874a4840276,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),,gcp,gcp-cloud-run,overprovisioned-resource-allocation,compute,"<p id="""">In Cloud Run, each revision is deployed with a fixed memory allocation (e.g., 512MiB, 1GiB, 2GiB, etc.). These settings are often overestimated during initial development or copied from templates. Unlike auto-scaling platforms that adapt instance size based on workload, Cloud Run continues to bill per the allocated amount regardless of actual memory used during execution.  If a service consistently uses significantly less memory than allocated, it results in avoidable overpayment per request — especially for high-throughput or long-running services. Since memory and CPU are billed together based on configured values, this inefficiency compounds quickly at scale.</p>","<p id="""">Billed based on:  * Allocated vCPU and memory (GiB) per request   * Duration of request execution (per 100ms increment)   * Number of requests   * Additional charges for egress and requests beyond free tier</p>","<ul id=""""><li id="""">Review actual memory usage per request over a representative time window</li><li id="""">Identify services with consistently low memory utilization relative to their configured limits</li><li id="""">Evaluate whether higher memory tiers were chosen to solve startup latency or cold start issues that no longer apply</li><li id="""">Cross-reference high-throughput services where per-request efficiency has significant cost impact</li></ul>","<ul id=""""><li id="""">Reconfigure services with right-sized memory allocations aligned to observed usage patterns</li><li id="""">Test progressively smaller memory configurations to find a stable baseline without introducing latency or OOM errors</li><li id="""">Implement monitoring for memory pressure or failures to validate new settings</li><li id="""">Use performance benchmarks and load tests in lower environments before promoting configuration changes to production</li></ul>","<p id=""""><a href=""https://cloud.google.com/run/pricing?hl=en"" id="""">Cloud Run Pricing</a><br><a href=""https://cloud.google.com/run/docs/configuring/memory-limits"" id="""">Cloud Run Resource Allocation</a></p>",FALSE,FALSE,,,,<p>GCP-Compute-7272</p>
Overprovisioned Memory in Cloud Run Services,overprovisioned-memory-in-cloud-run-services,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589419cb772959e146eae3,FALSE,FALSE,Sun Jun 22 2025 23:39:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),,gcp,gcp-cloud-run,overprovisioned-resource,compute,"<p id="""">Cloud Run allows users to allocate up to 8 GB of memory per container instance. If memory is overestimated — often as a buffer or based on unvalidated assumptions — customers pay for more than what the workload consumes during execution. Unlike in VM-based environments where memory might be shared or underutilized without direct cost impact, in Cloud Run, you're billed precisely for what you allocate.  This inefficiency often results from:  * Defaulting to high memory values for “safety”   * Not using monitoring tools to assess actual memory usage   * Lack of clear ownership over service tuning</p>","<p id="""">Charged based on:  * Allocated memory and CPU per instance   * Execution duration (rounded up to the nearest 100ms)   * Number of requests and networking egress (if applicable)  Even unused allocated memory is fully billed per 100ms of execution time, making memory overprovisioning a direct driver of excess cost.</p>","<ul id=""""><li id="""">Identify Cloud Run services with high memory allocation (e.g., \&gt;1 GB)</li><li id="""">Compare against actual memory usage (visible in Cloud Monitoring or Cloud Trace)</li><li id="""">Review historical memory usage variance across multiple invocations</li><li id="""">Flag workloads with stable memory use but large memory headroom</li><li id="""">Check for default templates or configurations that may enforce high memory settings</li></ul>","<ul id=""""><li id="""">Reduce memory allocation to match observed memory usage with a buffer for spikes</li><li id="""">Continuously monitor function-level memory metrics to right-size allocations over time</li><li id="""">Set up proactive alerts for services with memory allocation far exceeding usage</li><li id="""">Refactor container images or code to optimize memory consumption</li><li id="""">Establish governance policies or templates that encourage conservative starting values</li></ul>",,FALSE,FALSE,,,,<p>GCP-Compute-7004</p>
Overprovisioned Node Pool in GKE Cluster,overprovisioned-node-pool-in-gke-cluster,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941ad0b50092e6a5e6dd,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,gcp,gcp-gke,overprovisioned-resource,compute,"<p id="""">Node pools provisioned with large or specialized VMs (e.g., high-memory, GPU-enabled, or compute-optimized) can be significantly overprovisioned relative to the actual pod requirements. If workloads consistently leave a large portion of resources unused (e.g., low CPU/memory request-to-capacity ratio), the organization incurs unnecessary compute spend. This often happens in early cluster design phases, after application demand shifts, or when teams allocate for peak usage without autoscaling.</p>","<p id="""">Billed based on:  * Underlying Compute Engine VMs in the node pool (vCPU, memory, and attached storage)   * GPU usage (if applicable)   * Operating system licensing (for premium OS options)  Control plane is free for Autopilot clusters or billed per cluster for Standard mode.</p>","<ul id=""""><li id="""">Compare requested vs. allocatable CPU and memory across node pools</li><li id="""">Identify persistent gaps between allocated and requested resources</li><li id="""">Review cluster autoscaler activity and pod eviction logs to assess actual demand patterns</li><li id="""">Check for large nodes with single pods or with minimal utilization</li><li id="""">Determine whether node pools have taints preventing broader scheduling</li></ul>","<ul id=""""><li id="""">Resize nodes to align with observed workload requirements</li><li id="""">Enable or tune cluster autoscaler to manage node pool size dynamically</li><li id="""">Split heterogeneous workloads into separate node pools for right-sized resources</li><li id="""">Use Autopilot mode for smaller environments where Google manages node sizing</li><li id="""">Consolidate workloads and reduce fragmentation by adjusting pod limits and requests</li></ul>","<p id="""">https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler  https://cloud.google.com/kubernetes-engine/docs/how-to/resizing-node-pools  https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture</p>",FALSE,FALSE,,,,<p>GCP-Compute-6165</p>
Overprovisioned Storage in Azure SQL Elastic Pools or Managed Instances,overprovisioned-storage-in-azure-sql-elastic-pools-or-managed-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4d8f53ad27da30f11b,FALSE,FALSE,Thu May 22 2025 14:57:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-sql,overprovisioned-resource,databases,"<p id="""">  Azure SQL deployments often reserve more storage than needed, either due to default provisioning settings or anticipated future growth. Over time, if actual usage remains low, these oversized allocations generate unnecessary storage costs.</p><p id="""">In Elastic Pools, resizing can be done through standard configuration updates. In Managed Instances, reducing storage may require a shrink operation to reclaim unused space before reallocation is permitted. Without regular review, these overprovisioned environments persist as silent cost contributors.</p>","<ul id=""""><li id="""">Storage is billed per GB/month based on provisioned size</li><li id="""">Charges apply even if actual database usage is significantly lower than the allocated storage</li><li id="""">Additional charges apply for backup storage, compute, and IOPS depending on SKU</li></ul>","<ul id=""""><li id="""">Review total provisioned storage versus actual used capacity for Elastic Pools and Managed Instances</li><li id="""">Identify instances with consistently low storage usage relative to their provisioned limit</li><li id="""">Confirm whether current storage needs justify the allocated capacity</li><li id="""">Determine whether shrink operations are necessary to reclaim space</li></ul>","<ul id=""""><li id="""">Where supported, reduce provisioned storage to better align with actual usage</li><li id="""">For Managed Instances, safely execute `DBCC SHRINKFILE` or equivalent operations before resizing</li><li id="""">Incorporate storage reviews into regular database hygiene practices</li><li id="""">Use forecasting or growth trends to balance cost savings with future scaling needs</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/reserved-capacity-overview"" id="""">Resize storage for Azure SQL Elastic Pools</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/maintenance-auto-shrink"" id="""">Manage storage for Azure SQL Managed Instance</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Databases-6107</p>
Overprovisioned Throughput in EFS,overprovisioned-throughput-in-efs,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fdd768c1eead67527c,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Matt Wilder,aws,aws-efs,,storage,"<p id="""">When file systems are launched with Provisioned Throughput, teams often overestimate future demand — especially in environments cloned from production or sized “just to be safe.” Over time, many workloads consume far less throughput than allocated, especially in dev/test environments or during periods of reduced usage. These overprovisioned settings can silently accrue substantial monthly charges that go unnoticed without intentional review.</p><p id="""">This inefficiency is not flagged by AWS Trusted Advisor and is easy to miss. Elastic Throughput mode now offers a scalable alternative that automatically adjusts capacity — but isn’t always cheaper, depending on the workload’s sustained throughput.</p>","<p id="""">Amazon EFS supports three throughput modes:</p><p id="""">Bursting Throughput Mode – Default option for General Purpose file systems. Throughput automatically scales with the amount of storage; short bursts above the baseline are supported using a credit system. No separate throughput charge.</p><p id="""">Provisioned Throughput Mode – Throughput is explicitly configured in MB/s and billed separately from storage, regardless of actual usage.</p><p id="""">Elastic Throughput Mode – Throughput scales automatically to meet demand and is billed based on actual usage above the baseline included with storage.</p><p id="""">Provisioned throughput is billed continuously, even if the application’s actual throughput is far below the configured amount. Elastic mode eliminates this fixed cost but can be more expensive if workloads regularly exceed the baseline.</p>","<ul id=""""><li id="""">Identify EFS file systems configured with Provisioned Throughput Mode</li><li id="""">Compare provisioned throughput (MB/s) to actual usage over time</li><li id="""">Assess whether throughput usage consistently falls below provisioned baseline</li><li id="""">Flag file systems in dev/test environments that mirror production settings</li><li id="""">Evaluate whether switching to Elastic Throughput/Bursting Throughput would reduce cost without affecting performance</li><li id="""">Confirm whether burst patterns might result in higher costs under Elastic mode</li></ul>","<ul id=""""><li id="""">Reconfigure overprovisioned file systems with a reduced Provisioned Throughput value</li><li id="""">For workloads with low and predictable throughput, consider switching to Elastic Throughput</li><li id="""">For dev/test systems cloned from production, adjust throughput settings independently</li><li id="""">Establish a periodic review of throughput configurations as part of FinOps or infrastructure governance</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/efs/pricing/"" id="""">https://aws.amazon.com/efs/pricing/</a></li><li id=""""><a href=""https://docs.aws.amazon.com/efs/latest/ug/performance.html"" id="""">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/efs/latest/ug/performance.html#throughput-modes"" id="""">https://docs.aws.amazon.com/efs/latest/ug/performance.html#throughput-modes</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Storage-7191</p>
Overprovisioned Throughput in Pub/Sub Lite,overprovisioned-throughput-in-pub-sub-lite,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941915e028bdef7fb0fe,FALSE,FALSE,Sun Jun 22 2025 23:39:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),ChatGPT (Validated by Taylor Houck),gcp,gcp-pub-sub-lite,overprovisioned-resource-allocation,other,"<p id="""">Pub/Sub Lite is a cost-effective alternative to standard Pub/Sub, but it requires explicitly provisioning throughput capacity. When publish or subscribe throughput is overestimated, customers continue to pay for unused capacity — similar to idle virtual machines or overprovisioned IOPS.  This inefficiency is often found in development environments or early-stage production workloads where traffic patterns are unpredictable or have since decreased.</p>","<p id="""">* Billed based on provisioned publish and subscribe throughput (measured in MiB/s)   * Billed based on storage used for retained messages   * Charges accrue regardless of actual usage, based on the configured throughput capacity</p>","<ul id=""""><li id="""">Identify topics with provisioned throughput that significantly exceeds average or peak usage</li><li id="""">Review historical usage trends for sustained underutilization of publish or subscribe capacity</li><li id="""">Assess whether retained message storage aligns with actual replay or analytics needs</li><li id="""">Confirm whether the provisioned throughput matches updated workload characteristics</li><li id="""">Evaluate dev/test environments where capacity is set and forgotten</li></ul>","<ul id=""""><li id="""">Reduce provisioned throughput to better match actual traffic levels</li><li id="""">Consider right-sizing both publish and subscribe throughput independently</li><li id="""">Archive or delete unused topics with retained throughput settings</li><li id="""">Implement monitoring alerts for sustained low throughput utilization</li><li id="""">Reassess whether Pub/Sub Lite is still the optimal choice vs. standard Pub/Sub or another messaging service</li></ul>","<p id=""""><a href=""https://cloud.google.com/pubsub/lite/docs/pricing"" id="""">Pub/Sub Lite Pricing</a><br><a href=""https://cloud.google.com/pubsub/lite/docs/provisioning"" id="""">Capacity and Throughput Management</a><br><a href=""https://cloud.google.com/pubsub/lite/docs/monitoring"" id="""">Monitoring Throughput Usage</a></p>",FALSE,FALSE,,,,<p>GCP-Other-5227</p>
Overprovisioned Vertex AI Endpoints,overprovisioned-vertex-ai-endpoints-59cc7,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e1ab6a7b77cf03cfbc,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:27 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,gcp,gcp-vertex-ai,overprovisioned-minimum-capacity,ai,"<p id="""">Vertex AI Prediction Endpoints support autoscaling but require customers to specify a **minimum number of replicas**. These replicas stay online at all times to serve incoming traffic. When the minimum value is set too high for real traffic levels, the system maintains idle capacity that still incurs hourly charges. This inefficiency commonly arises when teams: * Use default replica settings during initial deployment, * Intentionally overprovision “just in case” without revisiting the configuration, or * Copy settings from production into lower-traffic dev or QA environments. Over time, unused replica hours accumulate into significant, silent spend.</p>","<p id="""">Vertex AI Endpoints are billed per node-hour based on the machine type provisioned. When the minimum replica count is set higher than actual usage requires, replicas remain active and accrue cost even when idle.</p>","<ul id=""""><li id="""">Compare minimum replica configuration against actual traffic patterns and request volume</li><li id="""">Identify endpoints with consistently low utilization or long periods of idle time</li><li id="""">Review environments (e.g., dev, test, staging) where minimum replicas match production settings</li><li id="""">Assess whether autoscaling parameters were set once and never revisited as workloads evolved</li></ul>","<ul id=""""><li id="""">Lower the minimum number of replicas to match real usage and traffic patterns</li><li id="""">Adopt conservative autoscaling baselines for dev and test environments</li><li id="""">Periodically review autoscaling configurations as demand changes over time</li><li id="""">Use load testing or peak analysis to ensure reduced replica counts still meet latency targets</li></ul>",,FALSE,FALSE,,,,GCP-AI-9476
Overreliance on Lambda at Sustained Scale,overreliance-on-lambda-at-sustained-scale,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fd0bc79c0c722f612c,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Igor Bareev,aws,aws-lambda,suboptimal-pricing-model,compute,"<p id="""">Lambda is designed for simplicity and elasticity, but its pricing model becomes expensive at scale. When a function runs frequently (e.g., millions of invocations per day) or for extended durations, the cumulative cost may exceed that of continuously running infrastructure. This is especially true for predictable workloads that don’t require the dynamic scaling Lambda provides.</p><p id="""">Teams often continue using Lambda out of convenience or architectural inertia, without revisiting whether the workload would be more cost-effective on EC2, ECS, or EKS. This inefficiency typically hides in plain sight—functions run correctly and scale as needed, but the unit economics are no longer favorable.</p>","<p id="""">Lambda is billed per invocation and execution duration (GB-seconds), with costs increasing linearly as usage scales. While ideal for low-to-moderate workloads with bursty or unpredictable traffic, Lambda becomes cost-inefficient at sustained high volumes. In contrast, compute services like EC2, ECS, or EKS offer lower per-unit cost when provisioned for predictable, long-running workloads—even when accounting for idle time.</p>","<ul id=""""><li id="""">Identify Lambda functions with consistently high invocation counts or sustained concurrency</li><li id="""">Analyze total monthly execution time and aggregate GB-seconds to estimate spend</li><li id="""">Compare Lambda spend against equivalent cost modeling for EC2, ECS, or EKS</li><li id="""">Determine whether the workload pattern is predictable and steady enough to justify reserved or container-based compute</li><li id="""">Assess whether cold start latency or fine-grained autoscaling is still required</li><li id="""">Review whether any past evaluations have been made regarding migration to alternative architectures</li></ul>","<ul id=""""><li id="""">Establish thresholds for Lambda usage that trigger cost-efficiency reviews</li><li id="""">Evaluate total Lambda cost versus equivalent EC2/ECS/EKS workloads</li><li id="""">Consider replatforming long-running or consistently triggered workloads to containerized or instance-based compute</li><li id="""">Incorporate architecture re-evaluation into regular FinOps or infrastructure reviews for high-throughput functions</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro.html"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro.html</a></li><li id=""""><a href=""https://aws.amazon.com/lambda/pricing/"" id="""">https://aws.amazon.com/lambda/pricing/</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-4763</p>
Oversized Hosting Plan for Azure Functions,oversized-hosting-plan-for-azure-functions,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fd52616e9b5d1d5fcd,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Stijn Depril,azure,,,compute,"<p id="""">Teams often choose the Premium or App Service Plan for Azure Functions to avoid cold start delays or enable VNET connectivity, especially early in a project when performance concerns dominate. However, these decisions are rarely revisited—even as usage patterns change.</p><p id="""">In practice, many workloads running on Premium or App Service Plans have low invocation frequency, minimal execution time, and no strict latency requirements. This leads to consistent spend on compute capacity that is largely idle. Because these plans still “work” and don’t cause reliability issues, the inefficiency is easy to overlook. Over time, this misalignment between hosting tier and actual usage creates significant invisible waste.</p>","<p id="""">Azure Functions are billed differently depending on the hosting plan selected.</p>","<ul id=""""><li id="""">Identify Function Apps running on Premium or App Service Plans</li><li id="""">Review invocation frequency and execution time to assess activity level</li><li id="""">Compare provisioned capacity (instances, memory) against actual usage patterns</li><li id="""">Check whether cold start latency is still a relevant concern for each workload</li><li id="""">Evaluate whether functions require VNET integration or advanced features unique to Premium/App Service</li><li id="""">Flag workloads with minimal activity that may not justify fixed-instance billing</li></ul>","<ul id=""""><li id="""">Move low-usage or non-critical Function Apps to the Consumption Plan</li><li id="""">Pilot plan downgrades in non-production or latency-tolerant environments</li><li id="""">Use cost modeling tools to estimate savings from switching to Consumption Plan</li><li id="""">Establish a review process to re-evaluate plan fit periodically, checking changes in invocation frequency, execution duration, latency needs, VNET requirements, and cost vs: usage.</li><li id="""">Document latency and feature requirements per workload to avoid defaulting to oversized plans</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-premium-plan"" id="""">https://learn.microsoft.com/en-us/azure/azure-functions/functions-premium-plan</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale#billing-models"" id="""">https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale#billing-models</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-consumption-costs"" id="""">https://learn.microsoft.com/en-us/azure/azure-functions/functions-consumption-costs</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Compute-1429</p>
Oversized RDS Instance Storage,oversized-rds-instance-storage,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b44b6006ad5b82dd05a,FALSE,FALSE,Thu May 22 2025 14:57:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-rds,overprovisioned-resource,databases,"<p id="""">This inefficiency occurs when an RDS instance is allocated significantly more storage than it consumes. For example, a 2TB volume might contain only 150GB of actual data. Since RDS does not allow reducing allocated storage on existing instances, these volumes continue to incur charges based on total provisioned size—not actual usage. This often goes unnoticed in long-running databases that no longer require their original allocation.</p>","<p id="""">RDS storage is billed per GB-month of provisioned capacity. Charges apply whether or not the database actually uses the full allocated space. Storage type (e.g., gp2, gp3, io1) and optional features like Multi-AZ, backups, and snapshots may further influence the cost.</p>","<ul id=""""><li id="""">Identify RDS instances where provisioned storage significantly exceeds actual usage</li><li id="""">Review storage metrics to validate consistent underutilization (e.g., &lt;25% usage over time)</li><li id="""">Evaluate whether storage needs have declined due to archival, data aging, or workload changes</li><li id="""">Confirm that attached storage is not constrained by IOPS provisioning or replication settings</li><li id="""">Consult with DBAs to determine if data can be migrated to a smaller, more cost-effective volume</li></ul>","<p id="""">Since RDS does not support reducing storage size in-place, you must migrate the database to a new instance with smaller allocated storage. This can be done by restoring from a snapshot or using the DB engine’s native dump and restore utilities. For minimal downtime, consider using AWS Database Migration Service (DMS) to replicate data to a new instance. After migration, enable storage AutoScaling to allow future growth while minimizing overprovisioning. Ensure proper validation and cutover planning before decommissioning the original instance.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/rds/pricing/\&quot;"" id="""">RDS Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\"" id="""">Amazon RDS Storage Types</a></li></ul>",FALSE,FALSE,,38,,<p>AWS-Databases-6814</p>
Oversized Worker or Driver Nodes in Databricks Clusters,oversized-worker-or-driver-nodes-in-databricks-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b47e09e434112c2f570,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Matt Weingarten,databricks,databricks-clusters,overprovisioned-resource,compute,"<p id="""">Databricks users can select from a wide range of instance types for cluster driver and worker nodes. Without guardrails, teams may choose high-cost configurations (e.g., 16xlarge nodes) that exceed workload requirements. This results in inflated costs with little performance benefit. To reduce this risk, administrators can use compute policies to define acceptable node types and enforce size limits across the workspace.</p>","<p id="""">Databricks costs are driven by:</p><ul id=""""><li id="""">Databricks Units (DBUs): Billed per hour based on node type</li><li id="""">Cloud Infrastructure Charges: Cost of underlying VMs, billed per second or minute</li></ul><p id="""">Larger node types (e.g., high-memory or high-I/O VMs) incur significantly higher charges. Oversizing clusters without justification leads to unnecessary DBU and infrastructure costs.</p>","<ul id=""""><li id="""">Review all cluster configurations to identify usage of large or high-cost instance types</li><li id="""">Query system tables for driver and worker node types across clusters</li><li id="""">Check whether compute policies are in place to limit allowable node sizes</li><li id="""">Engage with workload owners to confirm whether large instances are justified based on workload characteristics</li></ul>","<ul id=""""><li id="""">Define and enforce compute policies that restrict driver and worker node types to appropriate sizes</li><li id="""">Reconfigure existing clusters using oversized nodes to use smaller, cost-effective alternatives</li><li id="""">Allow exceptions only for workloads that demonstrably require high-performance nodes</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.databricks.com/aws/en/compute/cluster-config-best-practices"" id="""">Cluster Sizing Guidelines</a></li><li id=""""><a href=""https://docs.databricks.com/aws/en/admin/clusters/policies"" id="""">Compute Policies in Databricks</a></li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-2127</p>
Overuse of Photon in Non-Production Workloads,overuse-of-photon-in-non-production-workloads,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84c076c403a23d9438d,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Nicole Boyd,databricks,databricks-compute,inefficient-configuration,compute,"<p id="""">Photon is frequently enabled by default across Databricks workspaces, including for development, testing, and low-concurrency workloads. In these non-production contexts, job runtimes are typically shorter, SLAs are relaxed or nonexistent, and performance gains offer little business value.</p><p id="""">Enabling Photon in these environments can inflate DBU costs substantially without meaningful runtime improvements. By not differentiating cluster configurations between production and non-production, organizations may pay a premium for workloads that could run just as efficiently on standard compute.</p><p id="""">Cluster policies can be used to restrict Photon usage to explicitly tagged production workloads, helping enforce cost-conscious defaults and reduce unnecessary spend.</p>","<p id="""">Photon-enabled compute clusters incur a higher Databricks Unit (DBU) rate compared to standard compute. While the performance boost can be significant for complex workloads, the DBU multiplier — which can reach approximately 2.9x — may not be justified in development, staging, or ad-hoc environments where performance is less critical. This results in unnecessary cost when Photon is enabled for non-production workloads that don't benefit from its capabilities.</p>","<ul id=""""><li id="""">Identify compute clusters or job configurations with Photon enabled in dev, test, or sandbox environments</li><li id="""">Review naming conventions, tags, or workspace metadata to isolate non-production workload</li><li id="""">Analyze job characteristics: short runtimes, low concurrency, small data volumes, or non-SLA workloads using Photon</li><li id="""">Compare cost and runtime of similar jobs executed with and without Photon enabled</li><li id="""">Review cluster policies and workspace defaults to determine whether Photon is disabled by default for non-prod</li></ul>","<ul id=""""><li id="""">Disable Photon by default in dev/test environments using workspace settings or cluster policies</li><li id="""">Create separate cluster templates or policies for production and non-production workloads</li><li id="""">Use tagging or automation to flag or block Photon usage in low-priority environments</li><li id="""">Educate teams on the cost-performance tradeoffs of Photon and when its use is justified</li><li id="""">Periodically review Photon usage by environment and update governance rules accordingly</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.databricks.com/aws/en/compute/photon"" id="""">Databricks Photon Overview</a></li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-9761</p>
Pipeline Breaks from Outdated Dependency Images in Dataflow,pipeline-breaks-from-outdated-dependency-images-in-dataflow-fbd63,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80df4e37e54be2173c14,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Damian Ohienmhen,gcp,gcp-dataflow,operational-overhead-from-custom-image-maintenance,compute,"<p id="""">In restricted or isolated network environments, Dataflow workers often cannot reach the public internet to download runtime dependencies. To operate securely, organizations build custom worker images that bundle required libraries. However, these images must be manually updated to keep dependencies current. As upstream packages evolve, outdated internal images can cause pipeline errors, execution delays, or total job failures. Each failure wastes worker runtime, increases troubleshooting time, and leads to rebuild cycles that inflate operational and compute costs.</p>","<p id="""">Dataflow billing is based on worker instance time, storage, and additional data transfer. Pipeline interruptions or rebuilds caused by dependency issues increase both compute cost and engineering effort, leading to inefficiency even when direct spend isn’t immediately visible.</p>","<ul id=""""><li id="""">Review Dataflow job error logs for dependency or package resolution failures</li><li id="""">Identify whether pipelines rely on custom worker images with static dependency sets</li><li id="""">Assess the frequency of image rebuilds and whether delays correlate with dependency updates in public repositories</li><li id="""">Evaluate whether repeated pipeline restarts or failed jobs are linked to dependency mismatches</li></ul>","<ul id=""""><li id="""">Implement a scheduled process to rebuild and validate custom Dataflow images using the latest stable dependencies</li><li id="""">Maintain version tracking of bundled packages to detect when key libraries are updated upstream</li><li id="""">Automate dependency validation and image refresh through CI/CD workflows to minimize manual effort</li><li id="""">Review whether internet access or VPC Service Controls can be configured to safely allow dependency retrieval without fully manual image maintenance</li></ul>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/dataflow/docs/guides/using-custom-containers"" id="""">Using Custom Containers with Dataflow</a></li></ul>",FALSE,FALSE,,,,GCP-Compute-5273
Poorly Configured Autoscaling on Databricks Clusters,poorly-configured-autoscaling-on-databricks-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84c80f51af6ea8eb9eb,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Nicole Boyd,databricks,databricks-compute,inefficient-configuration,compute,"<p id="""">Autoscaling is a core mechanism for aligning compute supply with workload demand, yet it's often underutilized or misconfigured. In older clusters or ad-hoc environments, autoscaling may be disabled by default or set with tight min/max worker limits that prevent scaling. This can lead to persistent overprovisioning (and wasted cost during idle periods) or underperformance due to insufficient parallelism and job queuing. Poor autoscaling settings are especially common in manually created all-purpose clusters, where idle resources often go unnoticed.</p><p id="""">Overly wide autoscaling ranges can also introduce instability: Databricks may rapidly scale up to the upper limit if demand briefly spikes, leading to cost spikes or degraded performance. Understanding workload characteristics is key to tuning autoscaling appropriately.</p>","<p id="""">Databricks charges based on Databricks Units (DBUs) per node per hour. When autoscaling is misconfigured — such as by fixing the worker count (Min \= Max), setting narrow ranges, or disabling it entirely — clusters may remain overprovisioned during idle periods or underprovisioned during peak demand, resulting in inefficient compute spend or job failures.</p>","<ul id=""""><li id="""">Identify clusters with identical min and max worker values (fixed size)</li><li id="""">Review clusters with low task parallelism or frequent job queuing</li><li id="""">Check for long idle durations on high-capacity clusters</li><li id="""">Analyze job failures caused by memory or executor limits</li><li id="""">Use the Spark UI or Ganglia metrics to monitor how often clusters scale up or down</li><li id="""">Look for consistent underuse of allocated workers during job execution</li><li id="""">Review logs or metrics for unexpected or excessive scale-up behavior</li></ul>","<ul id=""""><li id="""">Use autoscaling for variable workloads, but avoid overly wide min/max ranges that allow clusters to over-expand. Databricks may aggressively scale up if limits are too high, leading to cost spikes and instability.</li><li id="""">For predictable, recurring jobs with stable compute requirements, consider using fixed-size clusters to avoid the cost and time of scaling transitions.</li><li id="""">Tune autoscaling thresholds based on real workload behavior. Start narrow and adjust iteratively, based on runtime performance and cluster utilization.</li><li id="""">Establish cluster policies to enforce sensible auto scaling defaults or require justification for disabling autoscaling entirely.</li><li id="""">Regularly review cluster usage patterns to refine scaling decisions. While some tools can automate this, most teams can start with basic monitoring of scaling events and job runtimes.</li></ul>",,FALSE,FALSE,,,,<p>Databricks-Compute-2508</p>
Provisioned Throughput OpenAI Deployment in Non-Production Environments,provisioned-throughput-openai-deployment-in-non-production-environments-cffc6,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e009ab2d162cc74c62,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:11 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Ariel Lichterman,azure,azure-cognitive-services,overprovisioned-deployment-model,ai,"<p id="""">PTU deployments guarantee dedicated throughput and low latency, but they also require paying for reserved capacity at all times. In non-production environments—such as dev, test, QA, or experimentation—usage patterns are typically sporadic and unpredictable. Deploying PTUs in these environments leads to consistent baseline spend without corresponding value. On-demand deployments scale usage cost with actual consumption, making them more cost-efficient for variable workloads.</p>","<p id="""">Provisioned Throughput Units are billed at a fixed hourly rate regardless of utilization. They are optimized for steady, high-throughput workloads. Non-production environments with low or inconsistent usage pay for committed capacity they rarely consume, making PTUs significantly more expensive than the on-demand consumption model.</p>","<ul id=""""><li id="""">Review OpenAI deployments in non-production environments to determine whether PTUs are configured instead of on-demand</li><li id="""">Assess utilization patterns to see if throughput demand fluctuates or remains low compared to the allocated PTUs</li><li id="""">Confirm whether the environment requires dedicated capacity, or if on-demand latency and throughput are sufficient</li><li id="""">Evaluate overall spend on PTU deployments relative to business value delivered in non-production settings</li></ul>","<ul id=""""><li id="""">Switch non-production OpenAI deployments from PTU to on-demand consumption pricing</li><li id="""">Reserve PTUs only for production workloads with sustained, predictable throughput requirements</li><li id="""">Establish governance standards to ensure deployment models match workload profiles across environments</li><li id="""">Periodically review OpenAI usage patterns to validate that non-production capacity aligns with actual utilization</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput"" id="""">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput</a></li></ul>",FALSE,FALSE,,202,,Azure-AI-9910
Recursive Invocation Loop Between Lambda and SQS,recursive-invocation-loop-between-lambda-and-sqs,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589417cb772959e146ea06,FALSE,FALSE,Sun Jun 22 2025 23:39:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Christine Oji,aws,aws-lambda,misconfigured-architecture,compute,"<p id="""">When a Lambda function processes messages from an SQS queue but fails to handle certain messages properly, the same messages may be returned to the queue and retried repeatedly. In some cases, especially if the Lambda is also writing messages back to the same or a chained queue, this can create a recursive invocation loop. This loop results in high invocation counts, prolonged execution, and unnecessary costs, particularly if retries continue without a termination strategy.</p>","<p id="""">Billed based on the number of invocations and duration of execution time in milliseconds, rounded up to the nearest 1ms. Repeated or unintended invocations directly increase cost.</p>","<ul id=""""><li id="""">Sudden spike in Lambda invocations without corresponding increase in business traffic</li><li id="""">High volume of messages visible in an SQS queue without corresponding successful Lambda completions</li><li id="""">Lambda metrics showing consistent re-execution of the same payloads</li><li id="""">Lack of configured dead-letter queues or maximum receive count thresholds</li><li id="""">Application behavior where Lambda publishes messages back to the originating or an indirectly connected SQS queue</li><li id="""">Reprocessing patterns identifiable by message IDs or payload similarity in logs</li><li id="""">Account-wide cost variance alerts pointing to Lambda or SQS service anomalies</li></ul>","<ul id=""""><li id="""">Introduce a <strong id="""">dead-letter queue (DLQ)</strong> to route failed messages after a maximum number of retries</li><li id="""">Set a <strong id="""">maximumReceiveCount</strong> on the SQS redrive policy to avoid indefinite reprocessing</li><li id="""">Refactor the Lambda logic to prevent re-enqueuing the same message unintentionally</li><li id="""">Monitor DLQ metrics and alarms to catch patterns before they become runaway costs</li><li id="""">Perform architecture reviews for recursive patterns in asynchronous messaging flows</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"" id="""">Using AWS Lambda with Amazon SQS</a><br><a href=""https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"" id="""">Amazon SQS Dead-Letter Queues</a><br><a href=""https://aws.amazon.com/lambda/pricing/"" id="""">AWS Lambda Pricing</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-8042</p>
Recursive Lambda Function Invocation,recursive-lambda-function-invocation,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589418191757855d9f6cbf,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Liam Greenamyre,aws,aws-lambda,recursive-invocation-misconfiguration,compute,"<p id="""">Recursive invocation occurs when a Lambda function triggers itself directly or indirectly, often through an event source like SQS, SNS, or another Lambda. This loop can be unintentional — for example, when the function writes output to a queue it also consumes. Without controls, this can lead to runaway invocations, dramatically increasing cost with no business value.</p>","<p id="""">* Billed per request and per millisecond of execution time   * Recursive invocations amplify both request count and total execution duration</p>","<ul id=""""><li id="""">Review Lambda functions for high and sustained invocation rates with no matching business activity</li><li id="""">Review event source and destination configurations for circular references (e.g., writing back to a triggering SQS queue)</li><li id="""">Check CloudWatch Logs for repetitive invocation patterns with identical payloads</li><li id="""">Correlate Lambda cost spikes with recent deployments or configuration changes</li><li id="""">Assess if DLQs or concurrency limits are missing from functions with self-trigger patterns</li></ul>","<ul id=""""><li id="""">Refactor logic to prevent self-invocation or recursive event loops</li><li id="""">Avoid writing to the same queue or stream that triggers the function</li><li id="""">Implement Dead Letter Queues to prevent retry loops</li><li id="""">Set concurrency limits using `ReservedConcurrency` or `MaximumRetryAttempts`</li><li id="""">Utilize anomaly detection software to flag unusual invocation or cost patterns</li><li id="""">Monitor function triggers and outputs as part of deployment validation</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html</a><br><a href=""https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a><br><a href=""https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#invocation-dlq"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\#invocation-dlq</a><br><a href=""https://aws.amazon.com/lambda/pricing/"" id="""">https://aws.amazon.com/lambda/pricing/</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-2439</p>
Resources Generating Excessive INFO Logs,resources-generating-excessive-info-logs-6c920,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80df4f45ed879c739fe0,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Yuval Goldstein,gcp,gcp-cloud-logging,excessive-log-verbosity,other,"<p id="""">Some GCP services and workloads generate INFO-level logs at very high frequencies — for example, load balancers logging every HTTP request or GKE nodes logging system health messages. While valuable for debugging, these logs can flood Cloud Logging with non-critical data. Without log-level tuning or exclusion filters, organizations incur continuous ingestion charges for messages that are seldom analyzed. Over time, this behavior compounds into a persistent waste driver across large-scale environments.</p>","<p id="""">Cloud Logging costs are driven by data ingestion volume and storage retention. Excessive INFO-level logs increase both metrics, especially when emitted by high-traffic resources such as API endpoints, Kubernetes clusters, or compute instances. Since these logs rarely indicate actionable events, their ingestion often yields limited operational value.</p>","<ul id=""""><li id="""">Identify resources or services with log ingestion volumes significantly above expected baselines</li><li id="""">Correlate ingestion costs with log severity levels to determine if INFO logs dominate total volume</li><li id="""">Review application or service configurations that define log verbosity levels</li><li id="""">Assess whether INFO-level logs provide measurable operational value or simply duplicate metrics already available through monitoring</li></ul>","<ul id=""""><li id="""">Adjust log verbosity in resource configurations or application code to emit fewer INFO-level messages</li><li id="""">Apply Logging sink filters or exclusions to prevent ingestion of repetitive INFO logs</li><li id="""">Aggregate high-volume INFO logs into sampled or summarized events where detailed records are unnecessary</li><li id="""">Establish logging standards that define appropriate verbosity levels for each environment and service type</li></ul>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/logging/docs/exclusions"" id="""">Cloud Logging Exclusions and Filters</a></li><li id=""""><a href=""https://cloud.google.com/stackdriver/pricing"" id="""">Cloud Logging Pricing</a></li></ul>",FALSE,FALSE,,,,GCP-Other-3116
Retention of Unused Data in Snowflake Table,retention-of-unused-data-in-snowflake-table,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4b1cb2a508e16a48c2,FALSE,FALSE,Thu May 22 2025 14:57:15 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-tables,excessive-data-retention,storage,"<p id="""">Retention of stale data occurs when old, no longer needed records are preserved within active Snowflake tables. Without lifecycle policies or regular purging, tables accumulate outdated data.</p><p id="""">Because Snowflake’s compute charges are tied to how much data is scanned, retaining large volumes of inactive or irrelevant data can drive up both storage and query execution costs unnecessarily.</p>",,"<ul id="""">  <li id="""">Review table size growth trends over time to identify tables accumulating data without regular trimming</li>  <li id="""">Analyze access patterns to determine whether older records are being queried or accessed by active workloads</li>  <li id="""">Assess whether current business or regulatory requirements justify the retention period for older data</li>  <li id="""">Identify opportunities to implement lifecycle policies or manual purging for data beyond the necessary retention window</li></ul>","<ul id="""">  <li id="""">Implement data retention policies to regularly archive or delete records older than the required retention period (e.g., retain only 90 days of data if historical lookbacks are not needed beyond that)</li>  <li id="""">Collaborate with business, analytics, and compliance teams to validate acceptable data retention thresholds</li>  <li id="""">Purge old records to reduce table storage size and improve query performance by minimizing unnecessary data scans</li>  <li id="""">Monitor table growth rates and periodically reassess lifecycle settings to ensure alignment with business needs</li></ul>","<ul id=""""><li id=""""><a href=""https://select.dev/posts/snowflake-unused-tables"" id="""">Unused Tables</a></li><li id=""""><a href=""https://medium.com/snowflake/optimizing-costs-in-snowflake-with-effective-storage-lifecycle-policies-027dd98eaafd"" id="""">Storage Lifecycle Policies</a></li><li id=""""><a href=""https://docs.snowflake.com/en/sql-reference/account-usage/access_history"" id="""">Access History</a></li></ul>",FALSE,FALSE,,,,<p>Snowflake-Storage-5068</p>
SFTP Feature Enabled on Azure Storage Account Without Usage,sftp-feature-enabled-on-azure-storage-account-without-usage,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4d11bba5eae58fcb02,FALSE,FALSE,Thu May 22 2025 14:57:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-storage-account,inefficient-configuration,storage,"<p id="""">  Azure users may enable the SFTP feature on Storage Accounts during migration tests, integration scenarios, or experimentation. However, if left enabled after initial use, the feature continues to generate flat hourly charges — even when no SFTP traffic occurs.</p><p id="""">Because this fee is incurred silently and independently of storage usage, it often goes unnoticed in cost reviews. When SFTP is not actively used for data ingestion or export, disabling it can eliminate unnecessary charges without impacting other access methods.</p>","<ul id=""""><li id="""">SFTP support on Azure Storage Accounts is billed at a <strong id="""">flat hourly rate</strong>, regardless of actual usage</li><li id="""">Pricing is approximately <strong id="""">$0.30/hour per account</strong> (\~$219/month)</li><li id="""">Charges begin as soon as SFTP is enabled and continue until explicitly disabled</li><li id="""">No data transfer or connection usage is required for the fee to apply</li></ul>","<ul id=""""><li id="""">Identify Storage Accounts with the SFTP feature enabled</li><li id="""">Check activity logs or metrics to determine whether SFTP sessions are active or recent</li><li id="""">Verify with application owners whether SFTP is still in use or required</li><li id="""">Flag accounts with enabled SFTP and no recent activity as candidates for review</li></ul>","<ul id=""""><li id="""">Disable the SFTP feature on any Storage Account where it is no longer needed</li><li id="""">Coordinate with owners to confirm that alternate access methods (e.g., HTTPS, SDK) are sufficient</li><li id="""">Consider including SFTP enablement in governance reviews to catch idle services before they accumulate charges</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/secure-file-transfer-protocol-support"" id="""">Enable SFTP support for Azure Blob Storage</a></li><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/storage/blobs/"" id="""">Azure Storage Pricing – SFTP</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Storage-2606</p>
Stale Dedicated Hosts for Stopped EC2 Mac Instances,stale-dedicated-hosts-for-stopped-ec2-mac-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941b24d97aecbaab586b,FALSE,FALSE,Sun Jun 22 2025 23:39:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Igor Bareev,aws,aws-ec2,orphaned-resource,compute,"<p id="""">When an EC2 Mac instance is stopped or terminated, its associated dedicated host remains allocated by default. Because Mac instances are the only EC2 type billed at the host level, charges continue to accrue as long as the host is retained. This can lead to significant waste when:  * Instances are stopped but the host is never released   * Hosts are retained unintentionally after workloads are migrated or decommissioned   * Automation only terminates instances without deallocating hosts</p>","<p id="""">Billing is based on the <strong id="""">dedicated host reservation</strong>, not instance runtime. Each EC2 Mac instance requires a dedicated Mac host (mac1 or mac2), which is billed per hour from allocation time — regardless of whether the instance is running, stopped, or terminated — unless the host is explicitly released.</p>","<ul id=""""><li id="""">Review whether any EC2 Mac dedicated hosts remain allocated without a running instance</li><li id="""">Check for host reservations persisting beyond expected lifespans (e.g., post-project teardown)</li><li id="""">Identify cost entries for mac1/mac2 host usage with no active workloads associated</li><li id="""">Evaluate if the use of Dedicated Host auto-release is disabled or misconfigured</li></ul>","<ul id=""""><li id="""">Release Mac dedicated hosts after stopping or terminating Mac EC2 instances if no longer needed</li><li id="""">Update automation workflows to include host deallocation logic when applicable</li><li id="""">Enable lifecycle tracking of dedicated hosts to avoid forgotten allocations</li><li id="""">Where appropriate, consolidate Mac workloads to minimize the number of hosts in use</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.html"" id="""">EC2 Mac Instances – Pricing and Host Behavior</a><br><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/releasing-dedicated-hosts.html"" id="""">Release Dedicated Hosts</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-8679</p>
Suboptimal AppStream Fleet Auto Scaling Policies,suboptimal-appstream-fleet-auto-scaling-policies,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894150331c6da9b0e7239,FALSE,FALSE,Sun Jun 22 2025 23:39:01 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Jason DiDomenico,aws,aws-appstream-2-0,inefficient-configuration,compute,"<p id="""">When fleet auto scaling policies maintain more active instances than are required to support current usage—particularly during off-peak hours—organizations incur unnecessary compute costs. Fleets often remain oversized due to conservative default configurations or lack of schedule-based scaling. Tuning the scaling policies to better reflect usage patterns ensures that streaming infrastructure aligns with actual demand.</p>","<p id="""">AppStream streaming instances are billed by the hour based on instance type and the number of running instances, regardless of whether those instances are being actively used. Minimum instance counts and scheduled provisioning settings directly influence total cost.</p>","<ul id=""""><li id="""">Identify AppStream fleets where the minimum instance count consistently exceeds active session demand over a representative period</li><li id="""">Review fleet usage patterns and idle capacity during off-peak hours or low-concurrency windows</li><li id="""">Check whether auto scaling policies are based on real usage or static assumptions</li><li id="""">Assess whether scheduled scaling is configured to adjust fleet size based on predictable usage cycles</li></ul>","<ul id=""""><li id="""">Adjust minimum instance counts in auto scaling policies to reflect observed demand</li><li id="""">Implement schedule-based scaling to reduce instance counts during predictable low-usage periods and increase during peak hours</li><li id="""">Regularly review and update scaling policies based on current usage data to ensure ongoing efficiency</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/appstream2/latest/developerguide/fleet-auto-scaling.html"" id="""">AppStream 2.0 Fleet Auto Scaling</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-5906</p>
Suboptimal Architecture Configuration for Lambda Functions,suboptimal-architecture-configuration-for-lambda-functions,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b484ef9030f968eba36,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Shireen Maini,aws,aws-lambda,suboptimal-configuration,compute,"<p id="""">While many AWS customers have migrated EC2 workloads to Graviton to reduce costs, Lambda functions often remain on the default x86 architecture. AWS Graviton2 (ARM) offers lower pricing and equal or better performance for most supported runtimes — yet adoption remains uneven due to legacy defaults or lack of awareness. Continuing to run eligible Lambda functions on x86 leads to unnecessary spending. The migration requires minimal configuration changes and can be verified through benchmarking and workload testing.</p>","<p id="""">Lambda is billed based on:</p><ul id=""""><li id="""">Number of invocations</li><li id="""">Duration (per millisecond) multiplied by memory configured</li><li id="""">Architecture — Graviton functions are ~20% cheaper than x86 equivalents</li></ul><p id="""">If a function continues to use x86 when it could run on ARM, the result is higher cost for equivalent workload performance.</p>","<ul id=""""><li id="""">Review Lambda function configurations to identify functions using x86_64 architecture</li><li id="""">Cross-reference with supported runtimes to determine Graviton compatibility</li><li id="""">Use AWS Lambda Power Tuning or performance benchmarking to validate migration feasibility</li><li id="""">Look for high-frequency functions with stable workloads that would benefit most from architecture optimization</li></ul>","<ul id=""""><li id="""">Update Lambda function configurations to use ARM/Graviton2 where compatible</li><li id="""">Benchmark function performance and duration to validate equal or improved performance</li><li id="""">For stable, high-throughput functions, consider pairing architecture changes with Compute Savings Plans</li><li id="""">Incorporate architecture selection into CI/CD pipelines to ensure consistency going forward</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/foundation-arch.html"" id="""">Choosing Lambda Function Architecture</a></li><li id=""""><a href=""https://aws.amazon.com/lambda/pricing/"" id="""">AWS Lambda Pricing</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-3004</p>
Suboptimal Architecture Selection for Azure Virtual Machines,suboptimal-architecture-selection-for-azure-virtual-machines,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fd04424a234e6c6ddb,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Jurian van Hoorn,azure,azure-virtual-machines,suboptimal-pricing-model,compute,"<p id="""">Azure provides VM families across three major CPU architectures, but default provisioning often leans toward Intel-based SKUs due to inertia or pre-configured templates. AMD and ARM alternatives offer substantial cost savings; ARM in particular can be 30–50% cheaper for general-purpose workloads. These price differences accumulate quickly at scale.</p><p id="""">ARM-based VMs in Azure (e.g., Dps_v5, Eps_v5) are suited for many common workloads, such as microservices, web applications, and containerized environments. However, not all applications are architecture-compatible, especially those with dependencies on x86-specific libraries or instruction sets. Organizations that skip architecture evaluation during provisioning miss out on cost-efficient options.</p>","<p id="""">Azure VMs are billed based on the underlying hardware architecture, instance size, and region. ARM-based VMs, identified by a ""p"" in the SKU name (e.g., D4ps_v5), offer significantly lower pricing than comparable x86-based instances. AMD-based SKUs (e.g., E4as_v5) are typically priced lower than Intel-based equivalents. However, Azure does not automatically guide architecture selection, so users often default to Intel SKUs out of habit or compatibility assumptions. This can lead to missed savings if workloads are architecture-flexible.</p>","<ul id=""""><li id="""">Run Azure Resource Graph queries to inventory current VM SKUs by architecture</li></ul>","<ul id=""""><li id="""">Assess workload compatibility with ARM or AMD architectures</li><li id="""">Propose migration to ARM-based SKUs for supported workloads to reduce compute costs</li><li id="""">Use AMD-based instances as an intermediate option when ARM compatibility is not feasible</li><li id="""">Incorporate architecture evaluation into provisioning workflows and cost governance reviews</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/compare-vm-skus"" id="""">https://learn.microsoft.com/en-us/azure/virtual-machines/compare-vm-skus</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/linux/arm-64"" id="""">https://learn.microsoft.com/en-us/azure/virtual-machines/linux/arm-64</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions#naming-convention-explanation"" id="""">https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions#naming-convention-explanation</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Compute-3629</p>
Suboptimal Architecture Selection in AWS Fargate,suboptimal-architecture-selection-in-aws-fargate,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58dc66ff97ebe3866b4,FALSE,FALSE,Mon Nov 03 2025 16:17:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Kevin Talbierz,aws,aws-fargate,suboptimal-architecture-selection,compute,"<p id="""">AWS Fargate supports both x86 and Graviton2 (ARM64) CPU architectures, but by default, many workloads continue to run on x86. Graviton2 delivers significantly better price-performance, especially for stateless, scale-out container workloads. Teams that fail to configure task definitions with the `ARM64` architecture miss out on meaningful efficiency gains. Because this setting is not enabled automatically and is often overlooked, it results in higher compute costs for functionally equivalent workloads.</p>","<p id="""">Fargate charges are based on the requested vCPU and memory resources for the duration of containerized workloads. The choice of CPU architecture (x86 vs. ARM) directly impacts pricing. Graviton2 (ARM64) offers up to 40% better price-performance compared to x86 for many workloads but must be explicitly specified.</p>","<ul id=""""><li id="""">Review ECS task definitions or EKS pod specs for Fargate workloads to determine if the `ARM64` architecture is specified</li><li id="""">Identify workloads where Graviton2 would be compatible (e.g., stateless services, compiled for ARM)</li><li id="""">Evaluate cost differences between current architecture and potential Graviton2 performance improvements</li><li id="""">Confirm that container images are compatible with ARM-based environments</li><li id="""">Ensure no external dependencies or libraries require x86</li></ul>","<ul id=""""><li id="""">Update Fargate task definitions to use `""runtimePlatform"": { ""cpuArchitecture"": ""ARM64"" }` where supported</li><li id="""">Rebuild container images for multi-architecture compatibility if needed</li><li id="""">Benchmark ARM-based performance to validate expected savings</li><li id="""">Standardize Graviton2 usage for eligible workloads via IaC templates or CI/CD pipelines</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/blogs/aws/announcing-aws-graviton2-support-for-aws-fargate-get-up-to-40-better-price-performance-for-your-serverless-containers/?utm_source=chatgpt.com"" id="""">https://aws.amazon.com/blogs/aws/announcing-aws-graviton2-support-for-aws-fargate-get-up-to-40-better-price-performance-for-your-serverless-containers/?utm_source=chatgpt.com</a></li></ul>",FALSE,FALSE,,N/A,,<p>AWS-Compute-6743</p>
Suboptimal Architecture Selection in AWS Lambda,suboptimal-architecture-selection-in-aws-lambda,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941b27147d2d66174017,FALSE,FALSE,Sun Jun 22 2025 23:39:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Andrew Shieh,aws,aws-lambda,suboptimal-configuration,compute,"<p id="""">Lambda functions default to the x86\_64 architecture, which is more expensive than Arm64. For many workloads, especially those written in interpreted languages (e.g., Python, Node.js) or compiled to architecture-neutral bytecode (e.g., Java), there is no dependency on x86-specific binaries. In such cases, moving to Arm64 can reduce compute costs by 20% without impacting functionality.  Despite this, many teams continue to run Lambda functions on x86\_64 due to legacy configurations, inertia, or lack of awareness. This leads to avoidable spending, particularly at scale or in high-volume environments.</p>","<p id="""">Lambda is billed based on:  * Number of requests   * Duration of execution (in milliseconds)   * Memory allocated   * CPU architecture (Arm64 functions cost <strong id="""">up to 20% less</strong> than x86\_64)</p>","<ul id=""""><li id="""">Architecture is set to <strong id="""">x86\_64</strong></li><li id="""">Function code does not rely on architecture-specific binaries or compiled dependencies</li><li id="""">Runtime is compatible with Arm64 (e.g., Python, Node.js, Java)</li><li id="""">High aggregate usage of “GB-seconds” across x86 functions contributes significantly to Lambda costs</li><li id="""">No benchmarking or cost comparison has been performed between x86 and Arm</li></ul>","<ul id=""""><li id="""">Benchmark representative functions on both architectures to validate performance and compatibility</li><li id="""">For functions using architecture-neutral runtimes or dependencies, migrate to <strong id="""">Arm64</strong> via configuration update</li><li id="""">Ensure CI/CD pipelines and IaC templates default to <strong id="""">Arm64</strong> for new functions</li><li id="""">Educate development teams on architecture implications and expected savings</li><li id="""">Use tagging or cost attribution to track savings post-migration</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/lambda/latest/dg/foundation-arch.html"" id="""">https://docs.aws.amazon.com/lambda/latest/dg/foundation-arch.html</a><br><a href=""https://aws.amazon.com/blogs/compute/using-arm64-for-aws-lambda-functions/"" id="""">https://aws.amazon.com/blogs/compute/using-arm64-for-aws-lambda-functions/</a><br><a href=""https://aws.amazon.com/lambda/pricing/"" id="""">https://aws.amazon.com/lambda/pricing/</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-8052</p>
Suboptimal Azure OpenAI Model Type,suboptimal-azure-openai-model-type-c8913,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e0caa277b14eb0eb6e,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:12 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Ariel Lichterman,azure,azure-cognitive-services,outdated-model-selection,ai,"<p id="""">Azure releases newer OpenAI models that provide better performance and cost characteristics compared to older generations. When workloads remain on outdated model versions, they may consume more tokens to produce equivalent output, run slower, or miss out on quality improvements. Because customers pay per token, using an older model can lead to unnecessary spending and reduced value. Aligning deployments to the most current, efficient model types helps reduce spend and improve application performance.</p>","<p id="""">On-demand Azure OpenAI deployments are billed per input and output token. Newer models often offer lower cost per processed token, higher throughput, and reduced latency. Continuing to run older models can increase token usage and degrade cost efficiency.</p>","<ul id=""""><li id="""">Review Azure OpenAI deployments to identify workloads using older or deprecated model versions</li><li id="""">Assess token consumption patterns to determine whether newer models could achieve the same results more efficiently</li><li id="""">Evaluate latency or performance issues that may be linked to older model behavior</li><li id="""">Check Azure’s model lifecycle and release notes to confirm whether a newer recommended model family exists</li></ul>","<ul id=""""><li id="""">Migrate workloads to the latest suitable Azure OpenAI model that provides improved efficiency and performance</li><li id="""">Establish a periodic review process to ensure deployed models are aligned with current Azure model offerings</li><li id="""">Incorporate model lifecycle awareness into architecture standards so workloads are upgraded as new versions become available</li><li id="""">Validate compatibility and output quality after migration to ensure a smooth transition to newer models</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models"" id="""">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models</a></li></ul>",FALSE,FALSE,,189,,Azure-AI-4754
Suboptimal Bedrock Custom Model,suboptimal-bedrock-custom-model-fbbe4,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e2904413aab595bb8c,FALSE,FALSE,Sun Dec 14 2025 09:18:26 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:35 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,aws,aws-bedrock,outdated-or-overpowered-model-configuration,ai,"<p id="""">Teams often start custom-model deployments with large architectures, full-precision weights, or older model versions carried over from training environments. When these models transition to Bedrock’s managed inference environment, the compute footprint (especially GPU class) becomes a major cost driver. Common inefficiencies include: * Deploying outdated custom models despite newer, more efficient variants being available, * Running full-size models for tasks that could be served by distilled or quantized versions, * Using accelerators overpowered for the workload’s latency requirements, or * Relying on default model artifacts instead of optimizing for inference. Because Bedrock Custom Models bill continuously for the backing compute, even small inefficiencies in model design or versioning translate into substantial ongoing cost.</p>","<p id="""">Bedrock Custom Models incur hourly charges for the underlying dedicated compute (e.g., GPU accelerator instance types). Model size, architecture, and precision level directly influence resource requirements. Using an unnecessarily large or outdated model increases hourly cost without improving output.</p>","<ul id=""""><li id="""">Identify custom models deployed using older or unoptimized architectures</li><li id="""">Review GPU or compute class selected for the endpoint relative to actual latency needs</li><li id="""">Assess whether distilled, quantized, or smaller model variants could deliver similar output quality</li><li id="""">Evaluate model performance to determine whether compute utilization is consistently low</li><li id="""">Check whether model version governance practices exist for custom inference workloads</li></ul>","<ul id=""""><li id="""">Upgrade to newer, more efficient versions of custom models when available</li><li id="""">Use model distillation, pruning, or quantization to reduce compute requirements</li><li id="""">Select smaller architectures for workloads with light or predictable inference needs</li><li id="""">Right-size the underlying accelerator class to match actual latency and throughput requirements</li><li id="""">Establish periodic model review processes so custom model endpoints remain optimized over time</li></ul>",,FALSE,FALSE,,,,AWS-AI-1382
Suboptimal Bedrock Inference Profile Model,suboptimal-bedrock-inference-profile-model-08c00,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e0002f767b06be92e5,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:14 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Ariel Lichterman,aws,aws-bedrock,outdated-model-selection,ai,"<p id="""">AWS frequently updates Bedrock with improved foundation models, offering higher quality and better cost efficiency. When workloads remain tied to older model versions, token consumption may increase, latency may be higher, and output quality may be lower. Using outdated models leads to avoidable operational costs, particularly for applications with consistent or high-volume inference activity. Regular modernization ensures applications take advantage of new model optimizations and pricing improvements.</p>","<p id="""">Bedrock Inference Profiles are billed based on model-specific rates per input and output token (or per request, depending on the model). Newer model versions often provide improved performance, lower per-token cost, or more efficient inference compared to older versions. Continuing to use outdated models can increase total cost for the same workload output.</p>","<ul id=""""><li id="""">Review Bedrock Inference Profiles to identify deployments using older or deprecated model versions</li><li id="""">Assess token usage trends to determine whether newer models could reduce cost-per-token for similar workloads</li><li id="""">Evaluate latency, performance, or quality issues that may be associated with older model versions</li><li id="""">Check AWS documentation for updated model recommendations or improved successor models</li></ul>","<ul id=""""><li id="""">Migrate workloads to the most recent Bedrock model version that meets performance and cost goals</li><li id="""">Implement periodic review processes to ensure model selection stays aligned with AWS’s latest model offerings</li><li id="""">Incorporate model lifecycle awareness into architecture standards so workloads modernize as new versions become available</li><li id="""">Validate application behavior and accuracy after transitioning to an updated model</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models.html"" id="""">https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models.html</a></li></ul>",FALSE,FALSE,,,,AWS-AI-1937
Suboptimal Bedrock Model Type,suboptimal-bedrock-model-type-61442,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e177a07682808bcc74,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:21 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,aws,aws-bedrock,outdated-model-selection,ai,"<p id="""">Bedrock’s model catalog evolves quickly as providers release new versions—such as successive Claude model families or updated Amazon Titan models. These newer models frequently offer improved performance, more efficient reasoning, better context handling, and higher-quality outputs compared to older generations. When workloads continue using older or deprecated models, they may require **more tokens**, experience **slower inference**, or miss out on accuracy improvements available in successor models. Because Bedrock bills per token or per inference unit, these inefficiencies can increase cost without adding value. Ensuring workloads align with the most suitable current-generation model improves both performance and cost-effectiveness.</p>","<p id="""">Bedrock generally charges per input and output token (or per inference unit for certain model families). While newer models may not always have a lower price per token, they often deliver **better accuracy, faster responses, or reduced token requirements**, improving the effective cost-efficiency of the workload. Continuing to run older models can lead to higher spend for lower-quality output.</p>","<ul id=""""><li id="""">Identify Bedrock workloads still using older, legacy, or deprecated model versions</li><li id="""">Assess whether similar results could be achieved with fewer tokens using newer model families</li><li id="""">Evaluate latency, throughput, or output-quality gaps that may indicate inefficiency in older models</li><li id="""">Review Bedrock’s model catalog, lifecycle status, or provider guidance to confirm if a recommended successor model exists</li></ul>","<ul id=""""><li id="""">Migrate workloads to the most current Bedrock model family that provides improved efficiency, accuracy, or throughput</li><li id="""">Introduce periodic model reviews into platform or architecture governance to ensure ongoing modernization</li><li id="""">Embed model lifecycle awareness into deployment processes so outdated models are identified and upgraded proactively</li><li id="""">Validate functional compatibility and output quality after migration to ensure a smooth transition</li></ul>",,FALSE,FALSE,,,,AWS-AI-4616
Suboptimal Cache Usage for Repetitive Azure OpenAI Workloads,suboptimal-cache-usage-for-repetitive-azure-openai-workloads-334a9,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e071742b27f385a54d,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:19 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,azure,azure-cognitive-services,missing-caching-layer,ai,"<p id="""">A large share of production AI workloads include repetitive or static requests—such as classification labels, routing decisions, FAQ responses, metadata extraction, or deterministic prompt templates. Without a caching layer, every repeated request is sent to the model, incurring full token charges and increasing latency. Azure OpenAI does not provide native caching, so teams must implement caching at the application or API gateway layer. When caching is absent, workloads repeatedly spend tokens for identical outputs, creating avoidable cost. This inefficiency often arises when teams optimize only for correctness—not cost—and default to calling the model for every invocation regardless of whether the response is predictable.</p>","<p id="""">Azure OpenAI on-demand usage is billed per input and output token. Re-running identical prompts consumes tokens unnecessarily when responses could be served from a cache. For workloads with repetitive queries, caching can reduce both cost and latency significantly.</p>","<ul id=""""><li id="""">Identify workloads that submit identical or highly similar prompts repeatedly</li><li id="""">Review token usage patterns showing recurring spikes from deterministic or static queries</li><li id="""">Examine application logs for repeated inference calls that produce identical responses</li><li id="""">Assess whether any caching layer (API, CDN, application-level, vector cache) is implemented for repetitive workloads</li></ul>","<ul id=""""><li id="""">Implement an application-level or gateway-level caching layer for deterministic or repetitive inference requests</li><li id="""">Cache outputs for classification, routing, metadata extraction, or FAQ-style responses</li><li id="""">Define cache TTLs appropriate to the workload’s freshness requirements</li><li id="""">Use normalized prompt signatures (hashing) to increase cache hit rates</li><li id="""">Periodically review workload patterns to expand caching coverage as use cases evolve</li></ul>",,FALSE,FALSE,,,,Azure-AI-2825
Suboptimal Cache Usage for Repetitive Bedrock Inference Workloads,suboptimal-cache-usage-for-repetitive-bedrock-inference-workloads-5e314,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e0da9245bab5a3069b,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:20 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,aws,aws-bedrock,missing-caching-layer,ai,"<p id="""">Bedrock workloads commonly include repetitive inference patterns—such as classification results, prompt templates generating deterministic outputs, FAQ responses, document tagging, and other predictable or low-variability tasks. Without a caching strategy (API-layer cache, application cache, or hash-based prompt cache), these workloads repeatedly invoke the model and incur token costs for answers that do not change. Because Bedrock does not offer native inference caching, customers must implement caching externally. When no cache layer exists, cost increases linearly with repeated calls, even though responses remain constant. This issue appears most often when teams treat all workloads as dynamic or generative, rather than separating deterministic tasks from open-ended ones.</p>","<p id="""">Bedrock charges for tokens (or inference units) per request. Repeatedly invoking a model with identical or highly similar prompts generates full cost each time. Caching can eliminate unnecessary calls and reduce both cost and latency.</p>","<ul id=""""><li id="""">Identify workloads where prompts are identical or follow a deterministic structure that produces repeatable outputs</li><li id="""">Review Bedrock invocation logs to find repeated calls with similar inputs and identical outputs</li><li id="""">Assess token usage patterns for workloads that handle classification, routing, summarization of static content, or metadata extraction</li><li id="""">Verify whether any application-level or API-layer caching mechanism is implemented for repetitive tasks</li></ul>","<ul id=""""><li id="""">Introduce an application-level cache or gateway cache for deterministic and repetitive inference workloads</li><li id="""">Cache outputs for classification, routing logic, FAQs, structured extraction, or static summarization tasks</li><li id="""">Use input hashing or canonicalized prompt signatures to ensure high cache hit rates</li><li id="""">Define TTL policies aligned with business requirements to maintain accuracy while minimizing cost</li><li id="""">Regularly evaluate workload patterns to identify additional caching opportunities as usage evolves</li></ul>",,FALSE,FALSE,,,,AWS-AI-7300
Suboptimal Cache Usage for Repetitive Inference,suboptimal-cache-usage-for-repetitive-inference-f6051,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e1babd04e5225d6975,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,gcp,gcp-vertex-ai,missing-caching-layer,ai,"<p id="""">A large portion of real-world AI workloads involve repetitive or deterministic inference patterns—such as classification labels, routing logic, metadata extraction, FAQ responses, keyword detection, or summarization of static content. Vertex AI does **not** provide native inference caching, so applications that repeatedly send identical prompts to the model incur avoidable cost. When no caching mechanism is implemented, workloads repeatedly invoke the model and consume tokens even though the output is predictable. Over time, especially at scale, these repetitive token charges accumulate into significant waste. This inefficiency is common in early-stage deployments where teams optimize for correctness rather than cost.</p>","<p id="""">Generative AI workloads are billed per input and output token. Without a caching layer, repeated requests for deterministic or low-variability tasks incur full token charges for every call, increasing cost and latency unnecessarily.</p>","<ul id=""""><li id="""">Identify workloads issuing identical or highly similar prompts that produce repeatable outputs</li><li id="""">Review inference logs for repeated requests with deterministic behavior</li><li id="""">Analyze token consumption patterns for tasks like routing, extraction, classification, or FAQ responses</li><li id="""">Evaluate whether any caching mechanism is implemented at the API gateway, service layer, or application layer</li></ul>","<ul id=""""><li id="""">Implement an application-level or gateway-level caching layer for deterministic or repetitive inference workloads</li><li id="""">Cache outputs for classification, routing, structured extraction, and FAQ-style responses</li><li id="""">Normalize or hash prompt inputs to improve cache hit rates</li><li id="""">Define cache TTLs based on data freshness requirements</li><li id="""">Periodically re-evaluate caching coverage as workload patterns evolve</li></ul>",,FALSE,FALSE,,,,GCP-AI-9521
Suboptimal Configuration of a CloudFront Distribution,suboptimal-configuration-of-a-cloudfront-distribution,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b43d521f398e9e193bc,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-cloudfront,inefficient-configuration,networking,"<p id="""">This inefficiency occurs when compression is either disabled or not functioning effectively on a CloudFront distribution. Static assets such as text, JSON, JavaScript, and CSS files are compressible and benefit significantly from compression. Without compression, CloudFront transfers larger objects, leading to increased data transfer charges and slower delivery performance—without improving user experience.</p>","<p id="""">CloudFront is billed based on the volume of data transferred and the number of HTTP/HTTPS requests. Compression reduces the size of data sent to users, which directly lowers transfer volume and speeds up delivery. When compression is disabled or misconfigured, larger payloads are sent, increasing bandwidth usage and data transfer costs.</p>","<ul id=""""><li id="""">Identify CloudFront distributions with compression disabled</li><li id="""">Check whether compressible MIME types (e.g., text/html, application/javascript) are being served uncompressed</li><li id="""">Review cache behavior settings to ensure compression is enabled for eligible file types</li><li id="""">Validate that origin responses include compressible content and allow compression</li><li id="""">Confirm whether compression is effectively applied at the edge by inspecting actual responses</li></ul>","<ul id=""""><li id="""">Enable compression for all applicable content types in CloudFront settings</li><li id="""">Review and adjust cache behaviors to ensure compression is applied at the edge</li><li id="""">Coordinate with origin services to ensure headers support compression (e.g., avoid disabling with restrictive cache-control headers)</li><li id="""">Test end-to-end to confirm that content is being served compressed under real-world scenarios</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/ServingCompressedFiles.html\&quot;"" id="""">Serving Compressed Files with CloudFront</a></li><li id=""""><a href=""https://aws.amazon.com/cloudfront/pricing/\"" id="""">CloudFront Pricing</a></li></ul>",FALSE,FALSE,,2,,<p>AWS-Networking-3832</p>
Suboptimal Cross-AZ Routing to NAT Gateway,suboptimal-cross-az-routing-to-nat-gateway,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4cd757428dc24f13f0,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Mike Graff,aws,aws-nat-gateway,inefficient-configuration,networking,"<p id="""">  NAT Gateways are designed to serve private subnets within the same Availability Zone. When subnets in one AZ are configured to route traffic through a NAT Gateway in a different AZ, the traffic crosses AZ boundaries and incurs inter-AZ data transfer charges in addition to the standard NAT processing fees.</p><p id="""">This typically happens when:</p><p id="""">* NAT Gateways are deployed in multiple AZs (as recommended for resilience), but  * Route tables for all subnets are configured to send traffic to a single NAT Gateway, ignoring AZ placement</p><p id="""">In high-throughput environments, this misalignment silently generates excess cost. Ensuring that each subnet routes through the NAT Gateway in its own AZ avoids inter-AZ charges and aligns with AWS architectural best practices.</p>","<ul id=""""><li id="""">NAT Gateway costs include:</li><li id="""">A flat hourly charge per gateway</li><li id="""">A per-GB charge for all data processed</li><li id="""">If traffic crosses Availability Zones (e.g., from a subnet in AZ-A to a NAT Gateway in AZ-B), <strong id="""">inter-AZ transfer charges</strong> apply</li><li id="""">These are billed separately from NAT Gateway processing fees and increase total data egress cost</li></ul>","<ul id=""""><li id="""">Identify whether multiple NAT Gateways exist across different Availability Zones</li><li id="""">Review route table configurations to determine if subnets route to a NAT Gateway in a different AZ</li><li id="""">Assess whether route tables are shared or duplicated across subnets without AZ awareness</li><li id="""">Analyze inter-AZ traffic patterns associated with NAT Gateway usage to confirm misrouted flows</li></ul>","<ul id=""""><li id="""">Update route tables to ensure that each subnet routes outbound traffic through the NAT Gateway in the same AZ</li><li id="""">Ensure one NAT Gateway is deployed per Availability Zone for fault tolerance and cost efficiency</li><li id="""">Review and revise any infrastructure templates or automation that create non-AZ-aware routing</li><li id="""">Incorporate AZ-based routing validation into network hygiene checks</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/vpc/pricing/"" id="""">AWS NAT Gateway Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"" id="""">Best Practices for NAT Gateways</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Networking-9673</p>
Suboptimal ElastiCache Engine Selection,suboptimal-elasticache-engine-selection,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b49a7a888da77b080ac,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Yulia Perlis,aws,aws-elasticache,suboptimal-configuration,databases,"<p id="""">Many workloads default to using Redis or Memcached without evaluating whether a lighter or more efficient engine would provide equivalent functionality at lower cost. Valkey is a Redis-compatible, open-source engine supported by ElastiCache that may offer improved price-performance and licensing benefits. For read-heavy or stateless workloads that don’t require Redis-specific features (e.g., persistence, advanced replication), Valkey can often be used as a drop-in replacement. Memcached, while simple, lacks key features like replication and persistence, and may be less cost-effective for certain access patterns. Choosing the wrong engine can result in overpaying for capabilities that aren’t needed — or missing opportunities to optimize.</p>","<p id="""">ElastiCache is billed per instance-hour based on instance type and engine. While pricing is nominally similar across engines, performance and resource efficiency may vary significantly depending on workload and engine selection.</p>","<ul id=""""><li id="""">Review the engine type for each ElastiCache cluster (Redis, Memcached, Valkey)</li><li id="""">Identify Redis clusters that don’t use features like persistence, pub/sub, or complex data types</li><li id="""">Evaluate whether Valkey would be a compatible and more efficient replacement</li><li id="""">Assess historical reasons for engine choice (e.g., legacy defaults vs. actual workload needs)</li></ul>","<ul id=""""><li id="""">For Redis-compatible but non-persistent workloads, consider migrating to Valkey</li><li id="""">If using Memcached, reevaluate whether Redis or Valkey offers better price-performance</li><li id="""">Note that engine migration typically requires cluster recreation and data migration — plan accordingly</li><li id="""">Incorporate engine selection as part of architecture reviews for new cache deployments</li></ul>",,FALSE,TRUE,,,,<p>AWS-Databases-4833</p>
Suboptimal Engine Selection in MemoryDB,suboptimal-engine-selection-in-memorydb,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589416df82bf10739313ab,FALSE,FALSE,Sun Jun 22 2025 23:39:02 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Erik Marke,aws,aws-memorydb,inefficient-configuration,databases,"<p id="""">MemoryDB now supports Valkey, a drop-in replacement for Redis OSS offering significant cost and performance advantages. However, many deployments still default to Redis OSS, incurring higher hourly costs and unnecessary data write charges. For compatible workloads, continuing to use Redis OSS instead of Valkey represents a missed opportunity for savings and modernization.</p>","<p id="""">MemoryDB charges based on provisioned instance hours, with costs varying by engine type. Valkey instances offer \~30% lower hourly rates and no data write charges up to 10 TB/month. Selecting Redis OSS instead of Valkey can lead to unnecessarily high compute and storage costs.</p>","<ul id=""""><li id="""">Identify MemoryDB clusters running the Redis OSS engine rather than Valkey</li><li id="""">Review the MemoryDB console or cost reporting data for engine metadata</li><li id="""">Assess whether workloads are compatible with Valkey (which is Redis API-compatible)</li></ul>","<ul id=""""><li id="""">Migrate eligible MemoryDB clusters to use the Valkey engine</li><li id="""">Validate API compatibility and performance requirements prior to migration</li><li id="""">Use zero-downtime upgrade capabilities where possible for seamless transition</li><li id="""">Consider making Valkey the default engine for new MemoryDB deployments</li></ul>","<p id=""""><a href=""https://aws.amazon.com/blogs/database/amazon-elasticache-and-amazon-memorydb-announce-support-for-valkey/"" id="""">Amazon MemoryDB for Valkey announcement</a><br><a href=""https://aws.amazon.com/memorydb/pricing/"" id="""">Amazon MemoryDB pricing</a><br><a href=""https://github.com/valkey-io/valkey"" id="""">Valkey GitHub project</a></p>",FALSE,FALSE,,,,<p>AWS-Databases-8704</p>
Suboptimal Integration Runtime Region Selection in Azure Data Factory,suboptimal-integration-runtime-region-selection-in-azure-data-factory-0bcad,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80dfec705886a19c1ad7,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:11 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Jurian van Hoorn,azure,azure-data-factory-v2,cross-region-data-movement,compute,"<p id="""">When Integration Runtimes are configured with the default “Auto Resolve” region setting, Azure may automatically provision them in a region different from the data sources or sinks. For example, an environment deployed in West Europe may run pipelines in US East. This causes unnecessary cross-region data transfer, increasing networking costs and pipeline latency. The inefficiency often goes unnoticed because data transfer costs are billed separately from pipeline compute charges.</p>","<p id="""">ADF billing includes charges for pipeline activity runs, data movement, and Integration Runtime compute. Data transferred between Azure regions incurs additional network egress costs, which can exceed compute costs when workloads are large or frequent.</p>","<ul id=""""><li id="""">Review Integration Runtime configurations and confirm their assigned regions</li><li id="""">Identify data movements that cross Azure region boundaries (e.g., West Europe → US East)</li><li id="""">Correlate pipeline-level data movement costs with Integration Runtime region assignments</li><li id="""">Assess whether “Auto Resolve” settings are active in environments that have regionally constrained data sources</li></ul>","<ul id=""""><li id="""">Explicitly configure Integration Runtimes to run in the same region as the data sources and targets</li><li id="""">Avoid the “Auto Resolve” setting unless cross-region data processing is intentional</li><li id="""">Establish governance to enforce region alignment across ADF environments</li><li id="""">Regularly review Data Factory cost breakdowns to ensure network charges align with expected data movement patterns</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" id="""">Integration Runtime Overview</a></li></ul>",FALSE,FALSE,,,,Azure-Compute-5332
Suboptimal Lifecycle Policy for Small Files on an S3 Bucket,suboptimal-lifecycle-policy-for-small-files-on-an-s3-bucket,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b45daf3a4a5cf70d81d,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-s3,inefficient-configuration,storage,"<p id="""">This inefficiency occurs when small files are stored in S3 storage classes that impose a minimum object size charge, resulting in unnecessary costs. Small files under 128 KB stored in Glacier Instant Retrieval, Standard-IA, or One Zone-IA are billed as if they were 128 KB. If these small files are accessed frequently, S3 Standard may be a better fit. For infrequently accessed small files, transitioning them to archival storage classes like Glacier Flexible Retrieval or Deep Archive can optimize storage spend. Poorly tuned lifecycle policies often allow small files to remain in suboptimal storage classes indefinitely.</p>","<p id="""">S3 storage is billed per GB stored per month, but for certain storage classes like Glacier Instant Retrieval, Standard-IA, and One Zone-IA, a minimum object size charge of 128 KB applies. Even if an object is smaller than 128 KB, billing treats it as if it were 128 KB. This means small files stored in these classes may incur disproportionately high costs compared to larger files. Choosing the right storage class is crucial to avoid unnecessary charges.</p>","<ul id=""""><li id="""">Review if the bucket contains a high proportion of small objects (e.g., under 128 KB)</li><li id="""">Evaluate whether these small objects are stored in Glacier Instant Retrieval, Standard-IA, or One Zone-IA storage classes</li><li id="""">Assess the access patterns of the small files to determine if frequent retrieval justifies Standard storage</li><li id="""">Check if lifecycle policies exist to transition small, infrequently accessed files to lower-cost archival classes</li><li id="""">Confirm whether the current storage configuration aligns with cost optimization goals for object size and access frequency</li></ul>","<p id="""">Move frequently accessed small objects to S3 Standard to avoid minimum size penalties. Transition infrequently accessed small objects to S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive through updated lifecycle policies. Implement automated lifecycle rules based on object size and last access time to proactively manage small object storage. Periodically audit buckets for small object distribution and adjust policies as data patterns change over time.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\&quot;"" id="""">Amazon S3 Storage Class Overview</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-glacier-storage-classes.html\"" id="""">Using Glacier Storage Classes</a></li></ul>",FALSE,FALSE,,19,,<p>AWS-Storage-2907</p>
Suboptimal Load Balancer Rule Configuration in Azure Standard Load Balancer,suboptimal-load-balancer-rule-configuration-in-azure-standard-load-balancer,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fc517b9049a23cf6ae,FALSE,FALSE,Thu Aug 28 2025 22:32:28 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Jurian van Hoorn,azure,azure-load-balancer,inefficient-configuration,networking,"<p id="""">As organizations migrate from the Basic to the Standard tier of Azure Load Balancer (driven by Microsoft’s retirement of the Basic tier), they may unknowingly inherit cost structures they didn’t previously face. Specifically, each load balancing rule—both inbound and outbound—can contribute to ongoing charges. In applications that historically relied only on Basic load balancers, outbound rules may never have been configured, meaning their inclusion post-migration could be unnecessary.</p><p id="""">This inefficiency tends to emerge in larger Azure estates where infrastructure-as-code or templated environments create load balancers in bulk, often replicating rules without review. Over time, dozens or hundreds of unused or outdated rules can accumulate, inflating network costs with no operational benefit.</p>","<p id="""">Azure Standard Load Balancer incurs hourly charges for each configured load balancing rule beyond the first five, as well as for each configured outbound rule and each data processed unit. Unlike the retired Basic tier (which had no per-rule charges), the Standard tier's billing is tied directly to the number of rules. This makes it important to assess the necessity of each rule, especially in environments with many load balancers or frequent replication across dev, QA, and staging.</p>","<ul id=""""><li id="""">Identify Standard Load Balancers with more than five inbound or outbound rules</li><li id="""">Review whether inbound and outbound rules are actively used by the application</li><li id="""">Check for templated or duplicated rules across non-production environments</li><li id="""">Determine whether rules were carried over from legacy Basic-tier load balancers</li><li id="""">Confirm with application teams whether each rule is still required</li><li id="""">Review load balancer configurations in environments that have recently been migrated or cloned</li></ul>","<ul id=""""><li id="""">Audit existing Standard Load Balancer rule sets to identify unused entries</li><li id="""">Remove unnecessary inbound and outbound rules, especially in non-production environments</li><li id="""">Avoid blanket rule creation in templated environments unless explicitly required</li><li id="""">Coordinate with application owners before removing rules to avoid disruption</li></ul>",,FALSE,FALSE,,,,<p>Azure-Networking-4339</p>
Suboptimal Log Class Configuration in CloudWatch,suboptimal-log-class-configuration-in-cloudwatch,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941b298489d35bfd7fc5,FALSE,FALSE,Sun Jun 22 2025 23:39:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Kyler Rupe,aws,aws-cloudwatch,misconfiguration,other,"<p id="""">By default, CloudWatch Log Groups use the Standard log class, which applies higher rates for both ingestion and storage. AWS also offers an Infrequent Access (IA) log class designed for logs that are rarely queried — such as audit trails, debugging output, or compliance records.  Many teams assume storage is the dominant cost driver in CloudWatch, but in high-volume environments, ingestion costs can account for the majority of spend. When logs that are infrequently accessed are ingested into the Standard class, it leads to unnecessary costs without impacting observability. The IA log class offers significantly reduced rates for ingestion and storage, making it a better fit for logs used primarily for post-incident review, compliance retention, or ad hoc forensic analysis.</p>","<p id="""">CloudWatch Logs are billed based on:  * Ingestion volume per GB   * Storage per GB-month   * Querying via Logs Insights (optional)  By default, Log Groups are set to the <strong id="""">Standard</strong> log class. However, AWS offers an <strong id="""">Infrequent Access</strong> (IA) log class with significantly lower ingestion and storage rates. This class is ideal for logs accessed occasionally, such as security audits, application debugging, or forensic analysis. Because the log class must be set at creation, many environments default to Standard even when IA would be more cost-effective.</p>","<ul id=""""><li id="""">Identify all log groups currently using the Standard log class</li><li id="""">For each, assess whether logs are actively queried or primarily retained for infrequent access</li><li id="""">Review ingestion volumes relative to query frequency to surface high-ingest, low-access patterns</li><li id="""">Evaluate log retention periods — long-retained logs with low query activity are strong candidates</li><li id="""">Analyze whether ingestion charges account for a significant portion of CloudWatch costs</li><li id="""">Flag logs used for compliance, audit, or post-incident forensics that could be routed to Infrequent Access</li></ul>","<ul id=""""><li id="""">Create new log groups using the <strong id="""">Infrequent Access</strong> class for applicable use cases</li><li id="""">Update application and service configurations to route logs to the new log groups</li><li id="""">Use subscription filters or log routing to separate high-access logs (Standard) from infrequent logs (IA)</li><li id="""">Apply tagging or naming conventions to distinguish log groups by access frequency or retention intent</li><li id="""">Periodically audit log group configurations and adjust routing based on evolving usage patterns</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogsLogClass.html"" id="""">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogsLogClass.html</a><br><a href=""https://aws.amazon.com/cloudwatch/pricing/#Logs"" id="""">https://aws.amazon.com/cloudwatch/pricing/\#Logs</a></p>",FALSE,FALSE,,,,<p>AWS-Other-2627</p>
Suboptimal Memory-to-CPU Ratio in EKS Cluster Node,suboptimal-memory-to-cpu-ratio-in-eks-cluster-node,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b423b573855ade54a08,FALSE,FALSE,Thu May 22 2025 14:57:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-eks,inefficient-configuration,compute,"<p id="""">When the EC2 instance types used for EKS node groups have a memory-to-CPU ratio that doesn’t match the workload profile, the result is poor bin-packing efficiency. For example, if memory-intensive containers are scheduled on compute-optimized nodes, memory may run out first while CPU remains unused. This forces new nodes to be provisioned earlier than necessary. Over time, this mismatch can lead to higher compute costs even if the cluster appears fully utilized.</p>","<p id="""">EKS clusters incur compute charges based on the EC2 instance types used in each node group. Each instance type has a fixed vCPU-to-memory ratio and is billed per second or hour. If this ratio doesn’t align with the actual requirements of the container workloads, either CPU or memory may be underutilized—resulting in inefficient node usage and unnecessary cost. The EKS service itself also incurs a fixed hourly fee per cluster.</p>","<ul id=""""><li id="""">Review container-level memory and CPU requests and limits across the cluster</li><li id="""">Assess node-level resource utilization to detect fragmentation (e.g., memory maxed out while CPU remains idle)</li><li id="""">Compare the vCPU-to-memory ratio of node types to the average resource profile of scheduled workloads</li><li id="""">Check whether mixed workload types are scheduled onto shared nodes, leading to inefficient bin-packing</li><li id="""">Determine if better alignment could be achieved using memory-optimized (e.g., R6g) or compute-optimized (e.g., C6g) instances</li><li id="""">Engage with platform teams to review the rationale behind current node type selection</li></ul>","<p id="""">Update node groups to use EC2 instance types with resource ratios better suited to the workloads they support. Where applicable, split workloads into dedicated node groups by profile (e.g., memory-heavy vs. compute-heavy) and apply appropriate instance families. For autoscaling clusters, consider enabling mixed-instance policies to allow better flexibility and reduce scheduling inefficiencies.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/eks/pricing/\&quot;"" id="""">AWS EKS Pricing</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/instance-types/\"" id="""">Amazon EC2 Instance Types</a></li></ul>",FALSE,FALSE,,93,,<p>AWS-Compute-7527</p>
Suboptimal Query Routing,suboptimal-query-routing,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4b4ef9030f968ebbbf,FALSE,FALSE,Thu May 22 2025 14:57:15 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-query-processing,suboptimal-query-routing-and-warehouse-utilization,other,"<p id="""">Organizations may experience unnecessary Snowflake spend due to inefficient query-to-warehouse routing, lack of dynamic warehouse scaling, or failure to consolidate workloads during low-usage periods. Third-party platforms offer solutions to address these inefficiencies:</p><ul id="""">  <li id="""">Sundeck enables highly customizable, SQL-based control over the query lifecycle through user-defined rules (Flows, Hooks, Conditions). Cost optimization techniques include adaptive warehouse routing, instant warehouse suspension, and off-peak consolidation. However, it requires users to maintain optimization logic manually.</li>  <li id="""">Keebo offers a fully automated AI-driven approach, dynamically tuning warehouse size, clustering, and memory configurations without requiring manual query intervention. It prioritizes minimal operational effort with continuous background optimization.</li></ul><p id="""">Choosing between these solutions depends heavily on the organization's internal capabilities and desired balance between control and automation.</p>",,"<ul id="""">  <li id="""">Assess whether query workloads are currently routed manually or rely on static warehouse assignments</li>  <li id="""">Review warehouse utilization patterns to identify opportunities for dynamic resizing, load balancing, or consolidation during off-peak periods</li>  <li id="""">Evaluate whether internal teams have the expertise and capacity to manage custom query optimization rules</li>  <li id="""">Consider whether an AI-driven optimization platform would better fit organizational needs for cost reduction with minimal manual overhead</li></ul>","<ul id="""">  <li id="""">Implement customizable query lifecycle management platforms (e.g., Sundeck) if granular control is required and in-house SQL/DevOps expertise is available</li>  <li id="""">Deploy AI-driven warehouse optimization platforms (e.g., Keebo) for organizations prioritizing ease of use and autonomous cost management</li>  <li id="""">Pilot third-party solutions in a limited environment to validate cost savings and performance impacts before full-scale adoption</li>  <li id="""">Continuously monitor optimization effectiveness and adjust platform configurations based on workload evolution and business priorities</li></ul>",,FALSE,FALSE,,,,<p>Snowflake-Other-6849</p>
Suboptimal Query Timeout Configuration,suboptimal-query-timeout-configuration,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4b227d926580d4c542,FALSE,FALSE,Thu May 22 2025 14:57:15 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-virtual-warehouse,suboptimal-configuration,compute,"<p id="""">If no appropriate query timeout is configured, inefficient or runaway queries can execute for extended periods (up to the default 2-day system limit). For as long as the query is running, the warehouse will remain active and accrue costs. Proper timeout settings help terminate inefficient queries, free up compute capacity, and allow the warehouse to become idle sooner, making it eligible for auto-suspend once the inactivity timer is reached.</p>",,"<ul id="""">  <li id="""">Review account-level query timeout settings to confirm if a global policy shorter than the default is enforced</li>  <li id="""">Identify warehouses with no customized query timeout configured for workloads requiring exceptions</li>  <li id="""">Evaluate historical query run durations to detect unusually long-running queries that could benefit from timeout enforcement</li>  <li id="""">Validate with application owners whether specific workloads legitimately require extended query durations before setting tighter policies</li></ul>","<ul id="""">  <li id="""">Configure a conservative account-level query timeout policy to limit maximum query execution times (e.g., 4–12 hours based on environment needs).</li>  <li id="""">Apply customized warehouse-level or user-level timeout policies for workloads that genuinely require longer execution windows.</li>  <li id="""">Regularly review and adjust query timeout settings as workload patterns evolve.</li>  <li id="""">Coordinate changes with data engineering teams to avoid unintended interruption of legitimate long-running jobs.</li>  <li id="""">Setup anomaly detection on long running queries</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history"" id="""">Query Cost Attribution</a></li><li id=""""><a href=""https://docs.snowflake.com/en/sql-reference/functions/query_history"" id="""">Query History</a></li></ul>",FALSE,FALSE,,,,<p>Snowflake-Compute-3317</p>
Suboptimal RDS Instance Storage Type,suboptimal-rds-instance-storage-type,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4447bdcbdaaafbf5bf,FALSE,FALSE,Thu May 22 2025 14:57:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-rds,inefficient-configuration,databases,"<p id="""">This inefficiency occurs when an RDS instance uses a high-cost storage type such as io1 or io2 but does not require the performance benefits it provides. In many cases, provisioned IOPS are set at or below the free baseline included with gp3 (3,000 IOPS and 125 MB/s). In such scenarios, continuing to use provisioned IOPS storage results in elevated cost with no functional advantage. These misconfigurations often persist due to legacy templates, default settings, or a lack of periodic review.</p>","<p id="""">RDS storage is billed per GB-month based on the storage type selected. Premium options like io1 and io2 charge separately for provisioned IOPS, while general-purpose types like gp3 include baseline IOPS and throughput at no extra cost. Overprovisioning or selecting the wrong storage tier leads to unnecessary spend.</p>","<ul id=""""><li id="""">Identify RDS instances using high-cost storage types (e.g., io1, io2)</li><li id="""">Review IOPS and throughput metrics to assess whether provisioned performance is being fully utilized</li><li id="""">Evaluate whether current workloads would meet SLAs with a general-purpose alternative like gp3</li><li id="""">Confirm with application owners whether the selected storage type is still required</li></ul>","<p id="""">If the provisioned IOPS are within the gp3 baseline, migrate to gp3 to reduce costs without impacting performance. Storage type changes may require a snapshot restore or be applied during a maintenance window, depending on the database engine and configuration. Update infrastructure templates and provisioning logic to default to gp3 when appropriate.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\&quot;"" id="""">RDS Storage Types</a></li><li id=""""><a href=""https://aws.amazon.com/rds/pricing/\"" id="""">RDS Pricing</a></li></ul>",FALSE,FALSE,,82,,<p>AWS-Databases-3695</p>
Suboptimal Region for EC2 Instance,suboptimal-region-for-ec2-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b42f17fe7182011892d,FALSE,FALSE,Thu May 22 2025 14:57:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-ec2,inefficient-architecture,compute,"<p id="""">Workloads are sometimes deployed in specific AWS regions based on legacy decisions, developer convenience, or perceived performance requirements. However, regional EC2 pricing can vary significantly, and placing instances in a suboptimal region can lead to higher compute costs, increased data transfer charges, or both. In particular, workloads that frequently communicate with resources in other regions—or that serve a user base concentrated elsewhere—can incur unnecessary costs. Re-evaluating regional placement can reduce these costs without compromising performance or availability when done strategically.</p>","<p id="""">EC2 instance pricing varies by region, with different hourly rates for the same instance type depending on local infrastructure costs.</p><ul id=""""><li id="""">Data transfer within the same Availability Zone is typically free</li><li id="""">Cross-AZ and cross-region traffic is charged per GB, often at higher rates</li><li id="""">Regional EC2 pricing differences and associated data transfer charges can create cost inefficiencies when workloads are placed in a region that is not optimal for price or proximity to dependent services</li></ul>","<ul id=""""><li id="""">Identify EC2 instances that generate significant cross-region or cross-AZ data transfer</li><li id="""">Review the current region’s EC2 pricing compared to other AWS regions supporting the same instance type</li><li id="""">Evaluate whether the instance communicates heavily with services or users located in another region</li><li id="""">Assess whether the workload has specific latency, compliance, or availability requirements that restrict regional placement</li><li id="""">Determine whether relocating the instance would reduce data transfer costs or take advantage of lower regional EC2 pricing</li><li id="""">Confirm with infrastructure or application owners whether the region was chosen intentionally or can be revisited</li></ul>","<p id="""">Where feasible, relocate EC2 instances to regions that are better aligned with the workload’s data sources, users, or dependent services. This may reduce both compute and data transfer costs. Always confirm that any changes to regional placement comply with application architecture, latency tolerances, and regulatory constraints.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/on-demand/\&quot;"" id="""">AWS EC2 Pricing by Region</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer\"" id="""">AWS Data Transfer Pricing</a></li></ul>",FALSE,FALSE,,69,,<p>AWS-Compute-9154</p>
Suboptimal Region for Internet-Only EC2 Instance,suboptimal-region-for-internet-only-ec2-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b46ec494686b892443f,FALSE,FALSE,Thu May 22 2025 14:57:10 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),,aws,aws-ec2,inefficient-architecture,compute,"<p id="""">When an EC2 instance is dedicated primarily to internet-facing traffic, regional differences in data transfer pricing can drive a substantial portion of total costs. Hosting such workloads in a region with higher egress rates leads to elevated expenses without improving performance. Migrating the workload to a lower-cost region can yield significant savings while maintaining equivalent service quality, especially when no strict latency or compliance requirements dictate the original location.</p>","<p id="""">EC2 instances are billed per second based on the selected instance type, operating system, and pricing model (On-Demand, Reserved, or Spot). Data transfer out to the public internet is billed separately per GB, and rates vary by AWS region. Some regions have significantly higher internet egress charges, which can disproportionately increase costs for internet-heavy workloads.</p>","<ul id=""""><li id="""">Identify EC2 instances whose primary traffic flows are outbound to the public internet rather than within AWS or private networks</li><li id="""">Review the region associated with each instance and compare public Data Transfer Out pricing against lower-cost regions</li><li id="""">Assess latency requirements, regulatory considerations, and service availability constraints that may impact regional choices</li><li id="""">Evaluate application dependencies, user location distribution, and DNS configurations to determine feasibility of migration</li><li id="""">Validate migration plans with infrastructure, networking, and application teams before execution</li></ul>","<p id="""">Migrate internet-focused EC2 instances to regions with lower Data Transfer Out pricing to optimize costs. Use methods such as AMI copies, EBS snapshots, or infrastructure-as-code templates to deploy the instance in the new region. Update DNS records, security groups, IAM roles, and route configurations as needed to complete the migration. Monitor performance after relocation to ensure that user experience and application behavior remain stable.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/ec2/index.html\&quot;"" id="""">Amazon EC2 Documentation</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer\"" id="""">AWS Data Transfer Pricing</a></li></ul>",FALSE,FALSE,,74,,<p>AWS-Compute-1072</p>
Suboptimal Routing Through NAT Gateway Instead of VPC Endpoint,suboptimal-routing-through-nat-gateway-instead-of-vpc-endpoint,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b49e09e434112c2f60e,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Jason Eckle,aws,aws-nat-gateway,inefficient-configuration,networking,"<p id="""">Workloads in private subnets often access AWS services like S3 or DynamoDB. If this traffic is routed through a NAT Gateway, it incurs both hourly and data processing charges. However, AWS offers VPC Gateway Endpoints (for S3/DynamoDB) and Interface Endpoints (for other services), which provide private access paths that bypass the NAT Gateway entirely. When teams fail to use VPC endpoints — often due to default routing configurations or lack of awareness — they unnecessarily route internal service calls through a costlier, public-facing path. This leads to persistent and avoidable spend.</p>","<p id="""">NAT Gateways are billed based on a flat hourly fee for provisioning, plus a per-gigabyte charge for data processed. VPC Gateway Endpoints for services like S3 and DynamoDB are free to use. Interface Endpoints for other AWS services do incur hourly and per-gigabyte charges but are typically more cost-effective than routing the same traffic through a NAT Gateway.</p>","<ul id=""""><li id="""">Check VPC route tables to identify traffic to AWS services routed through NAT Gateway</li><li id="""">Use CloudWatch metrics to analyze NAT Gateway data processing volumes</li><li id="""">Identify subnets that interact with S3, DynamoDB, or other AWS services and lack a configured VPC endpoint</li><li id="""">Correlate high NAT Gateway traffic with internal service calls to confirm inefficiency</li></ul>","<ul id=""""><li id="""">Create VPC Gateway Endpoints for services like S3 and DynamoDB in applicable regions</li><li id="""">Use Interface Endpoints for other AWS services frequently accessed by private subnet workloads</li><li id="""">Update route tables to redirect traffic through the appropriate VPC endpoint instead of NAT Gateway</li><li id="""">Educate engineering teams on when and how to use VPC endpoints to minimize network costs</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/vpc/pricing/#nat-gateway\&quot;"" id="""">AWS NAT Gateway Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html\"" id="""">VPC Endpoints Overview</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Networking-3847</p>
Suboptimal Storage Configuration for Aurora Cluster,suboptimal-storage-configuration-for-aurora-cluster,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b3f79272555d30fd3b8,FALSE,FALSE,Thu May 22 2025 14:57:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-aurora,misconfiguration,databases,"<p id="""">Many Aurora clusters default to using the Standard configuration, which charges separately for I/O operations. For workloads with frequent read and write activity, this can lead to unnecessarily high costs. Aurora I/O-Optimized eliminates I/O charges entirely and simplifies cost predictability. In environments with consistently high I/O usage, switching to I/O-Optimized often results in lower total spend.</p>","<p id="""">Aurora offers two storage configurations with distinct cost structures:</p><ul id=""""><li id="""">Aurora Standard: Lower per-GB storage cost, but incurs additional charges for I/O operations (reads and writes)</li><li id="""">Aurora I/O-Optimized: Higher per-GB storage cost, but includes unlimited I/O with no additional I/O charges</li></ul><p id="""">Choosing the right configuration depends on the balance between storage volume and I/O intensity. High-I/O workloads may see significantly lower costs under I/O-Optimized pricing.</p>","<ul id=""""><li id="""">Identify Aurora clusters currently using the Standard storage configuration</li><li id="""">Review historical cost breakdowns to evaluate the portion of spend attributed to I/O charges</li><li id="""">Determine whether the workload exhibits high read/write activity or transactional behavior</li><li id="""">Assess whether I/O charges regularly account for a significant share of total Aurora costs</li><li id="""">Estimate whether switching to I/O-Optimized would lower total cost based on current storage and I/O usage patterns</li><li id="""">Confirm that the cluster meets version and engine requirements for I/O-Optimized eligibility</li><li id="""">Engage database administrators or workload owners to validate I/O behavior and review trade-offs</li></ul>","<p id="""">Modify the cluster to use Aurora I/O-Optimized if the workload is read/write intensive and I/O costs are high. This change can be made without downtime and takes effect immediately for supported Aurora versions. Monitor ongoing cost and performance to confirm the change results in expected benefits.</p>","<p id=""""><a href=""https://aws.amazon.com/rds/aurora/pricing/\&quot;"" id="""">Aurora Pricing</a></p>",FALSE,FALSE,,90,"<p id="""">2 TB of storage with 100 million I/O requests per day in <em id="""">us-east-1</em>:</p><ul id=""""><li id=""""><strong id="""">Aurora Standard:</strong> $0.10/GB-month + $0.20 per 1 M I/O requests = $800/mo</li><li id=""""><strong id="""">Aurora I/O-Optimized:</strong> $0.225/GB-month (I/O included) = $450/mo</li><li id=""""><strong id="""">Savings:</strong> $350/mo (44% Savings)</li></ul>",<p>AWS-Databases-3877</p>
Suboptimal Storage Type for DynamoDB Table,suboptimal-storage-type-for-dynamodb-table,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4594d3e1f351ef314b,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),,aws,aws-dynamodb,inefficient-configuration,databases,"<p id="""">This inefficiency occurs when a table remains in the default Standard storage class despite having minimal or infrequent access. In these cases, switching to Standard-IA can significantly reduce monthly storage costs, especially for archival tables, compliance data, or legacy systems that are still retained but rarely queried.</p>","<p id="""">DynamoDB tables are billed based on the storage class assigned to the entire table.</p><ul id=""""><li id="""">Standard: Higher monthly storage cost, no per-read penalty.</li><li id="""">Standard-IA: Lower storage cost, with an added per-read charge.</li></ul><p id="""">Tables incur ongoing storage charges regardless of usage, so selecting the wrong storage class can lead to unnecessary cost.</p>","<ul id=""""><li id="""">Identify tables currently set to the Standard storage class</li><li id="""">Review read access patterns over the past 30+ days to confirm low usage</li><li id="""">Determine whether the table is used for active workloads or primarily exists for reference, compliance, or long-term retention</li><li id="""">Assess whether the workload can tolerate a small per-read fee and slightly higher latency</li><li id="""">Validate with data owners or service teams before changing the storage class</li></ul>","<p id="""">If the table is rarely read and doesn't serve an active production workload, consider switching it to Standard-IA. Start by confirming that read latency and per-request charges won’t impact performance or budget. Then update the table’s storage class using the AWS Console, CLI, or SDK. This change can be made without downtime or data migration. To prevent recurrence, review provisioning defaults in infrastructure-as-code templates or automation scripts to ensure archival or infrequently accessed tables default to Standard-IA when appropriate. Periodically audit storage class assignments across environments to identify tables that may no longer require high-frequency access.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.TableClasses.html\&quot;"" id="""">DynamoDB Table Classes</a></li><li id=""""><a href=""https://aws.amazon.com/dynamodb/pricing/\"" id="""">DynamoDB Pricing</a></li></ul>",FALSE,FALSE,,87,,<p>AWS-Databases-3316</p>
Suboptimal Storage for Logs,suboptimal-storage-for-logs-fd1d9,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80df6885b3fea5fbaf3c,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:09 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Yuval Goldstein,gcp,gcp-cloud-logging,misaligned-storage-destination,other,"<p id="""">Many organizations retain all logs in Cloud Logging’s standard storage, even when the data is rarely queried or required only for audit or compliance. Logging buckets are priced for active access and are not optimized for low-frequency retrievas, results in unnecessary expense. Redirecting logs to BigQuery or Cloud Storage can provide better cost efficiency, particularly when coupled with lifecycle policies or table partitioning. Choosing the optimal storage destination based on access frequency and analytics needs is essential to control log retention costs.</p>","<p id="""">. Exported logs incur costs in the target service — Logging buckets (per GB ingested/stored) Cloud Storage (per GiB stored and retrieved), BigQuery (per GiB stored and queried), or Pub/Sub (per GiB). Selecting the wrong destination or storage class can lead to higher-than-necessary long-term costs.</p>","<ul id=""""><li id="""">Review current log storage destinations and retention periods across projects and organizations</li><li id="""">Identify logs stored in Cloud Logging buckets that have not been queried or accessed recently</li><li id="""">Assess whether archival or analytical needs justify retention in Cloud Logging versus BigQuery or Cloud Storage *</li></ul>","<ul id=""""><li id="""">Redirect logs to cost-effective destinations such as BigQuery or Cloud Storage using sink.</li><li id="""">Use BigQuery partitioning and clustering to reduce query and storage costs for analytical log data</li></ul>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/stackdriver/pricing"" id="""">Cloud Logging Pricing</a></li><li id=""""><a href=""https://cloud.google.com/storage/docs/lifecycle"" id="""">Cloud Storage Lifecycle Management</a></li></ul>",FALSE,FALSE,,,,GCP-Other-8973
Suboptimal Table Plan Selection in Log Analytics,suboptimal-table-plan-selection-in-log-analytics,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fda8526632aa608184,FALSE,FALSE,Thu Aug 28 2025 22:32:29 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Jurian van Hoorn,azure,,suboptimal-pricing-model,other,"<p id="""">By default, all Log Analytics tables are created under the Analytics plan, which is optimized for high-performance querying and interactive analysis. However, not all telemetry requires real-time access or frequent querying. Some tables may serve audit, archival, or compliance use cases where querying is rare or unnecessary. Leaving such tables on the Analytics plan results in unnecessary spend—especially when ingestion volumes are high or the table receives data from verbose sources (e.g., diagnostic logs, platform metrics).</p><p id="""">Azure now allows users to assign different pricing plans at the table level, including the Basic plan, which offers significantly lower ingestion costs at the expense of reduced query functionality. This provides a valuable opportunity to align cost with access patterns by assigning less expensive plans to tables that are retained for record-keeping or compliance, rather than analysis.</p>","<p id="""">Azure Log Analytics charges based on the ingestion volume and pricing plan assigned to each table. The default plan—Analytics—offers full query capabilities but is the most expensive option. The Basic plan is intended for low-access or audit-only data and provides lower-cost storage with limited query functionality. Costs can accumulate rapidly when high-ingestion tables default to the Analytics plan despite minimal interaction or querying needs.</p>","<ul id=""""><li id="""">Identify Log Analytics tables assigned to the default Analytics plan</li><li id="""">Analyze which tables are infrequently queried or used solely for audit purposes</li><li id="""">Review ingestion volume by table to identify high-cost, low-access data</li><li id="""">Check whether verbose or chatty data sources are routed to high-cost plans unnecessarily</li><li id="""">Confirm with application owners whether full query functionality is required for each table</li><li id="""">Assess whether table plans are intentionally assigned or left at default</li></ul>","<ul id=""""><li id="""">Assign the Basic plan to tables that are retained for audit, archival, or compliance purposes</li><li id="""">Split high-volume ingestion sources into separate tables based on access needs</li><li id="""">Reconfigure ingestion routes to direct non-essential logs to lower-cost tables</li><li id="""">Periodically audit table plan assignments to ensure alignment with usage patterns</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-logs-tables?tabs=azure-portal"" id="""">https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-logs-tables?tabs=azure-portal</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Other-3505</p>
Suboptimal Use of Compute Savings Plans for Specialized Instances,suboptimal-use-of-compute-savings-plans-for-specialized-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84cb856928cc94697cd,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),David Gross,aws,aws-ec2,suboptimal-pricing-model,compute,"<p id="""">Accelerated EC2 instance types such as `p5.48xlarge` and `p5en.48xlarge (often used for ML/AI workloads)` are eligible for Compute Savings Plans, but the discount rates offered are modest compared to more common instance families. When organizations rely solely on CSPs, these lower priority instances are typically the last to benefit from the plan, especially if other instance types consume most of the discounted hours.</p><p id="""">As a result, p5 usage may fall through the cracks and be billed at full On-Demand rates despite an active CSP. This dynamic makes CSPs a potentially inefficient choice for workloads that heavily or predictably rely on these instance types. EC2 Instance Savings Plans provide better discount targeting for known usage patterns, and AWS now offers dedicated P5 and P5en Instance Savings Plans with up to 40% savings specifically for these instance types. Additionally, while Capacity Blocks offer the steepest discount, they come with operational rigidity and inflexible scheduling constraints that limit their applicability.</p>","<p id="""">AWS EC2 instances can be purchased via multiple pricing models: On-Demand, Compute Savings Plans (CSPs), EC2 Instance Savings Plans, Reserved Instances, and Capacity Blocks (for specific accelerated instance types).</p><p id="""">Compute Savings Plans apply dollar-based discounts based on discount percentage priority, with savings first applied to usage with the highest discount percentage. If multiple instances have the same discount percentage, the plan applies to usage with the lowest rate. This allocation methodology can result in accelerated instances like p5.48xlarge and p5en.48xlarge receiving lower priority if its discount percentages are less competitive than other instance types in the environment.</p><p id="""">For accelerated instances like `p5.48xlarge` and `p5en.48xlarge`, CSPs may offer relatively low discount rates, which makes them both less cost-effective and deprioritized in CSP allocation. This can lead to uncovered spend on these expensive instances unless the total CSP commitment is large enough to cover all usage.</p>","<ul id=""""><li id="""">Review usage patterns of high-cost accelerated instances such as `p5.48xlarge` and `p5en.48xlarge`</li><li id="""">Determine if these instances are being run consistently at high utilization levels</li><li id="""">Assess how much of their usage is actually covered under existing Compute Savings Plans</li><li id="""">Verify current CSP discount rates for various instances in your specific regions, as rates vary by region, term, and payment option</li><li id="""">Evaluate whether other instance types with higher CSP discount rates are exhausting the plan allocation</li><li id="""">Compare costs against dedicated EC2 Instance Savings Plans (up to 40% savings) for predictable workloads</li><li id="""">Confirm whether an EC2 Instance Savings Plan would have better alignment with observed usage patterns</li><li id="""">Identify potential overcommitment risks if CSPs were scaled up solely to include p-type instance usage</li></ul>","<ul id=""""><li id="""">Consider using dedicated EC2 Instance Savings Plans instead of CSPs for predictable, high-utilization p5 workloads</li><li id="""">Model CSP allocation on actual discount percentages in order to determine whether p-type instances are likely to be left uncovered</li><li id="""">Compare total cost of ownership between CSPs and dedicated instance savings plans for your specific usage patterns</li><li id="""">Evaluate potential fit for Capacity Blocks only if workloads are short-term, highly predictable, and operationally isolated</li><li id="""">Ensure any long-term commitment aligns with actual usage patterns and instance mix</li><li id="""">Review and update savings plan strategy as AWS introduces new instance-specific savings plan options</li></ul>",,FALSE,FALSE,,,,<p>AWS-Compute-1942</p>
Suboptimal Use of EFS Storage Classes,suboptimal-use-of-efs-storage-classes,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58e1a5cc4d734cc044a,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Amay Chandravanshi,aws,aws-efs,misaligned-storage-tiering,storage,"<p id="""">Many organizations default to storing all EFS data in the Standard class, regardless of how frequently data is accessed. This results in inefficient spend for workloads with significant portions of data that are rarely read. EFS IA and Archive tiers offer lower-cost alternatives for data with low or near-zero access, while Intelligent Tiering can automate placement decisions. Failing to leverage these options wastes storage spend and reduces cost efficiency.</p>","<p id="""">EFS charges separately for data stored in each class. Standard storage is priced highest, while IA and Archive offer lower storage costs but charge for data retrieval. Intelligent Tiering automatically transitions data between classes based on access patterns. Placing all data in Standard when much of it is infrequently accessed leads to unnecessary cost.</p>","<ul id=""""><li id="""">Review whether large EFS volumes are configured entirely in Standard storage class without considering access frequency</li><li id="""">Assess whether filesystems contain long-lived data with little or no recent access</li><li id="""">Evaluate if Intelligent Tiering could automate tier placement in cases where access patterns are unpredictable</li><li id="""">Compare storage distribution across Standard, IA, and Archive tiers to workload access requirements</li></ul>","<ul id=""""><li id="""">Transition infrequently accessed data to EFS IA or Archive storage classes to reduce cost</li><li id="""">Enable Intelligent Tiering for workloads where access frequency is variable or difficult to predict</li><li id="""">Maintain Standard storage only for data that requires frequent or high-performance access</li><li id="""">Periodically review access patterns and adjust lifecycle policies to align with current workload usage</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html"" id="""">https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html</a></li></ul>",FALSE,FALSE,,N/A,,<p>AWS-Storage-9239</p>
Suboptimal Use of Intel-Based Instances in OpenSearch,suboptimal-use-of-intel-based-instances-in-opensearch,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58e15fbc38836ef10ed,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Jérémy Nancel,aws,aws-opensearch,suboptimal-instance-selection,other,"<p id="""">AWS Graviton processors are designed to deliver better price-performance than comparable Intel-based instances, often reducing cost by 20–30% at equivalent workload performance. OpenSearch domains running on older Intel-based families consume more spend without providing additional capability. Since Graviton-powered instance types are functionally identical in features and performance for OpenSearch, continuing to run on Intel-based clusters represents unnecessary inefficiency.</p>","<p id="""">OpenSearch is billed based on the underlying EC2 instance hours consumed by the cluster, plus storage and data transfer. Choosing less efficient Intel-based instance types (e.g., `m5`, `r5`, `i3`, `i4i`) results in higher cost per unit of performance compared to Graviton-based instances (`m6g`, `c6g`, `r6g`, `i4g`).</p>","<ul id=""""><li id="""">Review all active OpenSearch domains and identify clusters running on Intel-based instance types that have equivalent Graviton alternatives</li><li id="""">Assess whether workload performance requirements (CPU, memory, IOPS) could be met by moving to Graviton families</li><li id="""">Confirm OpenSearch version compatibility to ensure upgrade paths to supported Graviton instance types</li><li id="""">Evaluate whether production and non-production domains are consistently aligned with optimal instance families</li></ul>","<ul id=""""><li id="""">Migrate OpenSearch domains from Intel-based instances (e.g., `m5`, `r5`, `i4i`) to equivalent Graviton families (`m6g`, `c6g`, `r6g`, `i4g`)</li><li id="""">Leverage in-place instance type updates for clusters where supported to minimize downtime</li><li id="""">Benchmark performance after migration to validate expected cost-performance improvements</li></ul>","<ul id=""""><li id="""">Amazon OpenSearch Service Instance Types</li></ul>",FALSE,FALSE,,N/A,,<p>AWS-Other-4626</p>
Suboptimal Use of On-Demand Instances in Fault-Tolerant EC2 Workloads,suboptimal-use-of-on-demand-instances-in-fault-tolerant-ec2-workloads,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b47ab340679d5a02fb7,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Matt Weingarten,aws,aws-ec2,suboptimal-pricing-model,compute,"<p id="""">Many EC2 workloads—such as development environments, test jobs, stateless services, and data processing pipelines—can tolerate interruptions and do not require the reliability of On-Demand pricing. Using On-Demand instances in these scenarios drives up cost without adding value. Spot Instances offer significantly lower pricing and are well-suited to workloads that can handle restarts, retries, or fluctuations in capacity. Without evaluating workload tolerance and adjusting pricing models accordingly, organizations risk consistently overpaying for compute.</p>","<p id="""">EC2 is billed based on:</p><ul id=""""><li id="""">Instance Type and Size — determines the base hourly rate</li><li id="""">Pricing Model</li><li id="""">On-Demand: Highest cost, flexible usage</li><li id="""">Spot: Deep discounts with interruption risk</li><li id="""">Reservations/Savings Plans: Lower cost for committed usage</li></ul><p id="""">On-Demand usage incurs the highest hourly rate. When applied to fault-tolerant workloads, it often leads to avoidable overspending.</p>","<ul id=""""><li id="""">Identify EC2 instances in non-production environments or batch frameworks running on On-Demand</li><li id="""">Review Auto Scaling Group and launch template settings for pricing model selection</li><li id="""">Cross-check usage patterns against workload type and fault tolerance characteristics</li><li id="""">Engage engineering teams to validate whether On-Demand usage is justified</li></ul>","<ul id=""""><li id="""">Reconfigure eligible workloads to use Spot Instances via launch templates or Auto Scaling policies</li><li id="""">Use mixed-instance and capacity-optimized allocation strategies for better availability</li><li id="""">Apply On-Demand only where availability, SLA, or licensing requirements demand it</li><li id="""">Encourage team-level cost reviews to identify low-risk transitions to Spot</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html"" id="""">AWS Spot Instances</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/"" id="""">EC2 Pricing Models Overview</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-9822</p>
Suboptimal Use of On-Demand Instances in Non-Production Clusters,suboptimal-use-of-on-demand-instances-in-non-production-clusters,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b47fa5354209ec4ba31,FALSE,FALSE,Thu May 22 2025 14:57:11 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Matt Weingarten,databricks,databricks-clusters,suboptimal-pricing-model,compute,"<p id="""">In Databricks, on-demand instances provide reliable performance but come at a premium cost. For non-production workloads—such as development, testing, or exploratory analysis—high availability is often unnecessary. Spot instances provide equivalent performance at a lower price, with the tradeoff of occasional interruptions. If teams default to on-demand usage in lower environments, they may be incurring unnecessary compute costs. Using compute policies to limit on-demand usage ensures greater consistency and efficiency across environments.</p>","<p id="""">Databricks charges for compute based on:</p><ul id=""""><li id="""">Databricks Units (DBUs): Varies by cluster configuration, including node type and pricing model (on-demand vs. Spot)</li></ul><ul id=""""><li id="""">Cloud Infrastructure Charges: Passed through from the cloud provider and dependent on instance pricing model</li></ul><p id="""">On-demand nodes incur the highest cost. Spot instances offer significant discounts but may be interrupted and are best used for dev/test workloads.</p>","<ul id=""""><li id="""">Query system tables to identify non-production clusters with high or full on-demand usage</li><li id="""">Review workspace and cluster policies to determine if Spot usage is being enforced</li><li id="""">Confirm whether the workloads running on these clusters are tolerant to interruptions</li><li id="""">Evaluate whether the use of on-demand instances is justified for each environment</li></ul>","<ul id=""""><li id="""">Implement compute policies that cap the percentage of on-demand nodes in relevant workloads</li><li id="""">Update existing cluster configurations to prioritize Spot usage for dev/test workloads</li><li id="""">Allow exceptions only when reliability or performance constraints are well documented</li></ul>","<ul id=""""><li id="""">Spot Instances in Databricks</li><li id="""">Compute Policies Overview</li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-9445</p>
Suboptimal Use of On-Demand Instances in a Non-Production EKS Cluster,suboptimal-use-of-on-demand-instances-in-a-non-production-eks-cluster,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b45581d6c4e55439ae1,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),,aws,aws-eks,inefficient-architecture,compute,"<p id="""">Running non-production clusters solely on On-Demand Instances results in unnecessarily high compute costs. Development, testing, and QA environments typically tolerate interruptions and do not require the continuous availability guaranteed by On-Demand capacity. Introducing Spot-backed node groups in non-production environments can significantly reduce infrastructure expenses without compromising business requirements.</p>","<p id="""">EC2 instances backing an EKS cluster are billed based on instance-hour usage. On-Demand Instances incur premium rates compared to Spot Instances, which offer steep discounts in exchange for potential interruptions. Non-production workloads can often tolerate Spot interruptions, making them ideal candidates for cost optimization.</p>","<ul id=""""><li id="""">Identify non-production EKS clusters based on environment tagging, naming conventions, or cluster metadata</li><li id="""">Review node group configurations to determine whether all nodes use On-Demand Instances</li><li id="""">Assess workload criticality and tolerance for Spot interruptions based on application requirements</li><li id="""">Analyze cluster autoscaler and provisioning policies to evaluate readiness for Spot adoption</li><li id="""">Confirm plans with development and infrastructure teams before introducing Spot-backed capacity</li></ul>","<p id="""">Provision new node groups, launch templates, or provisioners configured with Spot Instances. Migrate non-critical workloads to the Spot-backed nodes while retaining On-Demand capacity where appropriate for sensitive services. Monitor workload stability post-migration and tune autoscaling policies to optimize for cost and resilience. Update provisioning standards to prioritize Spot usage for future non-production clusters.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\&quot;"" id="""">Amazon EKS Documentation</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/spot/\"" id="""">AWS Spot Instances Overview</a></li></ul>",FALSE,FALSE,,70,,<p>AWS-Compute-6203</p>
Suboptimal Use of Provisioned Compute for Azure SQL Database,suboptimal-use-of-provisioned-compute-for-azure-sql-database-9bb83,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80df78215f797102b1d0,FALSE,FALSE,Sun Dec 14 2025 09:18:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:19 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Benjamin van der Maas,azure,azure-sql,incorrect-compute-tier-selection,databases,"<p id="""">Databases deployed on Provisioned compute incur continuous hourly charges even when workload demand is low. For databases that are active only briefly within an hour, or for limited hours per month, Serverless can provide significantly lower cost because it bills only for active compute time. The economic break-even point between Provisioned and Serverless depends on workload activity patterns. If monthly active time falls *below* the conceptual break-even range, Serverless is more cost-effective. If active time regularly exceeds that range, Provisioned may be more appropriate. This inefficiency typically appears when teams default to Provisioned compute without evaluating workload behavior over time.</p>","<p id="""">Provisioned compute is billed per vCore-hour regardless of usage. Serverless compute is billed per vCore-second plus storage, and suspends compute charges during auto-pause. Selecting Provisioned for workloads with low or sporadic utilization results in paying for unused capacity.</p>","<ul id=""""><li id="""">Review whether database activity is intermittent or consistently low throughout the day</li><li id="""">Assess whether monthly active time is substantially lower than the equivalent cost of Provisioned compute</li><li id="""">Evaluate whether workload patterns include extended idle periods suitable for auto-pause</li><li id="""">Check for development, testing, or sporadic workloads that do not require continuous compute availability</li></ul>","<ul id=""""><li id="""">Move the database to the Serverless compute tier when workloads are intermittent or have low active time</li><li id="""">Use Serverless auto-pause capabilities to eliminate compute charges during idle periods</li><li id="""">Periodically reassess compute model selection as application usage patterns evolve</li><li id="""">Apply workload profiling to ensure that Serverless performance meets application needs before migration</li></ul>",,FALSE,FALSE,,,,Azure-Databases-2345
Suboptimal Use of Search Optimization Service,suboptimal-use-of-search-optimization-service,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4c15e0cef4676c9994,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-search-optimization-service,suboptimal-configuration-and-usage,other,"<p id="""">Search Optimization can enable significant cost savings when selectively applied to workloads that heavily rely on point-lookup queries. By improving lookup efficiency, it allows smaller warehouses to satisfy performance SLAs, reducing credit consumption.</p><p id="""">However, inefficiencies arise when:</p><ul id="""">  <li id="""">Search Optimization is not enabled on critical lookup-heavy tables, forcing oversized warehouses.</li>  <li id="""">It is enabled unnecessarily on infrequently queried data, adding avoidable costs.</li>  <li id="""">Warehouse sizing is not adjusted after Search Optimization is implemented, missing the primary cost-saving opportunity.</li></ul><p id="""">Regular review of query patterns and warehouse sizing is essential to maximize the intended benefit of Search Optimization.</p>",,"<ul id="""">  <li id="""">Identify tables where Search Optimization is enabled and assess actual query usage patterns on optimized columns.</li>  <li id="""">Detect cases where Search Optimization is enabled but query volume against the indexed columns is low.</li>  <li id="""">Identify workloads that experience point-lookup query latency but operate on oversized warehouses without Search Optimization.</li>  <li id="""">Evaluate if warehouses supporting lookup-heavy workloads remain oversized despite Search Optimization being available.</li></ul>","<ul id="""">  <li id="""">Enable Search Optimization selectively on columns supporting frequent, high-value point-lookup queries</li>  <li id="""">After enabling Search Optimization, reassess and right-size warehouses where feasible.</li>  <li id="""">Remove Search Optimization from tables or columns with low query activity to eliminate unnecessary storage and maintenance costs.</li>  <li id="""">Periodically audit Search Optimization configurations against evolving workload patterns and business needs.</li></ul>",,FALSE,FALSE,,,,<p>Snowflake-Other-1052</p>
Suboptimal Use of Serverless Compute for Azure SQL Database,suboptimal-use-of-serverless-compute-for-azure-sql-database-4fe48,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e030fa76534fa1528a,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:26:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Benjamin van der Maas,azure,azure-sql,incorrect-compute-tier-selection,databases,"<p id="""">Serverless is attractive for variable or idle workloads, but it can become more expensive than Provisioned compute when database activity is high for long portions of the day. As active time increases, per-second compute accumulation approaches—or exceeds—the fixed monthly cost of a Provisioned tier. This inefficiency arises when teams adopt Serverless as a default without assessing workload patterns. Databases with steady demand, predictable traffic, or long active periods often operate more cost-effectively on Provisioned compute. The economic break-even point depends on workload activity, and when that threshold is consistently exceeded, Provisioned becomes the more efficient option.</p>","<p id="""">Serverless compute accrues cost based on active vCore-seconds, scaling with workload intensity. Provisioned compute charges a fixed hourly rate regardless of activity. For consistently active workloads, Serverless may produce higher cumulative cost than a fixed Provisioned configuration.</p>","<ul id=""""><li id="""">Review whether workload activity remains high for most hours of the day</li><li id="""">Assess whether cumulative monthly active time exceeds the conceptual break-even range for Serverless</li><li id="""">Evaluate workloads with steady or predictable utilization patterns that do not benefit from auto-pause</li><li id="""">Identify production databases where performance consistency is prioritized and continuous compute is required</li></ul>","<ul id=""""><li id="""">Move consistently active workloads from Serverless to the Provisioned compute tier to reduce cost</li><li id="""">Use workload profiling to validate that utilization patterns justify a fixed compute allocation</li><li id="""">Periodically reassess compute tier selection as application behavior evolves</li><li id="""">Ensure the chosen vCore configuration provides appropriate performance headroom after migration</li></ul>",,FALSE,FALSE,,,,Azure-Databases-6502
Suboptimal Vertex Model Type,suboptimal-vertex-model-type-90344,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e1afa63ca49ca611ac,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:23 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,gcp,gcp-vertex-ai,outdated-model-selection,ai,"<p id="""">Vertex AI model families evolve rapidly. New model versions (e.g., transitions within the Gemini family) frequently introduce improvements in efficiency, quality, and capability. When workloads continue using older, legacy, or deprecated models, they may consume more tokens, produce lower-quality results, or experience higher latency than necessary. Because generative workloads often scale quickly, even small efficiency gaps between generations can materially increase token consumption and cost. Teams that do not actively track model updates, or that set model types once and never revisit them, often miss opportunities to improve performance-per-dollar by upgrading to the most current supported model.</p>","<p id="""">Vertex AI Generative AI usage is billed per input and output token. While newer model versions may have similar per-token pricing, they often deliver more accurate outputs, require fewer tokens to achieve the same results, and provide better latency and throughput. Continuing to run older models can increase overall cost and degrade output quality.</p>","<ul id=""""><li id="""">Review Vertex AI deployments using older or deprecated model versions</li><li id="""">Assess token usage patterns to determine whether newer models deliver comparable results with fewer tokens</li><li id="""">Evaluate latency or accuracy concerns that may stem from older model behavior</li><li id="""">Check Vertex AI model lifecycle updates to confirm whether a more efficient successor model is available</li></ul>","<ul id=""""><li id="""">Migrate workloads to the latest suitable model version offering improved efficiency and performance</li><li id="""">Establish periodic review processes to ensure deployed models stay aligned with current Vertex AI offerings</li><li id="""">Incorporate model lifecycle awareness into architecture and deployment standards</li><li id="""">Validate accuracy and compatibility after upgrading to newer model versions to confirm expected benefits</li></ul>",,FALSE,FALSE,,,,GCP-AI-5949
Suboptimal Warehouse Auto-Suspend Configuration,suboptimal-warehouse-auto-suspend-configuration,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4be2c8699623007e1c,FALSE,FALSE,Thu May 22 2025 14:57:15 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-virtual-warehouse,suboptimal-configuration,compute,"<p id="""">If auto-suspend settings are too high, warehouses can sit idle and continue accruing unnecessary charges. Tightening the auto-suspend window ensures that the warehouse shuts down quickly once queries complete, minimizing credit waste while maintaining acceptable user experience (e.g., caching needs, interactive performance).</p>",,"<ul id=""""><li id="""">Review auto-suspend settings for each warehouse to identify configurations with unnecessarily high idle time thresholds (e.g., exceeding 5 minutes).</li><li id="""">Analyze query frequency and typical idle patterns to evaluate whether warehouses could be suspended sooner without materially impacting performance.</li><li id="""">Confirm whether workloads rely heavily on caching benefits for user facing workloads, which may justify slightly longer suspend timers.</li><li id="""">Validate proposed changes with application teams to ensure that faster auto-suspend does not introduce unacceptable query latency.</li></ul>","<ul id=""""><li id="""">Adjust warehouse auto-suspend settings to minimize idle billing while balancing performance needs.</li><li id="""">For batch and non-interactive workloads, consider shorter suspend intervals (e.g., around 60 seconds), recognizing that minimum billing granularity is already 60 seconds.</li><li id="""">For interactive workloads where query caching significantly improves performance, moderate suspend timers (e.g., up to 5 minutes) may be justified.</li><li id="""">Communicate cache loss implications to workload owners to ensure informed decisions about suspend timing.</li><li id="""">Periodically reassess suspend policies as workload patterns and user experience expectations evolve.</li></ul><p id="""">‍</p>",,FALSE,FALSE,,,,<p>Snowflake-Compute-7798</p>
Transactable vs. Non-Transactable Confusion in Azure Marketplace,transactable-vs-non-transactable-confusion-in-azure-marketplace,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58e7cf51faf3511bf9d,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Alexa Abbruscato,azure,azure-marketplace,commitment-misalignment,other,"<p id="""">Azure Marketplace offers two types of listings: transactable and non-transactable. Only transactable purchases contribute toward a customer’s MACC commitment. However, many teams mistakenly assume that all Marketplace spend counts, leading to missed opportunities to burn down commitments and risking budget inefficiencies. Selecting a non-transactable listing, when a transactable equivalent exists, can result in identical services being acquired at higher effective cost due to lost discounts. This confusion is exacerbated when procurement and engineering teams do not coordinate or consult Microsoft's guidance.</p>","<p id="""">Commitment-based spend (MACC); purchases that are not transactable do not contribute to MACC consumption.</p>","<ul id=""""><li id="""">Review whether forecasted Marketplace spend was intended to count toward MACC but is not reflected in actual commitment consumption</li><li id="""">Check if listings procured through Azure Marketplace are labeled as “Contact Me” or redirect to external vendor websites, indicating non-transactable offers</li><li id="""">Assess whether vendor agreements were finalized outside Azure, bypassing the Marketplace’s billing mechanism</li><li id="""">Compare purchases with Microsoft's Procurement Playbook to verify listing eligibility</li><li id="""">Analyze purchasing behavior across departments to ensure consistency in sourcing methods</li></ul>","<ul id=""""><li id="""">Prefer transactable listings in Azure Marketplace whenever MACC utilization is a priority</li><li id="""">Validate SKU eligibility against Microsoft’s Procurement Playbook or MACC eligibility lists</li><li id="""">Standardize sourcing templates and procurement workflows to explicitly document whether the offer contributes to MACC</li><li id="""">Involve both finance and technical buyers early in vendor selection to avoid misalignment</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/marketplace/marketplace-faq-purchasing#transactable-vs-non-transactable-offers"" id="""">https://learn.microsoft.com/en-us/azure/marketplace/marketplace-faq-purchasing#transactable-vs-non-transactable-offers</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/marketplace/azure-marketplace-publisher-guide/overview"" id="""">https://learn.microsoft.com/en-us/marketplace/azure-marketplace-publisher-guide/overview</a></li><li id=""""><a href=""https://www.microsoft.com/licensing/docs/view/MACC-Playbook?utm_source=chatgpt.com"" id="""">https://www.microsoft.com/licensing/docs/view/MACC-Playbook?utm_source=chatgpt.com</a></li></ul>",FALSE,FALSE,,N/A,,<p>Azure-Other-2803</p>
Unaccessed EBS Snapshot,unaccessed-ebs-snapshot,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b45df8bf2efcd0f073d,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-ebs,unused-resource,storage,"<p id="""">This inefficiency arises when snapshots are retained long after they’ve served their purpose. Snapshots may have been created for backups, migrations, or disaster recovery plans but were never deleted—even after the related workload or volume was decommissioned. Over time, these unused snapshots accumulate, continuing to incur storage costs without providing operational value.</p>","<p id="""">EBS snapshots are billed per GB-month for the total data stored. The first snapshot is a full backup; each subsequent snapshot stores only the changed blocks. However, all retained snapshots contribute to cost, since any unchanged data still referenced by newer snapshots keeps the original snapshot from being fully discarded.</p>","<p id="""">Identify snapshots that have not been used to create a new volume or AMI over a defined lookback period Review snapshot tags, descriptions, and creation context to assess whether they were part of a temporary workflow or a long-term retention strategy Check whether the associated volume or instance still exists, or if the snapshot has become orphaned Validate whether the snapshot is still required for compliance, recovery, or audit purposes</p>","<p id="""">Delete snapshots that are no longer needed or are linked to resources that no longer exist. Where retention is required but access is unlikely, document the business reason and implement lifecycle policies to automatically expire outdated snapshots. Consider tagging new snapshots with creation intent and retention timelines to support better cleanup workflows in the future.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/ebs/snapshots/pricing/\&quot;"" id="""">EBS Snapshot Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\"" id="""">Working with Snapshots</a></li></ul>",FALSE,FALSE,,9,,<p>AWS-Storage-6624</p>
Unarchived Long-Term EBS Snapshots,unarchived-long-term-ebs-snapshots,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941466be70396809f4f6,FALSE,FALSE,Sun Jun 22 2025 23:39:00 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Kyler Rupe,aws,aws-ebs,suboptimal-storage-tier,storage,"<p id="""">EBS Snapshot Archive is a lower-cost storage tier for rarely accessed snapshots retained for compliance, regulatory, or long-term backup purposes. Archiving snapshots that do not require frequent or fast retrieval can reduce snapshot storage costs by up to 75%. Despite this, many organizations retain large volumes of snapshots in the standard tier long after their operational value has expired.</p>","<p id="""">Charged per GB-month of snapshot data stored, with pricing dependent on whether the snapshot is in the standard tier or the lower-cost archive tier</p>","<ul id=""""><li id="""">Identify EBS snapshots that are more than 90 days old and have not been accessed or restored</li><li id="""">Review backup policies or compliance requirements that justify long-term retention</li><li id="""">Evaluate whether snapshots are one-time or periodic backups that are unlikely to be used operationally (e.g., end-of-project, monthly compliance snapshots)</li><li id="""">Check for large volumes of retained snapshots from now-defunct workloads or projects</li></ul>","<ul id=""""><li id="""">Archive eligible snapshots using the `ModifySnapshotTier` API, AWS CLI, or console</li><li id="""">Confirm organizational retention policies before archiving</li><li id="""">Ensure teams understand how to restore archived snapshots and the longer retrieval times involved</li><li id="""">Educate teams on snapshot lifecycle and the impact of incremental chaining for efficient archive selection</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/ebs/latest/userguide/snapshot-archive.html"" id="""">EBS Snapshot Archive Overview</a><br><a href=""https://docs.aws.amazon.com/ebs/latest/userguide/how_snapshots_work.html"" id="""">How EBS Snapshots Work</a><br><a href=""https://aws.amazon.com/ebs/snapshots/faqs/#topic-3"" id="""">EBS Snapshot Archive FAQ</a></p>",FALSE,FALSE,,,,<p>AWS-Storage-5466</p>
Unassigned Public IP Address,unassigned-public-ip-address,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4aa0cc9e0dfb3fa53b,FALSE,FALSE,Thu May 22 2025 14:57:14 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Anderson Oliveira,azure,aws-vpc,unused-resource,networking,"<p id="""">In Azure, it’s common for public IP addresses to be created as part of virtual machine or load balancer configurations. When those resources are deleted or reconfigured, the IP address may remain in the environment unassigned. While Basic SKUs are free when idle, Standard SKUs incur ongoing hourly charges, even if the address is not in use.Unassigned Standard public IPs provide no functional value but continue to generate cost, especially in environments with high churn or inconsistent cleanup practices.</p>","<ul id=""""><li id="""">Basic SKU: Free when unassigned</li><li id="""">Standard SKU: Billed hourly, regardless of assignment</li><li id="""">Pricing varies by region and IP version (IPv4 vs IPv6)</li><li id="""">Costs accrue even if the IP is not attached to any resource</li></ul>","<ul id=""""><li id="""">List all public IP addresses in the subscription</li><li id="""">Filter for IPs with Associated To status equal to - (unassigned)</li><li id="""">Check the SKU of each unassigned IP to determine if charges apply</li><li id="""">Cross-reference with recently deleted VMs or network interfaces to find residual IPs</li></ul>","<ul id=""""><li id="""">Delete unassigned Standard SKU public IPs that are no longer needed</li><li id="""">If an unassigned IP is intended for future use, consider converting it to Basic (if compatible)</li><li id="""">Incorporate IP resource cleanup into deprovisioning workflows</li><li id="""">Use Azure Policy or Resource Graph to flag unassigned public IPs for review</li></ul>","<ul id=""""><li id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/ip-addresses/\&quot;"" id="""">Azure Public IP Pricing</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/virtual-network/ip-services/public-ip-addresses\"" id="""">Public IP Address Types and SKUs</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Networking-2639</p>
Unassociated Elastic IP Address,unassociated-elastic-ip-address,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4894d3e1f351ef331f,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:50 GMT+0000 (Coordinated Universal Time),Jason Eckle,aws,aws-eip,unused-resource,networking,"<p id="""">Elastic IPs are often provisioned but forgotten — left unassociated, or still attached to EC2 instances that have been stopped. In either case, AWS treats the EIP as idle and applies an hourly charge. Although the cost per hour is relatively small, these charges accumulate quietly, especially across environments with frequent provisioning, decommissioning, or ephemeral workloads. Many organizations overlook the fact that even a single EIP attached to a stopped instance is billable. Without periodic review, this creates persistent, low-visibility waste across AWS accounts.</p>","<p id="""">Elastic IPs are billed under the following rules:</p><ul id=""""><li id="""">Free: One Elastic IP actively associated with a running EC2 instance</li><li id="""">Billed hourly:</li><li id="""">If the EIP is unassociated (not linked to any resource)</li><li id="""">If the EIP is associated with a stopped EC2 instance</li><li id="""">If there is more than one EIP per instance</li></ul><p id="""">This pricing model is designed to discourage allocation of static IP addresses that are not in active use.</p>","<p id="""">List all Elastic IPs and identify those that:</p><ul id=""""><li id="""">Are not associated with any resource</li><li id="""">Are associated with an EC2 instance that is currently stopped</li><li id="""">Are secondary EIPs beyond the first on any instance</li><li id="""">Use tags or resource metadata to validate whether these EIPs are still required for future use</li></ul>","<ul id=""""><li id="""">Release any EIPs that are no longer required</li><li id="""">Automate audits to identify unassociated or inactive EIPs on a recurring basis</li><li id="""">Update IaC templates or provisioning workflows to clean up networking assets during teardown</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/on-demand/#Elastic_IP_Addresses\&quot;"" id="""">Amazon EC2 Elastic IP Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html\"" id="""">Working with Elastic IP Addresses</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Networking-9876</p>
Unattached Block Volume (Non-Boot),unattached-block-volume-non-boot,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589417f5ef093107d4b687,FALSE,FALSE,Sun Jun 22 2025 23:39:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),,oci,oci-block-volume,orphaned-storage-resource,storage,"<p id="""">Block volumes that are not attached to any instance continue to incur charges. These often accumulate after instance deletion or reconfiguration. Unlike boot volumes, unattached data volumes may be harder to track if not labeled or tagged clearly.</p>","<p id="""">Charged per GB provisioned, regardless of attachment or usage state.</p>","<ul id=""""><li id="""">Lifecycle state is “AVAILABLE”</li><li id="""">No attached instance (attached-instance-id is null or missing)</li></ul>","<ul id=""""><li id="""">Delete unattached boot volumes that are no longer needed</li><li id="""">Establish lifecycle policies or instance termination settings that automatically delete boot volumes unless explicitly retained</li><li id="""">Periodically audit the Block Volumes service for orphaned resources</li></ul>","<p id=""""><a href=""https://docs.oracle.com/en-us/iaas/Content/Block/Tasks/managingvolumes.htm"" id="""">Managing Block Volumes</a></p>",FALSE,FALSE,,,,<p>OCI-Storage-9625</p>
Unattached Boot Volume,unattached-boot-volume,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894163c06b6092977ab4a,FALSE,FALSE,Sun Jun 22 2025 23:39:02 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Charlie Wolfe,oci,oci-block-volume,inactive-and-detached-volume,storage,"<p id="""">When a Compute instance is terminated in OCI, the associated boot volume is not deleted by default. If the termination settings don’t explicitly delete the boot volume, it persists and continues to generate storage charges. Because boot volumes are managed under the Block Volumes service, not within the Compute UI, they’re easy to overlook—especially in environments with frequent provisioning and teardown. Over time, these orphaned boot volumes can accumulate and contribute to unnecessary costs.</p>","<p id="""">OCI charges for provisioned block volume storage per GB per month, regardless of whether the volume is actively attached to a compute instance. This applies equally to boot volumes and data volumes. Detached boot volumes incur charges unless explicitly deleted.</p>","<ul id=""""><li id="""">Lifecycle state is “AVAILABLE”</li><li id="""">No attached instance (attached-instance-id is null or missing)</li></ul>","<ul id=""""><li id="""">Delete unattached boot volumes that are no longer needed</li><li id="""">Establish lifecycle policies or instance termination settings that automatically delete boot volumes unless explicitly retained</li><li id="""">Periodically audit the Block Volumes service for orphaned resources</li></ul>","<p id=""""><a href=""https://docs.oracle.com/en-us/iaas/Content/Block/Concepts/blockvolumes.htm"" id="""">OCI Block Volumes – Overview</a><br><a href=""https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/terminatinginstance.htm#termbootvol"" id="""">Terminate a Compute Instance</a></p>",FALSE,FALSE,,,,<p>OCI-Storage-8416</p>
Unconverted Convertible EC2 Reserved Instances,unconverted-convertible-ec2-reserved-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4c2afddacacaccbafa,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Jason DiDomenico,aws,aws-ec2,misconfigured-reservation,compute,"<p id="""">  Convertible Reserved Instances provide valuable pricing flexibility — but that flexibility is often underused. When EC2 workloads shift across instance families or OS types, the original RI may no longer apply to active usage. If the RI is not converted, the customer continues paying for unused commitment despite having the ability to adapt it.</p><p id="""">Because conversion is a manual process and requires matching or exceeding the original RI’s value, many organizations fail to optimize their coverage. Over time, this leads to growing pools of ineffective RIs that could have been aligned to real workloads.</p>","<ul id=""""><li id="""">Convertible RIs are billed hourly based on the original reservation terms</li><li id="""">Charges continue regardless of actual instance usage</li><li id="""">RIs must be <strong id="""">actively converted</strong> to align with changing usage — past underutilization is not refunded</li></ul>","<ul id=""""><li id="""">Identify Convertible RIs with low utilization over a trailing period</li><li id="""">Compare current EC2 usage to the instance types, families, and configurations reserved under each RI</li><li id="""">Highlight RIs that are eligible for conversion but remain mismatched to actual compute patterns</li><li id="""">Evaluate how much unused commitment could be recovered through a conversion</li></ul>","<ul id=""""><li id="""">Use the AWS Management Console or CLI to convert unused Convertible RIs to match current EC2 instance usage</li><li id="""">Ensure the new reservation configuration has equal or greater value, as required by AWS</li><li id="""">Monitor usage regularly to identify when new conversions may be needed</li><li id="""">Build RI optimization reviews into quarterly infrastructure hygiene practices to prevent recurring waste</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-modifying.html"" id="""">Modifying Convertible Reserved Instances</a></li><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/reserved-instances/"" id="""">AWS EC2 Pricing – Reserved Instances</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-5364</p>
Underuse of Fargate Spot for Interruptible Workloads,underuse-of-fargate-spot-for-interruptible-workloads,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941c7b34dce1d56e8732,FALSE,FALSE,Sun Jun 22 2025 23:39:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Andrew Shieh,aws,aws-fargate,pricing-model-misalignment,compute,"<p id="""">Many teams run workloads on standard Fargate pricing even when the workload is fault-tolerant and could tolerate interruptions. Fargate Spot provides the same performance characteristics at up to 70% lower cost, making it ideal for stateless jobs, batch processing, CI/CD runners, or retry-friendly microservices.</p>","<p id="""">Fargate is billed based on vCPU and memory usage per second. Standard Fargate pricing is significantly higher than Fargate Spot, which offers steep discounts for interruptible workloads.</p>","<ul id=""""><li id="""">Do not require continuous availability or persistent state</li><li id="""">Can tolerate retries or interruptions</li><li id="""">Are currently using standard pricing rather than Spot</li><li id="""">Run in non-production environments or are scheduled/batch in nature\\</li><li id="""">Have high “Fargate-vCPU-Hours” and “Fargate-GB-Hours” billed at on-demand rates</li></ul>","<ul id=""""><li id="""">Update ECS/EKS task definitions or profiles to enable Fargate Spot</li><li id="""">For mixed environments, use capacity providers or placement strategies to route eligible tasks to Spot</li><li id="""">Monitor interruption rates and implement retry logic where necessary</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/AmazonECS/latest/userguide/fargate-capacity-providers.html"" id="""">AWS Fargate Spot – Official Docs</a><br><a href=""https://aws.amazon.com/fargate/pricing/"" id="""">AWS Fargate Pricing</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-4468</p>
Underuse of Serverless Compute for Jobs and Notebooks,underuse-of-serverless-compute-for-jobs-and-notebooks,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f40942976408eecdf85e9,FALSE,FALSE,Thu May 22 2025 15:19:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Matt Weingarten,databricks,databricks-serverless-compute,suboptimal-execution-model,compute,"<p id="""">Databricks Serverless Compute is now available for jobs and notebooks, offering a simplified, autoscaled compute environment that eliminates cluster provisioning, reduces idle overhead, and improves Spot survivability. For short-running, bursty, or interactive workloads, Serverless can significantly reduce cost by billing only for execution time. However, Serverless is not universally available or compatible with all workload types and libraries. Organizations that exclusively rely on traditional clusters may be missing emerging opportunities to reduce spend and simplify operations by leveraging Serverless where appropriate.</p>","<p id="""">Databricks Serverless is billed based on:</p><ul id=""""><li id="""">Execution time only — costs accrue only during active job runtime</li></ul><ul id=""""><li id="""">Databricks Units (DBUs): Based on usage, abstracted from specific VM types</li><li id="""">No direct infrastructure charges — Databricks manages the underlying infrastructure</li></ul><p id="""">Compared to traditional clusters, Serverless eliminates idle charges and provisioning overhead, and can reduce total compute spend for appropriate workloads.</p>","<ul id=""""><li id="""">Query system tables to identify jobs or notebooks that run on traditional clusters despite short runtimes or high idle time</li><li id="""">Review whether Serverless Jobs Compute is enabled and available in the workspace</li><li id="""">Assess workload compatibility with Serverless requirements and feature support</li><li id="""">Analyze historical cluster usage for patterns that suggest better alignment with Serverless</li></ul>","<ul id=""""><li id="""">Pilot Serverless for eligible workloads, such as short, periodic jobs or ad-hoc notebooks</li><li id="""">Use compute policies or templates to promote Serverless adoption where appropriate</li><li id="""">Retain traditional clusters for workloads with unsupported libraries or long-lived compute patterns</li><li id="""">Monitor performance and cost impact to guide incremental rollout and inform future defaults</li></ul>","<ul id=""""><li id="""">Serverless Jobs Compute Overview</li><li id="""">Databricks Serverless Pricing</li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-7020</p>
Underuse of Serverless for Short or Interactive Workloads,underuse-of-serverless-for-short-or-interactive-workloads,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84c14ba6ddfdc23f856,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Nicole Boyd,databricks,databricks-sql,inefficient-configuration,compute,"<p id="""">Many organizations continue running short-lived or low-intensity SQL workloads — such as dashboards, exploratory queries, and BI tool integrations — on traditional clusters. This leads to idle compute, overprovisioning, and high baseline costs, especially when the clusters are always-on. Databricks SQL Serverless is optimized for bursty, interactive use cases with auto-scaling and pay-per-second pricing, making it better suited for this class of workloads. Failing to migrate to serverless for these patterns results in unnecessary cost without performance benefit.</p>","<p id="""">Serverless SQL compute is billed per second of execution time based on the size of the virtual warehouse used. Unlike full clusters, serverless compute does not incur idle time charges and automatically scales based on demand. Continuing to use traditional job or all-purpose clusters for low-throughput SQL workloads results in persistent infrastructure costs that serverless options are purpose-built to eliminate.</p>","<ul id=""""><li id="""">Identify SQL endpoints or clusters supporting dashboard or ad-hoc workloads with long idle periods between executions</li><li id="""">Review query duration and frequency — flag endpoints that are infrequently used or support low-throughput workloads</li><li id="""">Check whether teams are using dedicated clusters for interactive BI tools (e.g., Power BI, Tableau)</li><li id="""">Look for cost spikes associated with idle or underutilized SQL clusters versus expected user interaction volume</li></ul>","<ul id=""""><li id="""">Migrate lightweight SQL workloads and dashboards to Databricks SQL Serverless</li><li id="""">Enable serverless for high-concurrency, low-compute scenarios where persistent compute isn’t needed</li><li id="""">Set policies or guidelines to default to serverless for interactive workloads unless specific performance reasons require otherwise</li><li id="""">Educate data teams on the appropriate use cases for serverless versus provisioned clusters</li></ul>","<ul id=""""><li id="""">Databricks SQL Serverless Compute Overview</li><li id="""">Databricks SQL Pricing</li><li id="""">Configure SQL Warehouses</li></ul>",FALSE,FALSE,,,,<p>Databricks-Compute-1684</p>
Underutilized Azure Reserved Instance Due to Workload Drift,underutilized-azure-reserved-instance-due-to-workload-drift,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4e6e9d967771be64ab,FALSE,FALSE,Thu May 22 2025 14:57:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-reservations,commitment-misalignment,compute,"<p id="""">  As workloads evolve, Azure Reserved Instances (RIs) may no longer align with actual usage — due to refactoring, region changes, autoscaling, or instance-type drift. When this happens, the committed usage goes unused, while new workloads run on non-covered SKUs, resulting in both underutilized reservations and full-price on-demand charges elsewhere.</p><p id="""">The root inefficiency is architectural or operational drift away from what was originally committed — often due to team autonomy, poor RI governance, or legacy commitments. This leads to silent waste unless workloads are re-aligned to match existing reservations.</p>","<ul id=""""><li id="""">Azure Reservations are prepaid or monthly-commit financial instruments</li><li id="""">The discount applies only when running matching resources (e.g., same VM size and region)</li><li id="""">Underutilized reservations result in sunk cost with no usage offset</li><li id="""">Azure offers limited flexibility via instance size flexibility, re-scoping, or exchange options</li></ul>","<ul id=""""><li id="""">Identify Azure Reservations with consistently low utilization rates</li><li id="""">Determine which SKUs, regions, or resource types are covered but underused</li><li id="""">Compare current usage patterns to reservation scope (e.g., VM series, region, size)</li><li id="""">Assess whether workloads moved off the committed types due to organic evolution, performance needs, or misaligned provisioning</li></ul>","<ul id=""""><li id="""">Evaluate whether any existing workloads could be migrated to match the reservation scope</li><li id="""">For new workloads, consider provisioning on RI-covered instance types when technically viable</li><li id="""">Where appropriate, exchange the reservation for a more relevant SKU</li><li id="""">Re-scope the reservation to another subscription or region if cross-team usage can improve coverage</li><li id="""">Use RI utilization data to inform future purchasing decisions and reduce overcommit risk</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/cost-management-billing/reservations/understand-reserved-instance-usage"" id="""">Optimize reservation use with Azure usage data</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/cost-management-billing/reservations/understand-reservations"" id="""">Azure Reservations FAQ</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Compute-1935</p>
Underutilized Azure Virtual Machine,underutilized-azure-virtual-machine,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4113ea3079cef70c47,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,azure,azure-virtual-machines,overprovisioned-resource,compute,"<p id="""">Azure VMs are frequently provisioned with more vCPU and memory than needed, often based on template defaults or peak demand assumptions. When a VM operates well below its capacity for an extended period, it presents an opportunity to reduce costs through rightsizing. Without regular usage reviews, these inefficiencies can persist indefinitely.</p>","<p id="""">Azure VMs are billed based on:</p><ul id=""""><li id="""">VM size and family — Larger or more powerful VMs incur higher per-second or per-hour costs</li><li id="""">Uptime — Charged per second, with a minimum of one minute</li><li id="""">Managed disk and network usage</li></ul>","<ul id=""""><li id="""">Analyze average CPU and memory utilization of running VM’s to determine</li><li id="""">Review whether application requirements justify the current VM size</li><li id="""">Evaluate if the workload would perform similarly on a lower SKU within the same VM series</li><li id="""">Confirm that performance, licensing, or compatibility constraints don’t require oversizing</li><li id="""">Validate with stakeholders whether the VM can be resized without impact</li></ul>","<p id="""">Resize the VM to a smaller SKU that aligns with observed usage. For unpredictable workloads, consider implementing VM scale sets or burstable VM types (e.g., B-series). Periodically audit VM sizing as part of environment hygiene.</p>","<p id=""""><a href=""https://azure.microsoft.com/en-us/pricing/details/virtual-machines/"" id="""">Azure VM Pricing</a></p>",FALSE,FALSE,,114,,<p>Azure-Compute-2591</p>
Underutilized Cloud SQL Instance,underutilized-cloud-sql-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941acb772959e146eb29,FALSE,FALSE,Sun Jun 22 2025 23:39:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),,gcp,gcp-cloud-sql,underutilized-resource,databases,"<p id="""">Cloud SQL instances are often over-provisioned or left running despite low utilization. Since billing is based on allocated vCPUs, memory, and storage — not usage — any misalignment between actual workload needs and provisioned capacity leads to unnecessary spend.  Common causes include:  * Initial oversizing during launch that was never revisited   * Non-production environments with continuous uptime but minimal use   * Databases used intermittently (e.g., for nightly reports) but kept running 24/7  Without rightsizing or scheduling strategies, these instances generate ongoing cost with limited business value.</p>","<p id="""">Billed based on:  * vCPU and memory provisioned per instance-hour   * Storage provisioned (per GB-month) and IOPS for SSDs   * Backups and network egress  Charges accrue regardless of utilization levels; costs reflect provisioned capacity, not actual usage.</p>","<ul id=""""><li id="""">Review CPU and memory utilization trends over a representative period</li><li id="""">Identify instances with consistently low usage relative to provisioned specs</li><li id="""">Check for instances with uptime but no active connections or query activity</li><li id="""">Evaluate whether the environment (dev, test, QA) requires full-time availability</li><li id="""">Examine storage and IOPS usage to identify excessive disk allocation</li></ul>","<ul id=""""><li id="""">Right-size vCPU and memory allocations based on actual performance needs</li><li id="""">Schedule automatic shutdown for non-production instances during off-hours</li><li id="""">Use Cloud SQL’s stop/start capability for intermittent workloads</li><li id="""">Evaluate if the workload can be moved to a lower-cost alternative like Cloud SQL Auth Proxy \+ serverless alternatives for occasional querying</li><li id="""">Implement monitoring to trigger reviews when utilization stays below a threshold</li></ul>","<p id="""">* Cloud SQL Pricing  * View Cloud SQL Instance Metrics  * Start and Stop a Cloud SQL Instance</p>",FALSE,FALSE,,,,<p>GCP-Databases-6090</p>
Underutilized Compute Instance,underutilized-compute-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894162795fc3b6d319166,FALSE,FALSE,Sun Jun 22 2025 23:39:02 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),PointFive (ChatGPT-generated for seed coverage),oci,oci-compute-instances,underutilized-compute-resource,compute,"<p id="""">OCI Compute instances incur cost based on provisioned CPU and memory, even when the instance is lightly loaded. Instances that show consistently low usage across time, such as those used only for occasional tasks, test environments, or forgotten workloads, may be overprovisioned relative to their actual needs.</p>","<p id="""">Charged per vCPU and memory provisioned per hour, regardless of utilization.</p>","<ul id=""""><li id="""">Review instance-level CPU and memory utilization over a representative period</li><li id="""">Identify instances with consistently low activity relative to their allocated resources</li><li id="""">Evaluate whether the instance supports critical workloads or is part of a dev/test environment</li><li id="""">Assess whether rightsizing (smaller shape) or scheduled shutdown is feasible</li><li id="""">Check for seasonal or workload-specific spikes that might justify current size</li></ul>","<ul id=""""><li id="""">Rightsize the instance to a smaller shape that matches workload requirements</li><li id="""">Replace with burstable or flexible instance types where applicable</li><li id="""">Implement scheduled start/stop automation for predictable idle periods</li><li id="""">Decommission if no longer in use</li></ul>","<p id=""""><a href=""https://www.oracle.com/cloud/compute/pricing/"" id="""">OCI Compute Pricing</a><br><a href=""https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/computeinstanceviewingmetrics.htm"" id="""">Monitoring Compute Metrics</a></p>",FALSE,FALSE,,,,<p>OCI-Compute-3956</p>
Underutilized EC2 Commitment Due to Workload Drift,underutilized-ec2-commitment-due-to-workload-drift,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4f19f5ed05ed38ea79,FALSE,FALSE,Thu May 22 2025 14:57:19 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Jason DiDomenico,aws,aws-ec2,overcommitted-reservation,compute,"<p id="""">  When EC2 usage declines, shifts to different instance families, or moves to other services (e.g., containers or serverless), organizations may find that previously purchased Standard Reserved Instances or Savings Plans no longer match current workload patterns.</p><p id="""">This misalignment results in underutilized commitments—where costs are still incurred, but no usage is benefiting from the associated discounts. Since these commitments cannot be easily exchanged, refunded, or sold (except for eligible RIs on the RI Marketplace), the only viable path to recoup value is to steer workloads back toward the covered usage profile.</p>","<ul id=""""><li id=""""><strong id="""">Standard RIs</strong> are billed hourly for a specific instance type, family, region, and tenancy</li><li id=""""><strong id="""">Compute Savings Plans</strong> and <strong id="""">EC2 Instance Savings Plans</strong> offer broader flexibility, but still require a minimum hourly usage commitment</li><li id="""">If actual usage falls below the commitment threshold, the difference is still billed, resulting in unused reservation charges</li></ul>","<ul id=""""><li id="""">Identify underutilized Standard Reserved Instances or Savings Plans by analyzing usage versus commitment</li><li id="""">Break down which instance families, sizes, or regions are covered but underused</li><li id="""">Map current workloads that were previously on those instance types — and investigate why they shifted away (e.g., scaling, performance, compatibility)</li><li id="""">Look for gaps between what was committed and how workloads have evolved to highlight mismatches</li></ul>","<ul id=""""><li id="""">Review existing workloads to identify candidates that could migrate to the underutilized instance families</li><li id="""">For new or scaling workloads, prioritize launching on instance types that align with unused commitments</li><li id="""">Where possible, upgrade existing workloads to fit larger reserved types — while tracking the change to avoid overcommitment in future renewals</li><li id="""">Document any shifts made to accommodate current commitments and revisit those configurations once the reservation or plan term expires</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/ec2/pricing/reserved-instances/"" id="""">EC2 Reserved Instances</a></li><li id=""""><a href=""https://aws.amazon.com/savingsplans/"" id="""">Savings Plans</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-general.html"" id="""">Reserved Instance Marketplace</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Compute-7851</p>
Underutilized EC2 Instance,underutilized-ec2-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b40fa5354209ec4b6c2,FALSE,FALSE,Thu May 22 2025 14:57:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-ec2,overprovisioned-resource,compute,"<p id="""">EC2 instances are often overprovisioned based on rough estimates, legacy patterns, or performance buffer assumptions. If an instance consistently uses only a small fraction of its provisioned CPU or memory, it likely represents an opportunity for rightsizing. These inefficiencies persist unless usage is periodically reviewed and instance types are adjusted to align with actual workload requirements.</p>","<p id="""">EC2 instances are billed based on:</p><ul id=""""><li id="""">Instance type and size — Larger or more specialized instances incur higher hourly or per-second costs</li><li id="""">Uptime — Charged per second with a one-minute minimum (Linux); hourly for Windows or certain older usage types</li><li id="""">Storage and data transfer</li></ul>","<ul id=""""><li id="""">Review average CPU and memory utilization of running EC2 instances.</li><li id="""">Determine whether actual usage justifies the selected instance type or size</li><li id="""">Confirm whether performance buffers, licensing rules, or other constraints require overprovisioning</li><li id="""">Check whether the workload could be better served with burstable instances (e.g., T family) or smaller sizes within the same family</li><li id="""">Validate with application owners whether the instance can be safely resized</li></ul>","<p id="""">Resize the EC2 instance to a smaller type that better matches observed usage. Consider using burstable instances for smaller workloads with spiky or inconsistent usage. Review Auto Scaling policies to allow dynamic adjustment when necessary.</p>","<p id=""""><a href=""https://aws.amazon.com/ec2/pricing/"" id="""">AWS EC2 Pricing</a></p>",FALSE,FALSE,,14,,<p>AWS-Compute-5349</p>
Underutilized ElastiCache Node,underutilized-elasticache-node,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b498040f49223e29055,FALSE,FALSE,Thu May 22 2025 14:57:13 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Yulia Perlis,aws,aws-elasticache,overprovisioned-resource,databases,"<p id="""">ElastiCache clusters are often sized for peak performance or reliability assumptions that no longer reflect current workload needs. When memory and CPU usage remain consistently low, the node is likely overprovisioned. For Redis, memory is typically the primary sizing constraint, while Memcached workloads may be more CPU-sensitive. In dev, staging, or lightly used production environments, some nodes may be entirely idle.It's important to evaluate usage patterns in context — for example, replica nodes in Redis Multi-AZ configurations may show low utilization by design, but still serve a high-availability purpose. However, in non-critical environments or where HA is not required, those nodes can often be downsized or removed. Additionally, older ElastiCache instance types (e.g., r4, m3) are frequently less cost-efficient than newer generations like r6g or r7g, offering further savings through modernization.</p>","<p id="""">ElastiCache nodes are billed per instance-hour, based on:</p><ul id=""""><li id="""">Instance type and size (e.g., cache.r6g.large)</li><li id="""">Engine type (Redis or Memcached)</li></ul><p id="""">Charges apply continuously while provisioned, regardless of usage or traffic.</p>","<ul id=""""><li id="""">Review utilization metrics for each ElastiCache node:</li><li id="""">Memory usage for Redis</li><li id="""">CPU usage for Memcached and write-heavy Redis</li><li id="""">Identify nodes with consistently low utilization (e.g., &lt;30%) over a representative period</li><li id="""">Exclude replica nodes in HA configurations unless running in non-critical environments</li><li id="""">Correlate utilization with environment type and business function</li></ul>","<ul id=""""><li id="""">Rightsize nodes to smaller instance types that align with observed usage</li><li id="""">Modernize to newer instance families when possible to improve price-performance</li><li id="""">Remove idle or redundant nodes in dev, staging, or non-HA environments</li><li id="""">Incorporate cache sizing reviews into routine infrastructure audits</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/elasticache/pricing/\&quot;"" id="""">Amazon ElastiCache Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/metrics.html\"" id="""">Monitoring ElastiCache with CloudWatch</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Databases-5361</p>
Underutilized GCP VM Instance,underutilized-gcp-vm-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b41e37ebe2e81662f5d,FALSE,FALSE,Thu May 22 2025 14:57:05 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,gcp,gcp-compute-engine,overprovisioned-resource,compute,"<p id="""">GCP VM instances are often provisioned with more CPU or memory than needed, especially when using custom machine types or legacy templates. If an instance consistently consumes only a small portion of its allocated resources, it likely represents an opportunity to reduce costs through rightsizing. Without proactive reviews, these oversized instances can remain unnoticed and continue to incur unnecessary charges.</p>","<p id="""">Compute Engine VM instances are billed based on:</p><ul id=""""><li id="""">Machine type and configuration — Cost scales with the number of vCPUs, memory, and any premium features (e.g., GPUs, local SSDs)</li><li id="""">Uptime — Charged per second, with a one-minute minimum</li><li id="""">Persistent disk and network usage</li></ul>","<ul id=""""><li id="""">Analyze average CPU and memory utilization of running Compute Engine instance</li><li id="""">Determine whether actual usage justifies the current machine type or custom configuration</li><li id="""">Review whether the workload could be met using a smaller predefined or custom machine type</li><li id="""">Check for constraints such as licensing, startup latency, or performance overhead that may require overprovisioning</li><li id="""">Validate with application or infrastructure teams whether the instance can be resized without impact</li></ul>","<p id="""">Resize the instance to a smaller predefined or custom machine type that better aligns with observed usage. For elastic workloads, consider deploying managed instance groups with autoscaling. Periodically review usage patterns to ensure continued alignment with resource needs.</p>","<p id=""""><a href=""https://cloud.google.com/compute/all-pricing?hl=en"" id="""">GCP Compute Engine Pricing</a></p>",FALSE,FALSE,,114,,<p>GCP-Compute-8681</p>
Underutilized Instances in EC2 Auto Scaling Group,underutilized-instances-in-ec2-auto-scaling-group,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b45749c1d196f30fda9,FALSE,FALSE,Thu May 22 2025 14:57:09 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-ec2,underutilized-resource,compute,"<p id="""">Oversized instances within Auto Scaling Groups lead to inflated baseline costs, even when scaling adjusts the number of instances dynamically. When workloads consistently use only a fraction of the available CPU, memory, or network capacity, there is an opportunity to downsize to smaller, less expensive instance types without sacrificing performance. Right-sizing helps balance capacity and efficiency, reducing compute spend while preserving workload stability.</p><p id=""""><strong id="""">Detection:</strong></p><ul id="""">  <li id="""">Identify Auto Scaling Groups where instances exhibit low average CPU, memory, or network utilization relative to their capacity.</li>  <li id="""">Review instance sizing in relation to historical workload peaks and scaling behavior.</li>  <li id="""">Assess whether smaller, more cost-effective instance types could support the same workload with acceptable performance.</li>  <li id="""">Evaluate launch configurations or templates to determine if default instance types were selected without performance optimization.</li>  <li id="""">Confirm with application and infrastructure owners that resizing aligns with performance, availability, and SLA requirements.</li></ul>","<p id="""">EC2 instances are billed per second based on instance type, operating system, and purchase model (e.g., On-Demand, Reserved Instances, Spot Instances). Larger instance types incur higher charges regardless of whether the workload fully utilizes the available resources.</p>","<ul id=""""><li id="""">Identify Auto Scaling Groups where instances exhibit low average CPU, memory, or network utilization relative to their capacity.</li><li id="""">Review instance sizing in relation to historical workload peaks and scaling behavior.</li><li id="""">Assess whether smaller, more cost-effective instance types could support the same workload with acceptable performance.</li><li id="""">Evaluate launch configurations or templates to determine if default instance types were selected without performance optimization.</li><li id="""">Confirm with application and infrastructure owners that resizing aligns with performance, availability, and SLA requirements</li></ul>","<ul id=""""><li id="""">Evaluate smaller instance types that better match the workload’s actual resource requirements.</li><li id="""">Update the launch template or configuration for the Auto Scaling Group to use the selected instance type, and deploy changes during a low-traffic window if needed.</li><li id="""">After downsizing, monitor performance metrics to ensure the workload continues to meet application and SLA expectations.</li><li id="""">Regularly reassess instance sizing as usage patterns evolve to maintain ongoing cost efficiency.</li></ul>",,FALSE,FALSE,,35,,<p>AWS-Compute-2207</p>
Underutilized Kubernetes Workload,underutilized-kubernetes-workload,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f409403447ba494fb6a9f,FALSE,FALSE,Thu May 22 2025 15:19:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),,aws,aws-eks,underutilization,compute,"<p id="""">When Kubernetes workloads request more CPU and memory than they actually consume, nodes must reserve capacity that remains unused. This leads to lower node density, forcing the cluster to maintain more instances than necessary. Aligning resource requests with observed utilization improves cluster efficiency and reduces compute spend without sacrificing application performance.</p>","<p id="""">EKS control planes are billed per hour, while compute nodes are billed based on EC2 or Fargate capacity. Kubernetes resource ""requests"" for CPU and memory influence scheduling decisions and cluster resource allocation. Overprovisioned requests lead to inefficient node usage and inflated infrastructure costs.</p>","<ul id=""""><li id="""">Identify workloads where average CPU and memory usage are consistently much lower than requested values</li><li id="""">Analyze container-level metrics to assess request-to-usage ratios over time</li><li id="""">Leverage Vertical Pod Autoscaler recommendations, if available, to identify right-sizing opportunities</li><li id="""">Evaluate whether overprovisioning was intentional for specific performance or reliability requirements</li><li id="""">Validate proposed adjustments with application owners or SRE teams to ensure they meet operational needs</li></ul>","<p id="""">Update the CPU and memory requests for underutilized workloads to better match observed usage patterns. Apply changes through updated Kubernetes manifests or infrastructure-as-code pipelines. Monitor workloads after adjustment to confirm that performance and reliability remain within acceptable thresholds. Establish regular right-sizing reviews to keep resource allocations efficient as workloads evolve.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\&quot;"" id="""">Amazon EKS Documentation</a></li><li id="""">Kubernetes Resource Management</li></ul>",TRUE,FALSE,,,,<p>AWS-Compute-4579</p>
Underutilized PTU Quota for Azure OpenAI Deployments,underutilized-ptu-quota-for-azure-openai-deployments-da831,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e0ead4cde5fe938a43,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:16 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),Ariel Lichterman,azure,azure-cognitive-services,overprovisioned-capacity-allocation,ai,"<p id="""">When organizations size PTU capacity based on peak expectations or early traffic projections, they often end up with more throughput than regularly required. If real-world usage plateaus below provisioned levels, a portion of the PTU capacity remains idle but still generates full spend each hour. This is especially common shortly after production launch or during adoption of newer GPT-4 class models, where early conservative sizing leads to long-term over-allocation. Rightsizing PTUs based on observed usage patterns ensures that capacity matches actual demand.</p>","<p id="""">PTU pricing is based on the number of provisioned throughput units, not actual usage. Underutilized PTUs still incur full hourly charges, making over-allocation a direct source of avoidable cost.</p>","<ul id=""""><li id="""">Review PTU deployments for consistently low or flat throughput utilization over representative time periods</li><li id="""">Compare provisioned PTU levels against actual workload demand to identify idle capacity</li><li id="""">Identify deployments sized for initial peak estimates that no longer match steady-state usage</li><li id="""">Evaluate whether recent model or workload changes have altered throughput requirements</li></ul>","<ul id=""""><li id="""">Reduce PTU allocations to align with actual utilization while preserving required performance levels</li><li id="""">Implement recurring rightsizing reviews to adjust PTU levels as workload patterns evolve</li><li id="""">Use workload performance testing to validate that reduced capacity meets latency and throughput goals</li><li id="""">Consider shifting variable or declining workloads from PTUs to PAYG where appropriate</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput"" id="""">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-throughput</a></li></ul>",FALSE,FALSE,,188,,Azure-AI-9295
Underutilized Provisioned IOPS on an EBS Volume,underutilized-provisioned-iops-on-an-ebs-volume,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b43866414fe44b7291b,FALSE,FALSE,Thu May 22 2025 14:57:07 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-ebs,overprovisioned-resource,storage,"<p id="""">This inefficiency occurs when an EBS volume has provisioned IOPS levels that consistently exceed the actual I/O requirements of the workload it supports. This can happen when performance buffers are estimated too high, usage patterns change over time, or default settings are left unadjusted. Provisioned IOPS above the included baseline generate ongoing charges that may not reflect actual utilization, resulting in avoidable cost.</p>","<p id="""">EBS volumes with configurable IOPS are billed based on both provisioned storage (per GB-month) and provisioned IOPS (per IOPS-month). Most volume types include a baseline amount of IOPS, and additional IOPS are billed separately—regardless of whether they are used. Charges accumulate continuously for any overprovisioned IOPS that exceed actual workload demand.</p>","<ul id=""""><li id="""">Review actual IOPS usage over a representative time window (e.g., 14–30 days)</li><li id="""">Compare provisioned IOPS to peak and average demand to assess excess capacity</li><li id="""">Confirm whether performance requirements or bursty workloads justify the current configuration</li><li id="""">Validate with workload owners whether the IOPS level can be reduced without affecting performance</li></ul>","<p id="""">Reduce the provisioned IOPS to a level more consistent with observed usage. Ensure that baseline performance remains sufficient for the workload’s needs, and consider whether standard burstable performance is acceptable. If needed, reconfigure both IOPS and throughput settings to optimize cost-performance alignment. Reassess regularly to avoid regression.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\&quot;"" id="""">EBS Volume Types</a></li><li id=""""><a href=""https://aws.amazon.com/ebs/pricing/\"" id="""">EBS Pricing</a></li></ul>",FALSE,TRUE,,126,,<p>AWS-Storage-4756</p>
Underutilized RDS Commitment Due to Workload Drift,underutilized-rds-commitment-due-to-workload-drift,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4c1cb2a508e16a4b36,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Jason DiDomenico,aws,aws-rds,overcommitted-reservation,databases,"<p id="""">  RDS workloads often evolve — changing engine types, rightsizing instances, or shifting to Aurora or serverless models. When these changes occur after Reserved Instances have been purchased, the existing commitments may no longer match active usage. This results in silent overspend, as underutilized RIs continue billing without offsetting usage.</p><p id="""">Unlike Convertible EC2 RIs, RDS RIs cannot be exchanged. Selling unused RDS RIs is not supported. In rare cases, AWS Support may approve a goodwill adjustment, but this is not guaranteed. The most effective way to recover value is to steer eligible workloads back toward the reserved configuration.</p>","<ul id=""""><li id="""">Standard RDS Reserved Instances are billed hourly regardless of instance usage</li><li id="""">RIs are tied to specific configurations and cannot be modified once purchased</li><li id="""">If RDS usage patterns change or migrate away from covered configurations, the RI continues to accrue cost without providing benefit</li></ul>","<ul id=""""><li id="""">Identify underutilized RDS Reserved Instances using billing and utilization data</li><li id="""">Break down the instance classes, engines, and regions with low or zero usage</li><li id="""">Trace whether existing workloads have moved off those configurations — and why</li><li id="""">Quantify the cost of unused commitment to prioritize remediation</li></ul>","<ul id=""""><li id="""">Evaluate whether current or new workloads can run on the reserved instance types</li><li id="""">Prioritize launching new RDS instances that align with the unused commitment</li><li id="""">Where feasible, upgrade or shift workloads to covered instance classes while monitoring performance and future fit</li><li id="""">Document changes made to consume the commitment and reassess once the reservation expires</li><li id="""">In rare cases, contact AWS Support to request a reservation adjustment — note that approval depends on RI type, remaining term, and value parity</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/rds/reserved-instances/"" id="""">AWS RDS Reserved Instances</a></li><li id=""""><a href=""https://aws.amazon.com/rds/pricing/"" id="""">RDS Pricing</a></li></ul>",FALSE,TRUE,,,,<p>AWS-Databases-8986</p>
Underutilized RDS Instance,underutilized-rds-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4435c8723bd4347cbd,FALSE,FALSE,Thu May 22 2025 14:57:08 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:45 GMT+0000 (Coordinated Universal Time),,aws,aws-rds,overprovisioned-resource,databases,"<p id="""">This inefficiency occurs when an RDS instance is consistently operating below its provisioned capacity—for example, showing low CPU, or memory utilization over an extended period. This often results from conservative initial sizing, decreased workload demand, or failure to review and adjust after deployment. Running oversized RDS instances leads to unnecessary compute and licensing costs without delivering additional value.</p>","<p id="""">RDS instances are billed by the hour (or per second in some cases) based on the instance class (vCPU and memory), storage, and any additional features such as Multi-AZ deployments or Provisioned IOPS. Charges accrue continuously while the instance is running, regardless of workload activity or utilization.</p>","<ul id=""""><li id="""">Identify RDS instances with consistently low CPU and memory usage over a representative time window</li><li id="""">Compare observed performance to the instance class’s capabilities to assess overprovisioning</li><li id="""">Evaluate whether Auto Scaling is disabled or not configured for compute resizing</li><li id="""">Check if the instance is supporting low-demand, legacy, or test workloads</li><li id="""">Confirm with application owners whether the current instance size is still necessary based on performance requirements</li></ul>","<p id="""">Switch to a smaller, more cost-effective instance class that aligns with actual usage patterns. Use performance data to inform rightsizing decisions. For workloads with variable usage, consider enabling instance Auto Scaling (where supported) or migrating to Aurora Serverless if appropriate. Monitor post-migration performance to ensure service levels are maintained.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html\&quot;"" id="""">RDS Instance Classes</a></li><li id=""""><a href=""https://aws.amazon.com/rds/pricing/\"" id="""">RDS Pricing</a></li></ul>",FALSE,FALSE,,40,,<p>AWS-Databases-5711</p>
Underutilized Read Capacity on a DynamoDB Table,underutilized-read-capacity-on-a-dynamodb-table,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f409418a896df49e5d054,FALSE,FALSE,Thu May 22 2025 15:19:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),,aws,aws-dynamodb,underutilization,databases,"<p id="""">Provisioned capacity mode is appropriate for workloads with consistent or predictable throughput. However, when read capacity is significantly over-provisioned relative to actual usage, it results in wasted spend. This inefficiency is especially common in dev/test environments, legacy systems, or workloads that have tapered off over time but were never adjusted.</p>","<p id="""">In Provisioned Capacity Mode, DynamoDB charges hourly for:</p><ul id=""""><li id="""">Read Capacity Units (RCUs) and Write Capacity Units (WCUs) based on the provisioned level, not actual consumption</li><li id="""">Additional services such as backups, streams, and global tables also generate usage-based charges</li></ul><p id="""">Read and write capacity is billed continuously, regardless of whether it is fully used. Over-provisioning RCUs or WCUs leads to avoidable cost.</p>","<ul id=""""><li id="""">Identify DynamoDB tables using Provisioned capacity mode</li><li id="""">Review utilization history to assess average read throughput</li><li id="""">Determine whether actual read usage is consistently below the provisioned capacity</li><li id="""">Check for the absence of throttling or bursty usage patterns that would require excess headroom</li><li id="""">Evaluate whether Auto Scaling is enabled and if current minimum thresholds are still appropriate</li><li id="""">Confirm the table's environment (e.g., production vs. test) and business criticality</li><li id="""">Engage with workload owners to validate whether capacity can be safely reduced or if On-Demand is more appropriate</li></ul>","<p id="""">Reduce provisioned read capacity to better match observed usage, either manually or by tuning Auto Scaling settings. For workloads with highly variable or low average usage, consider switching to On-Demand mode, which offers pay-per-request pricing and removes the need to manage throughput provisioning.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CostOptimization_TableCapacityMode.html\&quot;"" id="""">AWS DynamoDB Cost Optimization Guide</a></li><li id=""""><a href=""https://aws.amazon.com/dynamodb/pricing/\"" id="""">AWS DynamoDB Pricing</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Databases-9307</p>
Underutilized Snowflake Warehouse,underutilized-snowflake-warehouse,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4c836265348cf4c910,FALSE,FALSE,Thu May 22 2025 14:57:16 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Simar Arora,snowflake,snowflake-virtual-warehouse,underutilized-resource,compute,"<p id="""">Underutilized Snowflake warehouses occur when a workload is assigned a larger warehouse size than necessary. For example, a workload that could efficiently execute on a Medium (M) warehouse may be running on a Large (L) or Extra Large (XL) warehouse.This leads to unnecessary credit consumption without a proportional benefit to performance. Underutilization is often driven by early provisioning decisions that were not later reassessed, or by a desire for marginal speed improvements that do not justify the increased operational cost.</p>","<p id="""">Snowflake charges based on the size and active runtime of a warehouse. Larger warehouses consume more credits per second of operation compared to smaller sizes, and relevant size should be used depending on workload requirements.</p>","<ul id=""""><li id="""">Analyze historical query activity for each warehouse to assess typical concurrency levels and peak usage patterns</li><li id="""">Evaluate whether workloads experience query queuing or whether the warehouse is frequently underloaded</li><li id="""">Review warehouse credit consumption relative to query volume and complexity over a representative time period</li><li id="""">Confirm with data engineering or application teams whether the warehouse can be downsized without affecting SLAs or performance requirements</li></ul>","<ul id=""""><li id="""">Right-size the Snowflake warehouse by selecting a smaller size (e.g., from L to M, or M to S) that adequately supports workload performance and concurrency needs.</li> <li id="""">Implement a periodic review process to reassess warehouse sizing based on observed usage patterns and changes in workload requirements</li><li id="""">Coordinate with business and engineering teams to validate any SLA requirements before resizing</li></ul>","<ul id=""""><li id=""""><a href=""https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf"" id="""">Snowflake Credit Consumption Table</a></li><li id=""""><a href=""https://docs.snowflake.com/en/sql-reference/functions/warehouse_load_history"" id="""">Warehouse Load History</a></li></ul>",TRUE,FALSE,,,,<p>Snowflake-Compute-4130</p>
Underutilized VM Commitments Due to Architectural Drift,underutilized-vm-commitments-due-to-architectural-drift,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589418f5ef093107d4b6eb,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:49 GMT+0000 (Coordinated Universal Time),Diana Lezcano,gcp,gcp-compute-engine,underutilized-commitment,compute,"<p id="""">VM-based Committed Use Discounts in GCP offer cost savings for predictable workloads, but they are rigid: they apply only to specified VM types, quantities, and regions. When organizations evolve their architecture — such as moving to GKE (Kubernetes), Cloud Run, or autoscaling — usage patterns often shift away from the original commitments. Because GCP lacks flexible reallocation options like AWS Convertible RIs or Savings Plans, underutilized commitments lead to sustained, silent waste. This is especially common when workload changes go uncoordinated with finance or centralized planning.</p>","<p id="""">CUDs are billed based on committed resource quantities (e.g., vCPUs, memory) for specific VM families and regions, regardless of actual usage. If consumption falls below the commitment level, the unused portion is still billed, generating cost with no value returned.</p>","<ul id=""""><li id="""">Review actual usage against committed VM types and regions across a representative time period</li><li id="""">Identify consistent underutilization of CUDs based on usage-to-commit ratio</li><li id="""">Confirm whether architectural shifts (e.g., to GKE, Cloud Run, or autoscaling groups) have occurred</li><li id="""">Check for commitments applied to workloads that have been decommissioned or scaled down</li><li id="""">Evaluate whether commitments are isolated to specific projects, regions, or teams without visibility across the org</li></ul>","<ul id=""""><li id="""">Consolidate workloads onto committed VM types where feasible</li><li id="""">Avoid renewing commitments for workloads that are scaling down or migrating</li><li id="""">Use Resource-based CUDs when architectural flexibility is needed</li><li id="""">Implement cross-functional governance for forecasting and commitment approvals</li><li id="""">Align future commitments with current and expected usage patterns, not historical ones</li></ul>",,FALSE,FALSE,,,,<p>GCP-Compute-9150</p>
Underutilized Write Capacity on a DynamoDB Table,underutilized-write-capacity-on-a-dynamodb-table,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f40947db021f413ae2457,FALSE,FALSE,Thu May 22 2025 15:19:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:54 GMT+0000 (Coordinated Universal Time),,aws,aws-dynamodb,underutilization,databases,"<p id="""">Provisioned capacity mode is appropriate for workloads with consistent or predictable throughput. However, when write capacity is significantly over-provisioned relative to actual usage, it results in wasted spend. This inefficiency is especially common in dev/test environments, legacy systems, or workloads that have tapered off over time but were never adjusted.</p>","<p id="""">In Provisioned Capacity Mode, DynamoDB charges hourly for:</p><ul id=""""><li id="""">Write Capacity Units (WCUs) and Read Capacity Units (RCUs) based on the provisioned level, not actual consumption</li><li id="""">Additional services such as backups, streams, and global tables also generate usage-based charges</li></ul><p id="""">Write and read capacity is billed continuously, regardless of whether it is fully used. Over-provisioning WCUs or RCUs leads to avoidable cost.</p>","<ul id=""""><li id="""">Identify DynamoDB tables using Provisioned capacity mode</li><li id="""">Review utilization history over a 14-day or longer period to assess average write throughput</li><li id="""">Determine whether actual write usage is consistently below the provisioned capacity</li><li id="""">Check for the absence of throttling or bursty usage patterns that would require excess headroom</li><li id="""">Evaluate whether Auto Scaling is enabled and if current minimum thresholds are still appropriate</li><li id="""">Confirm the table's environment (e.g., production vs. test) and business criticality</li><li id="""">Engage with workload owners to validate whether capacity can be safely reduced or if On-Demand is more appropriate</li></ul>","<p id="""">Reduce provisioned write capacity to better match observed usage, either manually or by tuning Auto Scaling settings. For workloads with highly variable or low average usage, consider switching to On-Demand mode, which offers pay-per-request pricing and removes the need to manage throughput provisioning.</p>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CostOptimization_TableCapacityMode.html\&quot;"" id="""">AWS DynamoDB Cost Optimization Guide</a></li><li id=""""><a href=""https://aws.amazon.com/dynamodb/pricing/\"" id="""">AWS DynamoDB Pricing</a></li></ul>",TRUE,FALSE,,,,<p>AWS-Databases-7946</p>
Underutilized or Overprovisioned AppStream Instances,underutilized-or-overprovisioned-appstream-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941521566de3b40895ee,FALSE,FALSE,Sun Jun 22 2025 23:39:01 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Jason DiDomenico,aws,aws-appstream-2-0,underutilization,compute,"<p id="""">AppStream fleets often default to instance types designed for worst-case or peak usage scenarios, even when average workloads are significantly lighter. This leads to consistently low utilization of CPU, memory, or GPU resources and inflated infrastructure costs. By right-sizing AppStream instances based on actual workload needs, organizations can reduce spend without compromising user experience.</p>","<p id="""">AppStream 2.0 streaming instances are billed per hour based on the selected instance type, with higher-cost options providing more CPU, memory, and GPU resources. If instance types are overprovisioned relative to actual usage, customers pay for unused capacity, driving unnecessary spend.</p>","<ul id=""""><li id="""">Identify AppStream 2.0 fleets or stacks with consistently low CPU, memory, or GPU utilization over a representative period</li><li id="""">Review instance type specifications in relation to average and peak workload demands</li><li id="""">Conduct performance benchmarking or usage sampling for specific applications or user groups</li><li id="""">Check whether current instance types were selected based on assumption or inherited defaults rather than workload data</li></ul>","<ul id=""""><li id="""">Right-size AppStream fleets by selecting smaller or less specialized instance types that meet current workload demands</li><li id="""">Conduct performance testing after downgrading to ensure that application responsiveness and user experience are preserved</li><li id="""">Update provisioning templates or fleet configurations to reflect optimized instance types going forward</li></ul>","<p id=""""><a href=""https://docs.aws.amazon.com/appstream2/latest/developerguide/instance-types.html"" id="""">Amazon AppStream 2.0 Fleet and Instance Types</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-8856</p>
Unexpired Non-Current Object Versions in S3,unexpired-non-current-object-versions-in-s3,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58fafa444c6b4ca958c,FALSE,FALSE,Mon Nov 03 2025 16:17:19 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:46:00 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:46:00 GMT+0000 (Coordinated Universal Time),Brendan McFarland,aws,aws-s3,missing-lifecycle-policy,storage,"<p id="""">When S3 versioning is enabled but no lifecycle rules are defined for non-current objects, outdated versions accumulate indefinitely. These non-current versions are rarely accessed but continue to incur storage charges. Over time, this leads to significant hidden costs, particularly in buckets with frequent object updates or automated data pipelines. Proper lifecycle management is required to limit or expire obsolete versions.</p>","<p id="""">S3 charges for all object versions retained in a bucket, including non-current versions. Each version consumes full storage capacity and accrues charges until explicitly deleted or transitioned by a lifecycle policy.</p>","<ul id=""""><li id="""">Review S3 buckets with versioning enabled to determine if lifecycle policies are applied to manage non-current versions</li><li id="""">Assess storage growth patterns and identify buckets where non-current versions represent a large share of total usage</li><li id="""">Confirm whether retained non-current versions are actively used or required for compliance purposes</li><li id="""">Evaluate consistency of versioning and lifecycle policy practices across production and non-production buckets</li></ul>","<ul id=""""><li id="""">Implement lifecycle policies to expire or transition non-current versions after an appropriate retention period</li><li id="""">Tailor policies by bucket or prefix to align with business, compliance, or recovery requirements</li><li id="""">Retain only the number of historical versions necessary for recovery or auditing; remove excess versions automatically</li><li id="""">Periodically review lifecycle rules to ensure alignment with evolving requirements and prevent reaccumulation</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html"" id="""">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-expire-general-considerations.html"" id="""">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-expire-general-considerations.html</a></li></ul>",FALSE,FALSE,,N/A,,<p>AWS-Storage-1232</p>
Unfiltered Recording of High-Churn Resource Types in AWS Config,unfiltered-recording-of-high-churn-resource-types-in-aws-config,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68b0d8fec0c02157930ebe1b,FALSE,FALSE,Thu Aug 28 2025 22:32:30 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Dor Danosh,aws,aws-config,,other,"<p id="""">By default, AWS Config can be set to record changes across all supported resource types, including those that change frequently, such as security group rules, IAM role policies, route tables, or network interfaces frequent ephemeral resources in containerized or auto-scaling setupsThese high-churn resources can generate an outsized number of configuration items and inflate costs — especially in dynamic or large-scale environments.</p><p id="""">This inefficiency arises when recording is enabled indiscriminately across all resources without evaluating whether the data is necessary. Without targeted scoping, teams may incur large charges for configuration data that provides minimal value, especially in non-production environments.This can also obscure meaningful compliance signals by introducing noise</p>","<p id="""">AWS Config charges based on:</p><p id="""">The number of configuration items recorded per resource</p><p id="""">The number of conformance pack evaluations</p><p id="""">The number of rule evaluations</p><p id="""">Recording costs scale with the volume of configuration changes, which varies significantly by resource type and environment volatility. This means billing can spike unexpectedly in dynamic environments. High-churn resources can generate large volumes of configuration items even if they are not relevant to compliance goals.</p>","<ul id=""""><li id="""">Identify whether AWS Config is recording all resource types across accounts or regions</li><li id="""">Analyze configuration item volumes by resource type to pinpoint high-frequency generators</li><li id="""">Review whether these high-churn resources are required for audit, compliance, or operational needs</li><li id="""">Check for AWS Config usage in non-production environments that may not require persistent tracking</li><li id="""">Evaluate if alternate tools (e.g., CloudTrail, VPC Flow Logs, GuardDuty) already provide similar insight for the same resources</li><li id="""">Compare Config data to findings from CloudTrail, VPC Flow Logs, or GuardDuty to detect redundant coverage</li></ul>","<ul id=""""><li id="""">Limit AWS Config recording to only essential resource types using resource recording groups</li><li id="""">Exclude high-churn resource types that provide minimal compliance or operational value</li><li id="""">Disable Config entirely in sandbox, test, or dev accounts if configuration history is not needed</li><li id="""">Regularly review and update recording scopes as infrastructure evolvesAutomating these reviews with periodic reports or Config’s advanced queries</li></ul>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/config/pricing/"" id="""">https://aws.amazon.com/config/pricing/</a></li><li id=""""><a href=""https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html"" id="""">https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/config/latest/developerguide/select-resources.html"" id="""">https://docs.aws.amazon.com/config/latest/developerguide/select-resources.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/config/latest/developerguide/view-config-items.html"" id="""">https://docs.aws.amazon.com/config/latest/developerguide/view-config-items.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html"" id="""">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/config/latest/developerguide/stop-start-recording.html"" id="""">https://docs.aws.amazon.com/config/latest/developerguide/stop-start-recording.html</a></li><li id=""""><a href=""https://docs.aws.amazon.com/config/latest/developerguide/querying-AWS-Resources.html"" id="""">https://docs.aws.amazon.com/config/latest/developerguide/querying-AWS-Resources.html</a></li></ul>",FALSE,FALSE,,,,<p>AWS-Other-1153</p>
Unmanaged Growth of Athena Query Output Buckets,unmanaged-growth-of-athena-query-output-buckets,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58e390482ddc13e45f9,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Abdeldjallil Koutchoukali,aws,aws-athena,missing-lifecycle-policy,compute,"<p id="""">Athena generates a new S3 object for every query result, regardless of whether the output is needed long term. Over time, this leads to uncontrolled growth of the output bucket, especially in environments with repetitive queries such as cost and usage reporting. Many of these files are transient and provide little value once the query is consumed. Without lifecycle rules, organizations pay for unnecessary storage and create clutter in S3.</p>","<p id="""">Athena query execution is billed per terabyte of data scanned, but query results are stored in S3 and billed according to S3 storage pricing. Each executed query produces an object in the output bucket, and costs accumulate as these objects persist over time without automated cleanup.</p>","<ul id=""""><li id="""">Review whether Athena query output buckets have lifecycle rules configured to delete or transition old objects</li><li id="""">Assess growth trends in the S3 bucket size used for Athena outputs relative to actual business need</li><li id="""">Check for repetitive or automated queries (e.g., CUR queries) that generate large volumes of transient results</li><li id="""">Confirm whether audit, compliance, or reporting requirements justify long-term retention of certain outputs</li></ul>","<ul id=""""><li id="""">Implement S3 Lifecycle Policies on Athena output buckets to automatically expire objects after a set period (e.g., 30, 60, 90 days)</li><li id="""">Use prefixes or tags to differentiate between temporary query outputs and long-term reports, applying tailored retention rules</li><li id="""">Regularly review and adjust retention policies to balance cost efficiency with business and compliance needs</li></ul>",,FALSE,FALSE,,N/A,,<p>AWS-Compute-1877</p>
Unnecessarily High Recording Granularity in AWS Config,unnecessarily-high-recording-granularity-in-aws-config,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58db3d45246ca0cb618,FALSE,FALSE,Mon Nov 03 2025 16:17:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:51 GMT+0000 (Coordinated Universal Time),Tom Cross,aws,aws-config,suboptimal-recording-configuration,other,"<p id="""">Organizations frequently inherit continuous recording by default (e.g., through landing zones) without validating the business need for per-change granularity across all resource types and environments. In change-heavy accounts (ephemeral resources, CI/CD churn, autoscaling), continuous mode drives very high CIR volumes with limited additional operational value. Selecting periodic recording for lower-risk resource types and/or non-production environments can maintain necessary visibility while reducing CIR volume and cost. Recorder settings are account/region scoped, so you can apply continuous in production where required and periodic elsewhere.</p>","<p id="""">Configuration Item Recorded (CIR)–based pricing: charges accrue for each configuration item that AWS Config records. Continuous recording captures a configuration item for every change event, while periodic recording captures state at fixed intervals (e.g., daily). Although the unit price for periodic items may differ, reducing recording frequency can materially lower total CIR volume and spend when per-change granularity isn’t required.</p><p id="""">*Note:* AWS Config also has other pricing dimensions (e.g., rule and conformance pack evaluations). This entry focuses specifically on the CIR component.</p>","<ul id=""""><li id="""">Identify accounts where AWS Config spend is dominated by configuration items recorded relative to overall cloud spend and risk profile.</li><li id="""">Confirm whether continuous recording is enabled broadly by default versus selectively by resource type/environment.</li><li id="""">Determine which resource categories experience frequent state changes and contribute disproportionately to recorded items.</li><li id="""">Validate with Security/Compliance/SRE whether per-change visibility is truly required for those categories.</li><li id="""">Assess whether observed recording volume aligns with actual investigative or audit needs.</li></ul>","<ul id=""""><li id="""">Shift suitable resource types and/or non-production environments from continuous to periodic recording where real-time change tracking isn’t required.</li><li id="""">Scope recording frequency by environment: continuous for production or high-risk resources; periodic for development/test or low-risk resources.</li><li id="""">Document the rationale and ownership (e.g., security vs. platform) to ensure shared expectations on visibility vs. cost.</li><li id="""">Monitor post-change configuration-item volume and spend to confirm expected reductions and adjust coverage as needed.</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/config/latest/developerguide/How-AWS-Config-Works.html?utm_source=chatgpt.com"" id="""">https://docs.aws.amazon.com/config/latest/developerguide/How-AWS-Config-Works.html?utm_source=chatgpt.com</a></li><li id=""""><a href=""https://docs.aws.amazon.com/config/latest/developerguide/config-rule-options.html?utm_source=chatgpt.com"" id="""">https://docs.aws.amazon.com/config/latest/developerguide/config-rule-options.html?utm_source=chatgpt.com</a></li></ul>",FALSE,FALSE,,N/A,,<p>AWS-Other-1361</p>
Unnecessary Costs from Unused Lambda Versions with SnapStart,unnecessary-costs-from-unused-lambda-versions-with-snapstart,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58fc5416171f922c451,FALSE,FALSE,Mon Nov 03 2025 16:17:19 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Jake McCracken,aws,aws-lambda,version-sprawl,compute,"<p id="""">Many teams publish new Lambda versions frequently (e.g., through CI/CD pipelines) but do not clean up old ones. When SnapStart is enabled, each of these versions retains an active snapshot in the cache, generating ongoing charges. Over time, accumulated unused versions can significantly increase spend without delivering any business value. This problem compounds in environments with high deployment velocity or many functions.</p>","<p id="""">Lambda SnapStart incurs charges for caching snapshots associated with each published function version. Even if a version is no longer invoked, the snapshot cache continues to accrue cost until explicitly removed.</p>","<ul id=""""><li id="""">Review the number of published Lambda versions with SnapStart enabled across accounts</li><li id="""">Assess whether older versions are still being invoked or if they remain idle but cached</li><li id="""">Correlate Lambda costs with the number of SnapStart-enabled versions retained in each environment</li><li id="""">Check for environments with frequent deployments where unused versions may accumulate</li></ul>","<ul id=""""><li id="""">Delete unused Lambda function versions with SnapStart enabled to eliminate unnecessary cache charges</li><li id="""">Implement version lifecycle management practices in CI/CD pipelines to automatically clean up old versions</li><li id="""">Retain only the most recent versions required for rollback or audit purposes</li><li id="""">Monitor the number of SnapStart-enabled versions per function and enforce limits to prevent sprawl</li></ul>",,FALSE,FALSE,,N/A,,<p>AWS-Compute-5995</p>
Unnecessary Default Log Retention in Datadog,unnecessary-default-log-retention-in-datadog,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6908d58edcdb4c59b6992aec,FALSE,FALSE,Mon Nov 03 2025 16:17:18 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:59 GMT+0000 (Coordinated Universal Time),Jérémy Nancel,datadog,datadog-logs,excessive-retention-configuration,other,"<p id="""">Many organizations keep Datadog’s default log retention settings without evaluating business requirements. Defaults may extend retention far beyond what is useful for troubleshooting, performance monitoring, or compliance. This leads to unnecessary storage and indexing costs, particularly in non-production environments or for logs with limited value after a short period. By adjusting retention per project, environment, or service, organizations can reduce spend while still meeting compliance and operational needs.</p>","<p id="""">Datadog bills based on the volume of ingested logs, traces, and metrics, as well as the length of their retention. Longer retention periods increase storage costs proportionally, regardless of whether historical data provides business value.</p>","<ul id=""""><li id="""">Identify services where default Datadog log retention settings are in place without business justification</li><li id="""">Compare retention length across projects, environments, and service types (general logs, error logs, APM traces, metrics)</li><li id="""">Validate whether business, compliance, or audit requirements justify keeping logs for the current duration</li><li id="""">Evaluate whether shorter retention windows would still support operational monitoring and troubleshooting needs</li></ul>","<ul id=""""><li id="""">Work with stakeholders to align log retention periods with business and compliance requirements</li><li id="""">Reduce Datadog retention settings per service type (logs, traces, metrics) to the minimum necessary for value delivery</li><li id="""">Implement differentiated retention policies by environment (production vs. non-production) or by log type</li><li id="""">Regularly review retention policies as requirements evolve to avoid reverting to unnecessary defaults</li></ul>",,FALSE,FALSE,,N/A,,<p>Datadog-Other-7529</p>
Unnecessary Multi-AZ Configuration for Non-Production RDS Instances,unnecessary-multi-az-configuration-for-non-production-rds-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589418ac473aaa0d4aa297,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Swapna Samuel,aws,aws-rds,misconfigured-redundancy,databases,"<p id="""">RDS Multi-AZ deployments are designed for production-grade fault tolerance. In non-production environments, this configuration doubles the cost of database instances and storage with little added value. Unless explicitly required for high-availability testing, Multi-AZ in dev, staging, or test environments typically results in avoidable expense.</p>","<p id="""">RDS instances incur compute and storage charges for both the primary and standby nodes in a Multi-AZ configuration, along with associated data transfer between zones.</p>","<p id="""">Identify RDS instances in non-production accounts or environments with Multi-AZ enabled  Use environment tags or naming conventions to distinguish staging and development workloads  Evaluate instance-level metrics to assess whether failover capabilities are actively used or needed  Check for duplicated storage and compute resources across Availability Zones</p>","<p id="""">Modify RDS instances in non-production environments to use single-AZ deployment  Update provisioning templates and IaC modules to default to single-AZ for non-prod  Create safeguards to prevent Multi-AZ deployments in dev/test unless explicitly approved</p>","<p id=""""><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"" id="""">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a><br><a href=""https://aws.amazon.com/rds/pricing/"" id="""">https://aws.amazon.com/rds/pricing/</a></p>",FALSE,FALSE,,,,<p>AWS-Databases-2018</p>
Unnecessary Multi-AZ Deployment for ElastiCache in Non-Production Environments,unnecessary-multi-az-deployment-for-elasticache-in-non-production-environments,682077b786159f81bf47f152,682077b786159f81bf47f0b2,68589418e2b68c22adf274f4,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:57 GMT+0000 (Coordinated Universal Time),,aws,aws-elasticache,misconfigured-redundancy,databases,"<p id="""">In non-production environments, enabling Multi-AZ Redis clusters introduces redundant replicas that may not deliver meaningful business value. These replicas are often kept in sync across Availability Zones, incurring both compute and inter-AZ data transfer costs. For development or test clusters that can tolerate occasional downtime or data loss, a single-AZ deployment is typically sufficient and significantly less expensive.</p>","<p id="""">Billed per node-hour, with separate charges for primary and replica nodes; cross-AZ replication incurs additional data transfer charges.</p>","<ul id=""""><li id="""">Check if ElastiCache Redis clusters in non-production accounts or environments are configured with multiple nodes across different Availability Zones</li><li id="""">Confirm whether replication groups have replicas where fault tolerance is not required</li><li id="""">Review whether the cluster supports ephemeral workloads that don’t require HA or persistence</li><li id="""">Assess whether production-level SLAs or failover requirements apply to these environments</li></ul>","<ul id=""""><li id="""">Reconfigure ElastiCache Redis clusters in non-production environments to use single-node, single-AZ deployments</li><li id="""">If persistence is not needed, consider using Memcached instead of Redis to further reduce costs</li><li id="""">Tag clusters appropriately to distinguish between environments and enforce automated guardrails</li></ul>","<p id=""""><a href=""https://aws.amazon.com/elasticache/pricing/"" id="""">Amazon ElastiCache Pricing</a><br><a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/BestPractices.html"" id="""">ElastiCache Best Practices</a></p>",FALSE,FALSE,,,,<p>AWS-Databases-3385</p>
Unnecessary Multi-AZ Deployment for Non-Production EC2 Instances,unnecessary-multi-az-deployment-for-non-production-ec2-instances,682077b786159f81bf47f152,682077b786159f81bf47f0b2,685894172610ccf42b2e00c5,FALSE,FALSE,Sun Jun 22 2025 23:39:03 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:56 GMT+0000 (Coordinated Universal Time),Swapna Samuel,aws,aws-ec2,misconfigured-redundancy,compute,"<p id="""">Multi-AZ deployment is often essential for production workloads, but its use in non-production environments (e.g., development, test, QA) offers minimal value. These environments typically do not require high availability, yet still incur the full cost of redundant compute, storage, and data transfer. This results in unnecessary spend without operational benefit.</p>","<p id="""">EC2 is billed based on instance runtime and size, with additional charges for inter-AZ data transfer and supporting infrastructure (e.g., EBS volumes, load balancers). Multi-AZ deployments double resource usage and introduce cross-AZ networking costs.</p>","<ul id=""""><li id="""">Review EC2 instances in non-production environments for Multi-AZ configuration</li><li id="""">Check for Auto Scaling Groups or load balancers distributing traffic across multiple Availability Zones</li><li id="""">Evaluate network transfer charges between Availability Zones</li><li id="""">Verify with workload owners whether fault tolerance is required for the environment</li></ul>","<ul id=""""><li id="""">Reconfigure EC2 workloads in non-production environments to use a single Availability Zone</li><li id="""">Avoid distributing instances or traffic across multiple AZs unless justified by a specific requirement</li><li id="""">Establish environment-based architectural guidelines to limit Multi-AZ usage</li></ul>","<p id=""""><a href=""https://aws.amazon.com/ec2/pricing/"" id="""">https://aws.amazon.com/ec2/pricing/</a><br><a href=""https://docs.aws.amazon.com/whitepapers/latest/aws-overview/architecting-for-availability.html"" id="""">https://docs.aws.amazon.com/whitepapers/latest/aws-overview/architecting-for-availability.html</a></p>",FALSE,FALSE,,,,<p>AWS-Compute-6547</p>
Unnecessary Multi-AZ Deployment for OpenSearch in Non-Production Environments,unnecessary-multi-az-deployment-for-opensearch-in-non-production-environments,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6858941820d639578245dad9,FALSE,FALSE,Sun Jun 22 2025 23:39:04 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),,aws,aws-opensearch,misconfigured-redundancy,databases,"<p id="""">Non-production OpenSearch domains often inherit Multi-AZ configurations from production setups without clear justification. This leads to redundant replica shards across AZs, inflating both compute and storage costs. Unless strict uptime or fault tolerance requirements exist, most dev/test workloads do not benefit from Multi-AZ redundancy.</p>","<p id="""">Billed per node-hour and EBS storage; cross-AZ replication adds networking charges; multiple AZs require additional replica nodes.</p>","<ul id=""""><li id="""">Check if OpenSearch domains in non-production environments are configured with multiple Availability Zones</li><li id="""">Verify whether replica shard configurations are enabled for test workloads that can tolerate index rebuilds or data loss</li><li id="""">Review fault-tolerance and SLA requirements to confirm whether Multi-AZ is warranted</li><li id="""">Evaluate whether backups or snapshots already meet data durability needs</li></ul>","<ul id=""""><li id="""">Reconfigure OpenSearch domains in non-production environments to use single-AZ deployments</li><li id="""">Reduce the number of replica shards where appropriate</li><li id="""">Implement automated tagging or policy enforcement to prevent accidental Multi-AZ usage in non-prod</li></ul>","<p id=""""><a href=""https://aws.amazon.com/opensearch-service/pricing/"" id="""">Amazon OpenSearch Pricing</a><br><a href=""https://docs.aws.amazon.com/opensearch-service/latest/developerguide/bp.html"" id="""">Deployment Best Practices</a></p>",FALSE,FALSE,,,,<p>AWS-Databases-9580</p>
Unnecessary Reset of Long-Term Storage Pricing in BigQuery,unnecessary-reset-of-long-term-storage-pricing-in-bigquery,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b489b69e4e353501aad,FALSE,FALSE,Thu May 22 2025 14:57:12 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:46 GMT+0000 (Coordinated Universal Time),Ben de Mora,gcp,gcp-bigquery,behavioral-inefficiency,databases,"<p id="""">BigQuery incentivizes efficient data retention by cutting storage costs in half for tables or partitions that go 90 days without modification. However, many teams unintentionally forfeit this discount by performing broad or unnecessary updates to long-lived datasets — for example, touching an entire table when only a few rows need to change. Even small-scale or programmatic updates can trigger a full reset of the 90-day timer on affected data. This behavior is subtle but expensive: it silently doubles the storage cost of large datasets for another 90-day cycle, even when the data itself is mostly static. Without intentional safeguards, organizations may repeatedly reset their discounted storage window without realizing it.</p>","<p id="""">BigQuery storage is billed based on:</p><ul id=""""><li id="""">Active (Standard) Storage: Full price for tables or partitions modified in the last 90 days</li><li id="""">Long-Term Storage: 50% discount applies after 90 days without modification.  Any change — including update, insert, or delete — resets the 90-day clock for modified data.</li></ul>","<ul id=""""><li id="""">Identify tables or partitions with high storage volume that consistently fail to enter long-term storage pricing</li><li id="""">Correlate storage cost fluctuations with write operations or ETL jobs</li><li id="""">Review update patterns on reference or static datasets (e.g., product catalogs, configuration data)</li><li id="""">Check whether queries or pipelines are modifying more data than necessary</li></ul>","<ul id=""""><li id="""">Limit write operations to the exact data that requires change — avoid broad table rewrites</li><li id="""">Partition large datasets so updates are scoped to specific partitions, minimizing disruption to cold data</li><li id="""">For static reference tables, use append-only patterns or restructure workflows to avoid unnecessary modification</li><li id="""">Educate teams on the impact of modifying long-lived data and incorporate cost-aware practices into pipeline development</li></ul>","<p id=""""><a href=""https://cloud.google.com/bigquery/pricing#storage"" id="""">BigQuery Storage Pricing</a></p>",FALSE,FALSE,,,,<p>GCP-Databases-4691</p>
Unnecessary Use of Embeddings for Simple Retrieval Tasks,unnecessary-use-of-embeddings-for-simple-retrieval-tasks-b7ed2,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e12afa8929258beb7d,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:31 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,databricks,databricks-vector-search,misapplied-embedding-architecture,ai,"<p id="""">Embedding-based retrieval enables semantic matching even when keywords differ. But many Databricks workloads—catalog lookups, metadata search, deterministic classification, or fixed-rule routing—do not require semantic understanding. When embeddings are used anyway, teams incur DBU cost for embedding generation, additional storage for vector columns or indexes, and more expensive similarity-search compute. This often stems from defaulting to a RAG approach rather than evaluating whether a simpler retrieval mechanism would perform equally well.</p>","<p id="""">Embedding generation consumes model inference compute (DBUs), and vector indexing/search consumes additional compute and storage. Using embeddings where they are unnecessary leads directly to elevated DBU usage and storage cost.</p>","<ul id=""""><li id="""">Identify pipelines generating embeddings for content that rarely changes or has deterministic lookup paths</li><li id="""">Compare search accuracy using keyword vs. vector retrieval</li><li id="""">Review DBU usage for embedding-generation workloads</li><li id="""">Assess vector index size and query volume relative to task complexity</li><li id="""">Look for RAG architectures implemented without clear semantic justification</li></ul>","<ul id=""""><li id="""">Replace embeddings with keyword or metadata-based search for simple or deterministic tasks</li><li id="""">Disable or remove embedding pipelines that do not provide semantic benefit</li><li id="""">Reduce vector index storage where semantic retrieval is unnecessary</li><li id="""">Benchmark retrieval accuracy before defaulting to embedding-based solutions</li><li id="""">Periodically audit ML/AI workloads to prevent embedding overuse</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.databricks.com/en/generative-ai/search/vector-search.html"" id="""">https://docs.databricks.com/en/generative-ai/search/vector-search.html</a></li></ul>",FALSE,FALSE,,,,Databricks-AI-4714
Unnecessary Use of Embeddings for Simple Retrieval Tasks,unnecessary-use-of-embeddings-for-simple-retrieval-tasks-98e11,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e1408799aed855f7c1,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:52 GMT+0000 (Coordinated Universal Time),,aws,aws-bedrock,misapplied-embedding-architecture,ai,"<p id="""">Embeddings enable semantic search by converting text into vectors that capture meaning. Keyword or metadata search performs exact or simple lexical matches. Many workloads—FAQ lookup, helpdesk routing, short product lookups, or rule-based filtering—do not benefit from semantic search. When embeddings are used anyway, organizations pay for embedding generation, vector storage, and similarity search without gaining accuracy or relevance improvements. This often happens when teams adopt RAG “by default” for problems that do not require semantic understanding.</p>","<p id="""">Embedding requests are billed per input token or per 1,000 tokens depending on the model provider. Downstream vector database queries and storage incur additional costs. Using embeddings unnecessarily increases spend across inference, storage, and retrieval layers.</p>","<ul id=""""><li id="""">Identify Bedrock workloads generating embeddings for simple keyword-matching scenarios</li><li id="""">Review accuracy differences between embedding-based search and basic text search</li><li id="""">Assess vector index growth driven by unnecessarily large embedding pipelines</li><li id="""">Look for RAG implementations built for content that rarely changes</li><li id="""">Evaluate whether embeddings were introduced without a clear semantic requirement</li></ul>","<ul id=""""><li id="""">Replace embeddings with keyword or metadata-based search when semantic similarity is not required</li><li id="""">Remove embedding-generation pipelines for deterministic or low-complexity tasks</li><li id="""">Decommission or reduce vector storage supporting non-semantic retrieval</li><li id="""">Validate whether simpler retrieval methods meet accuracy needs before using embeddings</li><li id="""">Periodically reassess RAG and vector-search usage to prevent unnecessary expansion</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.aws.amazon.com/bedrock/latest/userguide/embeddings.html"" id="""">https://docs.aws.amazon.com/bedrock/latest/userguide/embeddings.html</a></li></ul>",FALSE,FALSE,,,,
Unnecessary Use of Embeddings for Simple Retrieval Tasks,unnecessary-use-of-embeddings-for-simple-retrieval-tasks-5a990,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e181f6a3f97fabb908,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),,snowflake,snowflake-cortex,misapplied-embedding-architecture,ai,"<p id="""">Embeddings enable semantic similarity search by representing text as high-dimensional vectors. Keyword search, however, returns results based on lexical matches and is often sufficient for simple retrieval tasks such as FAQ matching, deterministic filtering, metadata lookup, or rule-based routing. When embeddings are used for these low-complexity scenarios, organizations pay for compute to generate embeddings, storage for vector columns, and compute-heavy cosine similarity searches — without improving accuracy or user experience. In Snowflake, this can also increase warehouse load and query runtime.</p>","<p id="""">Embedding generation and vector search operations consume Snowflake compute credits. Larger embeddings increase storage requirements and query processing costs. When embeddings are not necessary, both compute and storage consumption rise needlessly.</p>","<ul id=""""><li id="""">Identify tables with vector columns used for retrieval tasks that follow deterministic or keyword patterns</li><li id="""">Compare retrieval accuracy between vector search and simple keyword filtering</li><li id="""">Review compute consumption for embedding-generation pipelines that process static or rarely changing data</li><li id="""">Assess storage growth associated with large or unnecessary vector columns</li><li id="""">Determine whether semantic search was adopted without a clear functional requirement</li></ul>","<ul id=""""><li id="""">Use keyword, metadata, or SQL-based filtering for simple retrieval workloads</li><li id="""">Remove or stop generating embeddings where semantic similarity is not required</li><li id="""">Drop unused vector columns to reduce storage cost</li><li id="""">Benchmark simple search vs. vector search before allocating compute to embeddings</li><li id="""">Periodically review vector-search usage to prevent unnecessary architectural complexity</li></ul>","<ul id=""""><li id=""""><a href=""https://docs.snowflake.com/en/guides-overview-cortex"" id="""">https://docs.snowflake.com/en/guides-overview-cortex</a></li></ul>",FALSE,FALSE,,,,
Unnecessary Use of Embeddings for Simple Retrieval Tasks,unnecessary-use-of-embeddings-for-simple-retrieval-tasks-2e32a,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e1904413aab595bb64,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 16:08:56 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 16:23:56 GMT+0000 (Coordinated Universal Time),,azure,azure-cognitive-services,misapplied-embedding-architecture,ai,"<p id="""">Embeddings enable semantic retrieval by capturing the meaning of text, while keyword search returns results based on exact or lexical matches. Many Azure workloads—FAQ search, routing, deterministic classification, or structured lookups—achieve the same or better accuracy using simple keyword or metadata filtering. When embeddings are used for these uncomplicated tasks, organizations pay for token-based embedding generation, vector storage, and compute-heavy similarity search without receiving meaningful quality improvements. This inefficiency often occurs when RAG is used automatically rather than intentionally.</p><p>‍</p>","<p id="""">Embedding models are billed per input token. Vector indexing and search operations in Azure AI Search (or other vector stores) incur additional storage and query compute costs. Using embeddings when unnecessary creates avoidable multi-layer cost.</p>","<ul id=""""><li id="""">Identify workloads using embeddings for simple, deterministic retrieval tasks</li><li id="""">Review whether keyword search achieves similar accuracy</li><li id="""">Evaluate vector index growth and query volume relative to task complexity</li><li id="""">Check for embedding pipelines built around static or rarely changing content</li><li id="""">Determine whether semantic search was added without a clear functional requirement</li></ul>","<ul id=""""><li id="""">Replace embeddings with keyword or metadata-based search for simple retrieval tasks</li><li id="""">Disable or remove embedding generation pipelines that offer no semantic benefit</li><li id="""">Reduce vector index storage for workloads not requiring semantic search</li><li id="""">Benchmark retrieval accuracy using simpler search methods before defaulting to embeddings</li><li id="""">Periodically review retrieval architectures to prevent unnecessary vector-search adoption</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/azure/search/vector-search-overview"" id="""">https://learn.microsoft.com/azure/search/vector-search-overview</a></li></ul>",FALSE,FALSE,,,,
Unnecessary Use of Embeddings for Simple Retrieval Tasks,unnecessary-use-of-embeddings-for-simple-retrieval-tasks-810c5,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e1c75cccf92def52a3,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:53 GMT+0000 (Coordinated Universal Time),,gcp,gcp-vertex-ai,misapplied-embedding-architecture,ai,"<p id="""">Embeddings allow semantic search — they map text into vectors so the system can find content with similar meaning, even if the keywords don’t match. Keyword or metadata search, by contrast, looks for exact terms or simple filters. Many workloads (FAQ lookups, short product searches, rule-based routing) do not need semantic understanding and perform just as well with basic keyword logic. When teams use embeddings for these simple tasks, they pay for embedding generation, vector storage, and similarity search without gaining meaningful accuracy or functionality.</p>","<p id="""">Embedding generation is billed per input token, and vector databases incur storage and query compute costs. Using embeddings when they are not required creates avoidable spend across both modeling and infrastructure layers.</p>","<ul id=""""><li id="""">Identify workloads using embeddings for deterministic or keyword-matching tasks</li><li id="""">Review whether retrieval accuracy remains unchanged when using simple search</li><li id="""">Assess vector database size and query volume relative to task complexity</li><li id="""">Look for embedding pipelines built for content that rarely changes</li><li id="""">Evaluate whether a RAG architecture was adopted without a clear functional need</li></ul>","<ul id=""""><li id="""">Replace embeddings with keyword or metadata-based search for simple retrieval tasks</li><li id="""">Remove embedding generation pipelines where semantic similarity is unnecessary</li><li id="""">Reduce or decommission vector database storage tied to non-semantic workloads</li><li id="""">Validate accuracy using simpler retrieval methods before reintroducing embeddings</li><li id="""">Reassess retrieval architecture periodically to prevent embedding sprawl</li></ul>",,FALSE,FALSE,,,,
Unnecessary Use of RA-GRS for Azure SQL Backup Storage,unnecessary-use-of-ra-grs-for-azure-sql-backup-storage,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b4d3674d922d5950f29,FALSE,FALSE,Thu May 22 2025 14:57:17 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:47 GMT+0000 (Coordinated Universal Time),Balazs Engedi,azure,azure-sql,inefficient-configuration,databases,"<p id="""">  Azure SQL databases often use the default backup configuration, which stores backups in RA-GRS storage to ensure geo-redundancy. While suitable for high-availability production systems, this level of resilience may be unnecessary for development, testing, or lower-impact workloads.</p><p id="""">Using RA-GRS without a business requirement results in avoidable costs. Downgrading to LRS or ZRS — where appropriate — can significantly reduce monthly backup storage spend. This change has no impact on backup frequency or retention behavior, only the underlying storage replication method.</p>","<ul id=""""><li id="""">Backup storage is charged based on GB/month</li><li id=""""><strong id="""">RA-GRS</strong> storage is the default and most expensive option</li><li id=""""><strong id="""">LRS</strong> (Locally Redundant Storage) and <strong id="""">ZRS</strong> (Zone Redundant Storage) are lower-cost alternatives</li><li id="""">Changing backup storage tier can reduce backup costs by <strong id="""">up to 50%</strong> without affecting primary database performance</li></ul>","<ul id=""""><li id="""">Review backup redundancy settings for Azure SQL databases</li><li id="""">Identify databases with RA-GRS enabled by default</li><li id="""">Determine whether geo-redundant backup storage is a business or compliance requirement</li><li id="""">Validate whether LRS or ZRS would be sufficient based on workload criticality and risk tolerance</li></ul>","<ul id=""""><li id="""">For non-critical or non-regulated workloads, change the backup redundancy setting to LRS (or ZRS where supported)</li><li id="""">Document any exceptions where RA-GRS must be retained for compliance</li><li id="""">Incorporate backup configuration reviews into provisioning and governance processes</li><li id="""">Monitor backup storage cost trends following changes to confirm savings</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/long-term-retention-overview?tabs=azure-portal#configure-backup-storage-redundancy"" id="""">Configure Backup Storage Redundancy in Azure SQL</a></li><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy"" id="""">Azure Storage Redundancy Options</a></li></ul>",FALSE,FALSE,,,,<p>Azure-Databases-2089</p>
Unoptimized Billing Model for BigQuery Dataset Storage,unoptimized-billing-model-for-bigquery-dataset-storage,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84cb63b535f97015693,FALSE,FALSE,Sun Aug 10 2025 19:51:40 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:58 GMT+0000 (Coordinated Universal Time),Laurent Dumont,gcp,gcp-bigquery,inefficient-configuration,databases,"<p id="""">Highly compressible datasets, such as those with repeated string fields, nested structures, or uniform rows, can benefit significantly from physical storage billing. Yet most datasets remain on logical storage by default, even when physical storage would reduce costs.</p><p id="""">This inefficiency is common for cold or infrequently updated datasets that are no longer optimized or regularly reviewed. Because storage behavior and data characteristics evolve, failing to periodically reassess the billing model may result in persistent waste.</p>","<p id="""">BigQuery supports two billing models for table storage:</p><p id="""">Logical Storage (default): Billed based on the uncompressed size of user data. This model includes time travel and fail-safe storage at no extra cost.</p><p id="""">Physical Storage: Billed based on the actual compressed bytes on disk. Time travel and fail-safe storage are charged separately at the same rate as active storage.</p><p id="""">While physical storage can significantly reduce costs for highly compressible datasets (e.g., compression savings exceeding \~45%), its higher per-byte cost (\~1.8x) and additional charges for retention-related features may lead to higher total costs for frequently updated or less compressible data.</p>","<ul id=""""><li id="""">Evaluate datasets with high logical-to-physical compression ratios.</li><li id="""">The billing model is applied at the dataset level. Make sure that the compression ratio is aligned to all tables within the dataset.</li><li id="""">Run periodic queries against `INFORMATION_SCHEMA.TABLE_STORAGE` to compare logical and physical storage sizes across multiple dimensions: active, long-term, time travel and fail-safe.</li><li id="""">Prioritize datasets that are infrequently updated (therefore with low time travel and fail-safe storage volumes) or primarily used for historical lookback</li><li id="""">Use the storage pricing to simulate cost in the physical model \- multiply by the new rates, and add the time travel and fail-safe storage bytes.</li><li id="""">Flag datasets with simulated physical storage price lower than the current logical storage price</li><li id="""">Balance the frequency of detection queries with their cost, and avoid excessive scanning of large datasets</li></ul>","<ul id=""""><li id="""">Switch eligible datasets to physical storage billing when compression advantages are material</li><li id="""">There is no performance impact between the two billing models.</li><li id="""">Changing the billing model takes 24 hours before it’s reflected in the GCP billing SKUs.</li><li id="""">There is a 14-day waiting period when a change is made to the billing model.</li><li id="""">Periodically reassess dataset compression ratios to determine if billing model changes are warranted</li><li id="""">Use partitioning and clustering to improve compressibility where possible</li><li id="""">Apply billing model changes to cold or infrequently modified data first, as their structure is more stable</li><li id="""">Document the dataset billing model decisions to ensure transparency and reproducibility</li><li id="""">Monitor billing changes for 2-3 months after switching to validate expected savings</li><li id="""">Consider dataset lifecycle \- archive old partitions to cheaper storage classes first</li></ul>","<ul id=""""><li id=""""><a href=""https://cloud.google.com/bigquery/docs/datasets-intro#dataset_storage_billing_models"" id="""">Physical vs logical storage</a></li><li id=""""><a href=""https://cloud.google.com/bigquery/docs/information-schema-tables"" id="""">Querying INFORMATION\_SCHEMA for table metadata</a></li><li id=""""><a href=""https://cloud.google.com/bigquery/docs/information-schema-table-storage#calculating_storage_billing"" id="""">TABLE\_STORAGE view</a></li><li id=""""><a href=""https://cloud.google.com/bigquery/pricing?hl=en#section-4"" id="""">BigQuery Pricing</a></li><li id=""""><a href=""https://cloud.google.com/bigquery/docs/information-schema-table-storage"" id="""">BigQuery storage forecast queries</a></li></ul>",FALSE,FALSE,,,,<p>GCP-Databases-3284</p>
Unused EBS Volume Attached to a Stopped EC2 Instance,unused-ebs-volume-attached-to-a-stopped-ec2-instance,682077b786159f81bf47f152,682077b786159f81bf47f0b2,682f3b42eea36dec99f42fd3,FALSE,FALSE,Thu May 22 2025 14:57:06 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:48 GMT+0000 (Coordinated Universal Time),,aws,aws-ebs,unused-resource,storage,"<p id="""">This inefficiency occurs when an EC2 instance is stopped but still has one or more attached EBS volumes. Although the compute resource is not generating charges while stopped, the attached volumes continue to incur full storage and performance-related costs. These volumes are often overlooked in cost reviews, especially if the instance is temporarily paused or has been left in a stopped state long-term. Without regular validation, these volumes may represent unused capacity that delivers no value.</p>","<p id="""">EBS volumes are billed per GB-month of provisioned storage, with additional charges for provisioned IOPS (for io1/io2) and throughput (for gp3). Charges apply continuously, regardless of whether the volume is attached to a running instance. When a volume is attached to an EC2 instance that is stopped, the instance incurs no charges—but the volume continues to accumulate storage and performance-related costs.</p>","<ul id=""""><li id="""">Identify EC2 instances in a stopped state during the defined lookback period</li><li id="""">Check whether attached EBS volumes remain actively provisioned</li><li id="""">Validate whether the instance or its volumes are needed for recovery, migration, or scheduled activation</li><li id="""">Consult application owners to determine if the instance and its storage are still required</li></ul>","<p id="""">If the volume is no longer needed, back up any essential data and delete the volume to stop ongoing storage charges. Consider creating a snapshot if the data may be useful for future recovery. If the instance will be restarted soon, set a policy or reminder to revisit after a fixed period. Implement scheduled audits to catch volumes attached to stopped instances that no longer serve an operational purpose.</p>","<ul id=""""><li id=""""><a href=""https://aws.amazon.com/ebs/pricing/\&quot;"" id="""">EBS Pricing</a></li><li id=""""><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html\"" id="""">Amazon EC2 Instance Lifecycle</a></li></ul>",FALSE,FALSE,,107,,<p>AWS-Storage-7849</p>
Unused S3 Storage Lens Advanced,unused-s3-storage-lens-advanced,682077b786159f81bf47f152,682077b786159f81bf47f0b2,6898f84d624d7b2d1171beda,FALSE,FALSE,Sun Aug 10 2025 19:51:41 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Sun Dec 14 2025 10:45:55 GMT+0000 (Coordinated Universal Time),Igor Bareev,aws,aws-s3,unused-resource,storage,"<p id="""">S3 Storage Lens Advanced provides valuable insights into storage usage and trends, but it incurs a recurring cost. Organisations often enable it during an optimization initiative but fail to turn it off afterwards. When no active storage efficiency efforts are underway, these advanced metrics can become an unnecessary expense, especially at large scale across many buckets.</p><p id="""">Advanced metrics include things like:</p><p id="""">Cost optimization recommendations</p><p id="""">Data retrieval patterns</p><p id="""">Encryption and access control analytics</p><p id="""">Historical trends beyond the 30-day free tier</p><p id="""">Because the configuration is global or account-level, it’s easy to forget that these additional metrics are enabled and quietly incurring cost. This inefficiency often surfaces in organizations that over-invest in observability tooling without aligning it to an ongoing operational workflow.</p>","<p id="""">S3 Storage Lens Advanced is billed based on:</p><p id="""">The number of metrics collected per object and bucket</p><p id="""">The number of buckets monitored</p><p id="""">Data retention period (up to 15 months for advanced metrics)</p><p id="""">Pricing is tiered and additive to standard S3 storage costs</p><p id="""">Costs can range from a few dollars to hundreds of dollars monthly, depending on bucket count and object volume</p><p id="""">Charges apply per AWS account or organization, depending on the configuration scope</p><p id="""">Billing continues as long as Storage Lens Advanced is enabled, even if the data is not actively reviewed or used.</p>","<ul id=""""><li id="""">Check if S3 Storage Lens Advanced is enabled in the AWS account or organization</li><li id="""">Review whether the advanced metrics are actively used or being surfaced in optimization reviews</li><li id="""">Evaluate if there is an ongoing effort that requires advanced visibility into S3 usage and activity</li><li id="""">Assess whether the retention duration and granularity provided by the advanced tier are necessary for current goals</li><li id="""">Determine the number of buckets being monitored and whether all of them need advanced analytics</li><li id="""">Check CloudTrail logs for recent S3 Storage Lens dashboard access to gauge actual usage</li><li id="""">Review whether alternative tools (AWS Cost Explorer, third-party solutions) provide sufficient S3 insights</li></ul>","<ul id=""""><li id="""">Disable S3 Storage Lens Advanced when not actively needed</li><li id="""">Use the free tier for basic visibility and re-enable Advanced only during optimization cycles</li><li id="""">Set a periodic review of observability tools to ensure paid features still deliver value</li><li id="""">Scope S3 Storage Lens Advanced to only necessary accounts or buckets if partial usage is possible</li><li id="""">Export historical data before disabling if needed for compliance or analysis</li><li id="""">Consider using AWS Config rules or Cost Anomaly Detection as lighter-weight alternatives for basic monitoring</li></ul>","<ul id=""""><li id="""">https://aws.amazon.com/s3/pricing/ https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage\_lens.html</li></ul>",FALSE,FALSE,,,,<p>AWS-Storage-4410</p>
Using High-Cost Bedrock Models for Low-Complexity Tasks,using-high-cost-bedrock-models-for-low-complexity-tasks-734ca,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e0de3d4810827325da,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 16:04:58 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 16:23:56 GMT+0000 (Coordinated Universal Time),,aws,aws-bedrock,overpowered-model-selection,ai,"<p id="""">Many Bedrock workloads involve low-complexity tasks such as tagging, classification, routing, entity extraction, keyword detection, document triage, or lightweight summarization. These tasks **do not require** the advanced reasoning or generative capabilities of higher-cost models such as Claude 3 Opus or comparable premium models. When organizations default to a high-end model across all applications—or fail to periodically reassess model selection—they pay elevated costs for work that could be performed effectively by smaller, lower-cost models such as Claude Haiku or other compact model families. This inefficiency becomes more pronounced in high-volume, repetitive workloads where token counts scale quickly.</p>","<p id="""">Bedrock typically charges per input and output token (or per inference unit for some models). Larger, more capable models have substantially higher cost per token than smaller, task-optimized models. Using premium models for simple operations increases cost without improving quality.</p>","<ul id=""""><li id="""">Identify workloads generating simple, deterministic, or low-complexity outputs that do not require advanced generative reasoning</li><li id="""">Review Bedrock model choices to determine whether premium models are used across multiple applications by default</li><li id="""">Analyze token usage patterns showing high spend on repetitive or structured tasks</li><li id="""">Evaluate whether smaller or task-specific models could provide comparable accuracy and latency</li><li id="""">Assess whether engineering teams lack model selection guidelines, leading to overuse of high-cost options</li></ul>","<ul id=""""><li id="""">Select the smallest Bedrock model family capable of meeting accuracy, latency, and quality requirements</li><li id="""">Use compact or task-optimized models (e.g., Claude 3 Haiku or equivalent lightweight models) for classification, extraction, routing, and deterministic tasks</li><li id="""">Establish internal guidelines to prevent premium models from being used as global defaults</li><li id="""">Periodically re-evaluate deployed models as new, cost-efficient model families become available</li><li id="""">Test model outputs across multiple model tiers to ensure that quality remains acceptable after right-sizing</li></ul>",,FALSE,FALSE,,,,AWS-Compute-3917
Using High-Cost Models for Low-Complexity Tasks,using-high-cost-models-for-low-complexity-tasks-cc721,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e02039a9b3ae1c42c5,FALSE,FALSE,Sun Dec 14 2025 09:18:24 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 16:05:07 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 16:23:56 GMT+0000 (Coordinated Universal Time),,azure,azure-cognitive-services,overpowered-model-selection,ai,"<p id="""">Some workloads — such as text classification, keyword extraction, intent detection, routing, or lightweight summarization — do not require the capabilities of the most advanced model families. When high-cost models are used for these simple tasks, organizations pay elevated token rates for work that could be handled effectively by more efficient, lower-cost models. This mismatch typically arises from defaulting to a single model for all tasks or not periodically reviewing model usage patterns across applications.</p>","<p id="""">On-demand Azure OpenAI deployments are billed per input and output token. Larger, more capable models (e.g., GPT-4 class) have significantly higher cost per token. Choosing a model that exceeds workload requirements increases spend without improving output quality.</p>","<ul id=""""><li id="""">Identify workloads executing simple tasks that do not require advanced reasoning or generative abilities</li><li id="""">Review token consumption and model selection to determine whether premium models are being used broadly</li><li id="""">Assess whether output quality or accuracy would remain sufficient with smaller, task-optimized models</li><li id="""">Evaluate whether development teams rely on a “one model fits all” pattern across multiple applications</li></ul>","<ul id=""""><li id="""">Match each workload to the smallest Azure OpenAI model family that satisfies accuracy, latency, and quality needs</li><li id="""">Use task-optimized models (e.g., embeddings, lightweight classification models) instead of general-purpose generative models</li><li id="""">Establish model selection guidelines to prevent high-cost models from being used as defaults</li><li id="""">Periodically re-evaluate applications to ensure model choices align with evolving model offerings and workload complexity</li></ul>","<ul id=""""><li id=""""><a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models"" id="""">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models</a></li></ul>",FALSE,FALSE,,,,
Using High-Cost Models for Low-Complexity Tasks,using-high-cost-models-for-low-complexity-tasks-bec92,682077b786159f81bf47f152,682077b786159f81bf47f0b2,693e80e19db805a91efd051d,FALSE,FALSE,Sun Dec 14 2025 09:18:25 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:31:22 GMT+0000 (Coordinated Universal Time),Mon Dec 15 2025 14:48:50 GMT+0000 (Coordinated Universal Time),,gcp,gcp-vertex-ai,overpowered-model-selection,ai,"<p id="""">Vertex AI workloads often include low-complexity tasks such as classification, routing, keyword extraction, metadata parsing, document triage, or summarization of short and simple text. These operations do **not** require the advanced multimodal reasoning or long-context capabilities of larger Gemini model tiers. When organizations default to a single high-end model (such as Gemini Ultra or Pro) across all applications, they incur elevated token costs for work that could be served efficiently by **Gemini Flash** or smaller task-optimized variants. This mismatch is a common pattern in early deployments where model selection is driven by convenience rather than workload-specific requirements. Over time, this creates unnecessary spend without delivering measurable value.</p>","<p id="""">Generative AI usage is billed per input and output token. Larger, more capable models (e.g., Gemini Ultra or Pro) have significantly higher cost per token compared to smaller models optimized for fast, lightweight tasks. Choosing a model that exceeds workload requirements increases spend without improving output quality.</p>","<ul id=""""><li id="""">Identify workloads performing simple or deterministic tasks that do not require advanced generative reasoning</li><li id="""">Review model selections across projects to find consistent use of high-cost models as global defaults</li><li id="""">Assess token consumption patterns for repetitive or structured inference where lighter models would suffice</li><li id="""">Evaluate whether output quality, accuracy, or latency requirements can be met by lower-tier models</li><li id="""">Determine whether teams lack model selection guidelines or rely on a “one model fits all” pattern</li></ul>","<ul id=""""><li id="""">Select the smallest Vertex AI model tier that satisfies accuracy, latency, and quality requirements</li><li id="""">Use Gemini Flash or other lightweight model variants for classification, extraction, routing, and similar simple tasks</li><li id="""">Establish internal model selection standards to prevent unnecessary use of premium models</li><li id="""">Periodically re-evaluate deployed model choices as new Gemini model tiers and optimizations are released</li><li id="""">Validate functional behavior after model right-sizing to ensure quality remains acceptable</li></ul>",,FALSE,FALSE,,,,GCP-AI-4421